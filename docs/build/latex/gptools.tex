% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}
\hypersetup{bookmarksdepth=3}
\setcounter{tocdepth}{3}

\title{gptools Documentation}
\date{February 12, 2015}
\release{0.2}
\author{Mark Chilenski}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@mb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\renewcommand\PYGZsq{\textquotesingle}

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}


Source home: \href{https://github.com/markchil/gptools}{https://github.com/markchil/gptools}

Releases: \href{https://pypi.python.org/pypi/gptools/}{https://pypi.python.org/pypi/gptools/}

Installation is as simple as:

\begin{Verbatim}[commandchars=\\\{\}]
pip install gptools
\end{Verbatim}


\chapter{Overview}
\label{index:overview}\label{index:gptools-gaussian-process-regression-with-support-for-arbitrary-derivatives}
{\hyperref[gptools:module-gptools]{\code{gptools}}} is a Python package that provides a convenient, powerful and extensible implementation of Gaussian process regression (GPR). Central to \code{gptool}`s implementation is support for derivatives and their variances. Furthermore, the implementation supports the incorporation of arbitrary linearly transformed quantities into the GP.

There are two key classes:
\begin{itemize}
\item {} 
{\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} is the main class to represent a GP.

\item {} 
{\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} (and its many subclasses) represents a covariance kernel, and must be provided when constructing a {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}}. Separate kernels to describe the underlying signal and the noise are supported.

\end{itemize}

A third class, {\hyperref[gptools:gptools.utils.JointPrior]{\code{JointPrior}}}, allows you to construct a hyperprior of arbitrary complexity to dictate how the hyperparameters are handled.

Creating a Gaussian process is as simple as:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{gptools}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{gptools}\PYG{o}{.}\PYG{n}{SquaredExponentialKernel}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{gp} \PYG{o}{=} \PYG{n}{gptools}\PYG{o}{.}\PYG{n}{GaussianProcess}\PYG{p}{(}\PYG{n}{k}\PYG{p}{)}
\end{Verbatim}

But, the default bounds on the hyperparameters are very wide and can cause the optimizer/MCMC sampler to fail. So, it is usually a better idea to define the covariance kernel as:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{gptools}\PYG{o}{.}\PYG{n}{SquaredExponentialKernel}\PYG{p}{(}\PYG{n}{param\PYGZus{}bounds}\PYG{o}{=}\PYG{p}{[}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{1e3}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\end{Verbatim}

You will have to pick appropriate numbers by inspecting the typical range of your data.

Furthermore, you can include an explicit mean function by passing
the appropriate {\hyperref[gptools:gptools.mean.MeanFunction]{\code{MeanFunction}}} instance into the \emph{mu} keyword:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{gp} \PYG{o}{=} \PYG{n}{gptools}\PYG{o}{.}\PYG{n}{GaussianProcess}\PYG{p}{(}\PYG{n}{k}\PYG{p}{,} \PYG{n}{mu}\PYG{o}{=}\PYG{n}{gptools}\PYG{o}{.}\PYG{n}{LinearMeanFunction}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}

This will enable you to perform inference (both empirical and full Bayes) for
the hyperparameters of the mean function. Essentially, {\hyperref[gptools:module-gptools]{\code{gptools}}} can
perform nonlinear Bayesian regression with a Gaussian process fit to the
residuals.

You then add the training data using the {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.add_data]{\code{add\_data()}}} method:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{gp}\PYG{o}{.}\PYG{n}{add\PYGZus{}data}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{err\PYGZus{}y}\PYG{o}{=}\PYG{n}{stddev\PYGZus{}y}\PYG{p}{)}
\end{Verbatim}

Here, \emph{err\_y} is the \(1\sigma\) uncertainty on the observations \emph{y}. For exact values, simply omit this keyword. Adding a derivative observation is as simple as specifying the derivative order with the \emph{n} keyword:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{gp}\PYG{o}{.}\PYG{n}{add\PYGZus{}data}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{Verbatim}

This will force the slope at \(x=0\) to be exactly zero. Quantities that represent an arbitrary linear transformation of the ``basic'' observations can be added by specifying the \emph{T} keyword:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{gp}\PYG{o}{.}\PYG{n}{add\PYGZus{}data}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{T}\PYG{o}{=}\PYG{n}{T}\PYG{p}{)}
\end{Verbatim}

This will add the value(s) \(y = T Y(x)\) to the training data, where here \(Y\) represents the ``basic'' (untransformed) observations and \(y\) represents the transformed observations. This also supports the \emph{err\_y} and \emph{n} keywords. Here, \emph{err\_y} is the error on the transformed quantity \(y\). \emph{n} applies to the latent variables \(Y(x)\).

Once the GP has been populated with training data, there are two approaches supported to handle the hyperparameters.

The simplest approach is to use an empirical Bayes approach and compute the maximum a posteriori (MAP) estimate. This is accomplished using the {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.optimize_hyperparameters]{\code{optimize\_hyperparameters()}}} method of the {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} instance:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{gp}\PYG{o}{.}\PYG{n}{optimize\PYGZus{}hyperparameters}\PYG{p}{(}\PYG{p}{)}
\end{Verbatim}

This will randomly start the optimizer at points distributed according to the hyperprior several times in order to ensure that the global maximum is obtained. If you have a machine with multiple cores, these random starts will be performed in parallel. You can set the number of starts using the \emph{random\_starts} keyword, and you can set the number of processes used using the \emph{num\_proc} keyword.

For a more complete accounting of the uncertainties in the model, you can choose to use a fully Bayesian approach by using Markov chain Monte Carlo (MCMC) techniques to produce samples of the hyperposterior. The samples are produced directly with {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.sample_hyperparameter_posterior]{\code{sample\_hyperparameter\_posterior()}}}, though it will typically be more convenient to simply call {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.predict]{\code{predict()}}} with the \emph{use\_MCMC} keyword set to True.

In order to make predictions, use the {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.predict]{\code{predict()}}} method:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{y\PYGZus{}star}\PYG{p}{,} \PYG{n}{err\PYGZus{}y\PYGZus{}star} \PYG{o}{=} \PYG{n}{gp}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{x\PYGZus{}star}\PYG{p}{)}
\end{Verbatim}

By default, the mean and standard deviation of the GP posterior are returned. To compute only the mean and save some time, set the \emph{return\_std} keyword to False. To make predictions of derivatives, set the \emph{n} keyword. To make a prediction of a linearly transformed quantity, set the \emph{output\_transform} keyword.

For a convenient wrapper for applying {\hyperref[gptools:module-gptools]{\code{gptools}}} to multivariate data, see \code{profiletools} at \href{https://github.com/markchil/profiletools}{https://github.com/markchil/profiletools}


\chapter{Kernels}
\label{index:kernels}
A number of kernels are provided to allow many types of data to be fit:
\begin{itemize}
\item {} 
{\hyperref[gptools.kernel:gptools.kernel.noise.DiagonalNoiseKernel]{\code{DiagonalNoiseKernel}}} implements homoscedastic noise. The noise is tied to a specific derivative order. This allows you to, for instance, have noise on your observations but have noiseless derivative constraints, or to have different noise levels for observations and derivatives. Note that you can also specify potentially heteroscedastic noise explicitly when adding data to the process.

\item {} 
{\hyperref[gptools.kernel:gptools.kernel.squared_exponential.SquaredExponentialKernel]{\code{SquaredExponentialKernel}}} implements the SE kernel which is infinitely differentiable.

\item {} 
{\hyperref[gptools.kernel:gptools.kernel.matern.MaternKernel]{\code{MaternKernel}}} implements the entire Matern class of covariance functions, which are characterized by a hyperparameter \(\nu\). A process having the Matern kernel is only mean-square differentiable for derivative order \(n<\nu\). Note that this class does not support arbitrary derivatives at this point. If you need this feature, try using {\hyperref[gptools.kernel:gptools.kernel.matern.MaternKernelArb]{\code{MaternKernelArb}}}, but note that this is very slow!

\item {} 
{\hyperref[gptools.kernel:gptools.kernel.matern.Matern52Kernel]{\code{Matern52Kernel}}} implements a specialized Matern kernel with \(\nu=\frac{5}{2}\) which efficiently supports 0th and 1st derivatives.

\item {} 
{\hyperref[gptools.kernel:gptools.kernel.rational_quadratic.RationalQuadraticKernel]{\code{RationalQuadraticKernel}}} implements the rational quadratic kernel, which is a scale mixture over SE kernels.

\item {} 
{\hyperref[gptools.kernel:gptools.kernel.gibbs.GibbsKernel1d]{\code{GibbsKernel1d}}} and its subclasses implements the Gibbs kernel, which is a nonstationary form of the SE kernel.

\item {} 
{\hyperref[gptools.kernel:gptools.kernel.core.MaskedKernel]{\code{MaskedKernel}}} creates a kernel that only operates on a subset of dimensions. Use this along with the sum and product operations to create kernels that encode different properties in different dimensions.

\item {} 
{\hyperref[gptools.kernel:gptools.kernel.core.ArbitraryKernel]{\code{ArbitraryKernel}}} creates a kernel with an arbitrary covariance function and computes the derivatives as needed using \code{mpmath} to perform numerical differentiation. Naturally, this is very slow but is useful to let you explore the properties of arbitrary kernels without having to write a complete implementation.

\end{itemize}

In most cases, these kernels have been constructed in a way to allow inputs of arbitrary dimension. Each dimension has a length scale hyperparameter that can be separately optimized over or held fixed. Arbitrary derivatives with respect to each dimension can be taken, including computation of the covariance for those observations.

Other kernels can be implemented by extending the {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} class. Furthermore, kernels may be added or multiplied together to yield a new, valid kernel.


\chapter{Notes}
\label{index:notes}
{\hyperref[gptools:module-gptools]{\code{gptools}}} has been developed and tested on Python 2.7 and scipy 0.14.0. It may work just as well on other versions, but has not been tested.

If you find this software useful, please be sure to cite it:

M.A. Chilenski (2014). gptools: Gaussian processes with arbitrary derivative constraints and predictions, GNU General Public License. github.com/markchil/gptools

A formal publication on this software and its applications is in preparation. Once this is published, this readme will be updated with the relevant citation.


\chapter{Contents}
\label{index:contents}

\section{gptools package}
\label{gptools:gptools-package}\label{gptools::doc}

\subsection{Subpackages}
\label{gptools:subpackages}

\subsubsection{gptools.kernel package}
\label{gptools.kernel::doc}\label{gptools.kernel:gptools-kernel-package}

\paragraph{Submodules}
\label{gptools.kernel:submodules}

\paragraph{gptools.kernel.core module}
\label{gptools.kernel:gptools-kernel-core-module}\label{gptools.kernel:module-gptools.kernel.core}\index{gptools.kernel.core (module)}
Core kernel classes: contains the base {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} class and helper subclasses.
\index{Kernel (class in gptools.kernel.core)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.core.}\bfcode{Kernel}}{\emph{num\_dim=1}, \emph{num\_params=0}, \emph{initial\_params=None}, \emph{fixed\_params=None}, \emph{param\_bounds=None}, \emph{param\_names=None}, \emph{enforce\_bounds=False}, \emph{hyperprior=None}}{}
Bases: \code{object}

Covariance kernel base class. Not meant to be explicitly instantiated!

Initialize the kernel with the given number of input dimensions.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{num\_dim} : positive int
\begin{quote}

Number of dimensions of the input data. Must be consistent with the \emph{X}
and \emph{Xstar} values passed to the {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}}
you wish to use the covariance kernel with. Default is 1.
\end{quote}

\textbf{num\_params} : Non-negative int
\begin{quote}

Number of parameters in the model.
\end{quote}

\textbf{initial\_params} : \code{Array} or other Array-like, (\emph{num\_params},), optional
\begin{quote}

Initial values to set for the hyperparameters. Default is None, in
which case 1 is used for the initial values.
\end{quote}

\textbf{fixed\_params} : \code{Array} or other Array-like of bool, (\emph{num\_params},), optional
\begin{quote}

Sets which hyperparameters are considered fixed when optimizing the log
likelihood. A True entry corresponds to that element being
fixed (where the element ordering is as defined in the class).
Default value is None (no hyperparameters are fixed).
\end{quote}

\textbf{param\_bounds} : list of 2-tuples (\emph{num\_params},), optional
\begin{quote}

List of bounds for each of the hyperparameters. Each 2-tuple is of the
form (lower{}`, \emph{upper}). If there is no bound in a given direction, it
works best to set it to something big like 1e16. Default is (0.0, 1e16)
for each hyperparameter. Note that this is overridden by the \emph{hyperprior}
keyword, if present.
\end{quote}

\textbf{param\_names} : list of str (\emph{num\_params},), optional
\begin{quote}

List of labels for the hyperparameters. Default is all empty strings.
\end{quote}

\textbf{enforce\_bounds} : bool, optional
\begin{quote}

If True, an attempt to set a hyperparameter outside of its bounds will
result in the hyperparameter being set right at its bound. If False,
bounds are not enforced inside the kernel. Default is False (do not
enforce bounds).
\end{quote}

\textbf{hyperprior} : \code{JointPrior} instance or list, optional
\begin{quote}

Joint prior distribution for all hyperparameters. Can either be given
as a \code{JointPrior} instance or a list of \emph{num\_params}
callables or py:class:\emph{rv\_frozen} instances from \code{scipy.stats},
in which case a \code{IndependentJointPrior} is constructed with
these as the independent priors on each hyperparameter. Default is a
uniform PDF on all hyperparameters.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{ValueError} :
\begin{quote}

If \emph{num\_dim} is not a positive integer or the lengths of the input
vectors are inconsistent.
\end{quote}

\textbf{GPArgumentError} :
\begin{quote}

if \emph{fixed\_params} is passed but \emph{initial\_params} is not.
\end{quote}

\end{description}\end{quote}
\paragraph{Attributes}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

num\_params
 & 
int
 & 
Number of parameters
\\

num\_dim
 & 
int
 & 
Number of dimensions
\\

params
 & 
\code{Array} of float, (\emph{num\_params},)
 & 
Array of parameters.
\\

fixed\_params
 & 
\code{Array} of bool, (\emph{num\_params},)
 & 
Array of booleans indicated which parameters in params are fixed.
\\

param\_names
 & 
list of str, (\emph{num\_params},)
 & 
List of the labels for the hyperparameters.
\\

hyperprior
 & 
\code{JointPrior} instance
 & 
Joint prior distribution for the hyperparameters.
\\
\hline\end{tabulary}

\index{param\_bounds (gptools.kernel.core.Kernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.param_bounds}\pysigline{\bfcode{param\_bounds}}
\end{fulllineitems}

\index{\_\_call\_\_() (gptools.kernel.core.Kernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.

Note that this method only returns the covariance -- the hyperpriors
and potentials stored in this kernel must be applied separately.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. If None, no derivatives are taken. Default is None
(no hyperparameter derivatives).
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\end{description}\end{quote}
\paragraph{Notes}

THIS IS ONLY A METHOD STUB TO DEFINE THE NEEDED CALLING FINGERPRINT!

\end{fulllineitems}

\index{set\_hyperparams() (gptools.kernel.core.Kernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.set_hyperparams}\pysiglinewithargsret{\bfcode{set\_hyperparams}}{\emph{new\_params}}{}
Sets the free hyperparameters to the new parameter values in new\_params.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{new\_params} : \code{Array} or other Array-like, (len(\code{self.params}),)
\begin{quote}

New parameter values, ordered as dictated by the docstring for the
class.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{num\_free\_params (gptools.kernel.core.Kernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.num_free_params}\pysigline{\bfcode{num\_free\_params}}
Returns the number of free parameters.

\end{fulllineitems}

\index{free\_param\_idxs (gptools.kernel.core.Kernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.free_param_idxs}\pysigline{\bfcode{free\_param\_idxs}}
Returns the indices of the free parameters in the main arrays of parameters, etc.

\end{fulllineitems}

\index{free\_params (gptools.kernel.core.Kernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.free_params}\pysigline{\bfcode{free\_params}}
Returns the values of the free hyperparameters.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\textbf{free\_params} : \code{Array}
\begin{quote}

Array of the free parameters, in order.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{free\_param\_bounds (gptools.kernel.core.Kernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.free_param_bounds}\pysigline{\bfcode{free\_param\_bounds}}
Returns the bounds of the free hyperparameters.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\textbf{free\_param\_bounds} : \code{Array}
\begin{quote}

Array of the bounds of the free parameters, in order.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{free\_param\_names (gptools.kernel.core.Kernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.free_param_names}\pysigline{\bfcode{free\_param\_names}}
Returns the names of the free hyperparameters.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\textbf{free\_param\_names} : \code{Array}
\begin{quote}

Array of the names of the free parameters, in order.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{\_\_add\_\_() (gptools.kernel.core.Kernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.__add__}\pysiglinewithargsret{\bfcode{\_\_add\_\_}}{\emph{other}}{}
Add two Kernels together.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{other} : {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}
\begin{quote}

Kernel to be added to this one.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{sum} : {\hyperref[gptools.kernel:gptools.kernel.core.SumKernel]{\code{SumKernel}}}
\begin{quote}

Instance representing the sum of the two kernels.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{\_\_mul\_\_() (gptools.kernel.core.Kernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.__mul__}\pysiglinewithargsret{\bfcode{\_\_mul\_\_}}{\emph{other}}{}
Multiply two Kernels together.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{other} : {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}
\begin{quote}

Kernel to be multiplied by this one.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{prod} : {\hyperref[gptools.kernel:gptools.kernel.core.ProductKernel]{\code{ProductKernel}}}
\begin{quote}

Instance representing the product of the two kernels.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{BinaryKernel (class in gptools.kernel.core)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.BinaryKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.core.}\bfcode{BinaryKernel}}{\emph{k1}, \emph{k2}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{gptools.kernel.core.Kernel}}}

Abstract class for binary operations on kernels (addition, multiplication, etc.).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{k1, k2} : {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} instances to be combined

\end{description}\end{quote}
\paragraph{Notes}

\emph{k1} and \emph{k2} must have the same number of dimensions.
\index{enforce\_bounds (gptools.kernel.core.BinaryKernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.BinaryKernel.enforce_bounds}\pysigline{\bfcode{enforce\_bounds}}
Boolean indicating whether or not the kernel will explicitly enforce the bounds defined by the hyperprior.

\end{fulllineitems}

\index{fixed\_params (gptools.kernel.core.BinaryKernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.BinaryKernel.fixed_params}\pysigline{\bfcode{fixed\_params}}
\end{fulllineitems}

\index{free\_param\_bounds (gptools.kernel.core.BinaryKernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.BinaryKernel.free_param_bounds}\pysigline{\bfcode{free\_param\_bounds}}
Returns the bounds of the free hyperparameters.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\textbf{free\_param\_bounds} : \code{Array}
\begin{quote}

Array of the bounds of the free parameters, in order.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{free\_param\_names (gptools.kernel.core.BinaryKernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.BinaryKernel.free_param_names}\pysigline{\bfcode{free\_param\_names}}
Returns the names of the free hyperparameters.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\textbf{free\_param\_names} : \code{Array}
\begin{quote}

Array of the names of the free parameters, in order.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{params (gptools.kernel.core.BinaryKernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.BinaryKernel.params}\pysigline{\bfcode{params}}
\end{fulllineitems}

\index{set\_hyperparams() (gptools.kernel.core.BinaryKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.BinaryKernel.set_hyperparams}\pysiglinewithargsret{\bfcode{set\_hyperparams}}{\emph{new\_params}}{}
Set the (free) hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{new\_params} : \code{Array} or other Array-like
\begin{quote}

New values of the free parameters.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{ValueError} :
\begin{quote}

If the length of \emph{new\_params} is not consistent with \code{self.params}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{SumKernel (class in gptools.kernel.core)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.SumKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.core.}\bfcode{SumKernel}}{\emph{k1}, \emph{k2}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.BinaryKernel]{\code{gptools.kernel.core.BinaryKernel}}}

The sum of two kernels.
\index{\_\_call\_\_() (gptools.kernel.core.SumKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.SumKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{*args}, \emph{**kwargs}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If the \emph{hyper\_deriv} keyword is given and is not None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{ProductKernel (class in gptools.kernel.core)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.ProductKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.core.}\bfcode{ProductKernel}}{\emph{k1}, \emph{k2}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.BinaryKernel]{\code{gptools.kernel.core.BinaryKernel}}}

The product of two kernels.
\index{\_\_call\_\_() (gptools.kernel.core.ProductKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.ProductKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{**kwargs}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If the \emph{hyper\_deriv} keyword is given and is not None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{ChainRuleKernel (class in gptools.kernel.core)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.ChainRuleKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.core.}\bfcode{ChainRuleKernel}}{\emph{num\_dim=1}, \emph{num\_params=0}, \emph{initial\_params=None}, \emph{fixed\_params=None}, \emph{param\_bounds=None}, \emph{param\_names=None}, \emph{enforce\_bounds=False}, \emph{hyperprior=None}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{gptools.kernel.core.Kernel}}}

Abstract class for the common methods in creating kernels that require application of Faa di Bruno's formula.
\index{\_\_call\_\_() (gptools.kernel.core.ChainRuleKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.ChainRuleKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. If None, no derivatives are taken. Hyperparameter
derivatives are not supported at this point. Default is None.
\end{quote}

\textbf{symmetric} : bool
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If the \emph{hyper\_deriv} keyword is not None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{ArbitraryKernel (class in gptools.kernel.core)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.ArbitraryKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.core.}\bfcode{ArbitraryKernel}}{\emph{cov\_func}, \emph{num\_dim=1}, \emph{num\_proc=0}, \emph{num\_params=None}, \emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{gptools.kernel.core.Kernel}}}

Covariance kernel from an arbitrary covariance function.

Computes derivatives using \code{mpmath.diff()} and is hence in general
much slower than a hard-coded implementation of a given kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{num\_dim} : positive int
\begin{quote}

Number of dimensions of the input data. Must be consistent with the \emph{X}
and \emph{Xstar} values passed to the
{\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} you wish to use
the covariance kernel with.
\end{quote}

\textbf{cov\_func} : callable, takes \textgreater{}= 2 args
\begin{quote}

Covariance function. Must take arrays of \emph{Xi} and \emph{Xj} as the
first two arguments. The subsequent (scalar) arguments are the
hyperparameters. The number of parameters is found by inspection of
\emph{cov\_func} itself, or with the num\_params keyword.
\end{quote}

\textbf{num\_proc} : int or None, optional
\begin{quote}

Number of procs to use in evaluating covariance derivatives. 0 means
to do it in serial, None means to use all available cores. Default is
0 (serial evaluation).
\end{quote}

\textbf{num\_params} : int or None, optional
\begin{quote}

Number of hyperparameters. If None, inspection will be used to infer
the number of hyperparameters (but will fail if you used clever business
with {\color{red}\bfseries{}*}args, etc.). Default is None (use inspection to find argument
count).
\end{quote}

\textbf{**kwargs} :
\begin{quote}

All other keyword parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\end{description}\end{quote}
\paragraph{Attributes}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

cov\_func
 & 
callable
 & 
The covariance function
\\

num\_proc
 & 
non-negative int
 & 
Number of processors to use in evaluating covariance derivatives. 0 means serial.
\\
\hline\end{tabulary}

\index{\_\_call\_\_() (gptools.kernel.core.ArbitraryKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.ArbitraryKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. If None, no derivatives are taken. Hyperparameter
derivatives are not supported at this point. Default is None.
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If the \emph{hyper\_deriv} keyword is not None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{MaskedKernel (class in gptools.kernel.core)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.MaskedKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.core.}\bfcode{MaskedKernel}}{\emph{base, total\_dim=2, mask={[}0{]}, scale=None}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{gptools.kernel.core.Kernel}}}

Creates a kernel that is only masked to operate on certain dimensions, or has scaling/shifting.

This can be used, for instance, to put a squared exponential kernel in one
direction and a Matern kernel in the other.

Overrides {\hyperref[gptools.kernel:gptools.kernel.core.MaskedKernel.__getattribute__]{\code{\_\_getattribute\_\_()}}} and {\hyperref[gptools.kernel:gptools.kernel.core.MaskedKernel.__setattr__]{\code{\_\_setattr\_\_()}}} to make all
setting/accessing go to the \emph{base} kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{base} : {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} instance
\begin{quote}

The {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} to apply in the dimensions specified in \emph{mask}.
\end{quote}

\textbf{total\_dim} : int, optional
\begin{quote}

The total number of dimensions the masked kernel should have. Default
is 2.
\end{quote}

\textbf{mask} : list or other array-like, optional
\begin{quote}

1d list of indices of dimensions \emph{X} to include when passing to the
\emph{base} kernel. Length must be \emph{base.num\_dim}. Default is {[}0{]} (i.e.,
just pass the first column of \emph{X} to a univariate \emph{base} kernel).
\end{quote}

\textbf{scale} : list or other array-like, optional
\begin{quote}

1d list of scale factors to apply to the elements in \emph{Xi}, \emph{Xj}. Default
is ones. Length must be equal to 2{}`base.num\_dim{}`.
\end{quote}

\end{description}\end{quote}
\index{\_\_getattribute\_\_() (gptools.kernel.core.MaskedKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.MaskedKernel.__getattribute__}\pysiglinewithargsret{\bfcode{\_\_getattribute\_\_}}{\emph{name}}{}
Gets all attributes from the base kernel.

The exceptions are `base', `mask', `maskC', `num\_dim', `scale' and any
special method (i.e., a method/attribute having leading and trailing
double underscores), which are taken from {\hyperref[gptools.kernel:gptools.kernel.core.MaskedKernel]{\code{MaskedKernel}}}.

\end{fulllineitems}

\index{\_\_setattr\_\_() (gptools.kernel.core.MaskedKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.MaskedKernel.__setattr__}\pysiglinewithargsret{\bfcode{\_\_setattr\_\_}}{\emph{name}, \emph{value}}{}
Sets all attributes in the base kernel.

The exceptions are `base', `mask', `maskC', `num\_dim', `scale' and any
special method (i.e., a method/attribute having leading and trailing
double underscores), which are set in {\hyperref[gptools.kernel:gptools.kernel.core.MaskedKernel]{\code{MaskedKernel}}}.

\end{fulllineitems}

\index{\_\_call\_\_() (gptools.kernel.core.MaskedKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.MaskedKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{**kwargs}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.

Note that in the argument specifications, \emph{D} is the \emph{total\_dim}
specified in the constructor (i.e., \code{num\_dim} for the
{\hyperref[gptools.kernel:gptools.kernel.core.MaskedKernel]{\code{MaskedKernel}}} instance itself).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. If None, no derivatives are taken. Default is None
(no hyperparameter derivatives).
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{gptools.kernel.gibbs module}
\label{gptools.kernel:module-gptools.kernel.gibbs}\label{gptools.kernel:gptools-kernel-gibbs-module}\index{gptools.kernel.gibbs (module)}
Provides classes and functions for creating SE kernels with warped length scales.
\index{tanh\_warp\_arb() (in module gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.tanh_warp_arb}\pysiglinewithargsret{\code{gptools.kernel.gibbs.}\bfcode{tanh\_warp\_arb}}{\emph{X}, \emph{l1}, \emph{l2}, \emph{lw}, \emph{x0}}{}
Warps the \emph{X} coordinate with the tanh model
\begin{gather}
\begin{split}l = \frac{l_1 + l_2}{2} - \frac{l_1 - l_2}{2}\tanh\frac{x-x_0}{l_w}\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{X} : \code{Array}, (\emph{M},) or scalar float
\begin{quote}

\emph{M} locations to evaluate length scale at.
\end{quote}

\textbf{l1} : positive float
\begin{quote}

Small-\emph{X} saturation value of the length scale.
\end{quote}

\textbf{l2} : positive float
\begin{quote}

Large-\emph{X} saturation value of the length scale.
\end{quote}

\textbf{lw} : positive float
\begin{quote}

Length scale of the transition between the two length scales.
\end{quote}

\textbf{x0} : float
\begin{quote}

Location of the center of the transition between the two length scales.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{l} : \code{Array}, (\emph{M},) or scalar float
\begin{quote}

The value of the length scale at the specified point.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{gauss\_warp\_arb() (in module gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.gauss_warp_arb}\pysiglinewithargsret{\code{gptools.kernel.gibbs.}\bfcode{gauss\_warp\_arb}}{\emph{X}, \emph{l1}, \emph{l2}, \emph{lw}, \emph{x0}}{}
Warps the \emph{X} coordinate with a Gaussian-shaped divot.
\begin{gather}
\begin{split}l = l_1 - (l_1 - l_2) \exp\left ( -4\ln 2\frac{(X-x_0)^2}{l_{w}^{2}} \right )\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{X} : \code{Array}, (\emph{M},) or scalar float
\begin{quote}

\emph{M} locations to evaluate length scale at.
\end{quote}

\textbf{l1} : positive float
\begin{quote}

Global value of the length scale.
\end{quote}

\textbf{l2} : positive float
\begin{quote}

Pedestal value of the length scale.
\end{quote}

\textbf{lw} : positive float
\begin{quote}

Width of the dip.
\end{quote}

\textbf{x0} : float
\begin{quote}

Location of the center of the dip in length scale.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{l} : \code{Array}, (\emph{M},) or scalar float
\begin{quote}

The value of the length scale at the specified point.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{GibbsFunction1dArb (class in gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.GibbsFunction1dArb}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.gibbs.}\bfcode{GibbsFunction1dArb}}{\emph{warp\_function}}{}
Bases: \code{object}

Wrapper class for the Gibbs covariance function, permits the use of arbitrary warping.

The covariance function is given by
\begin{gather}
\begin{split}k = \left ( \frac{2l(x)l(x')}{l^2(x)+l^2(x')} \right )^{1/2}\exp\left ( -\frac{(x-x')^2}{l^2(x)+l^2(x')} \right )\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{warp\_function} : callable
\begin{quote}

The function that warps the length scale as a function of X. Must have
the fingerprint (\emph{Xi}, \emph{l1}, \emph{l2}, \emph{lw}, \emph{x0}).
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.kernel.gibbs.GibbsFunction1dArb method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.GibbsFunction1dArb.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{sigmaf}, \emph{l1}, \emph{l2}, \emph{lw}, \emph{x0}}{}
Evaluate the covariance function between points \emph{Xi} and \emph{Xj}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xi, Xj} : \code{Array}, \code{mpf} or scalar float
\begin{quote}

Points to evaluate covariance between. If they are \code{Array},
\code{scipy} functions are used, otherwise \code{mpmath}
functions are used.
\end{quote}

\textbf{sigmaf} : scalar float
\begin{quote}

Prefactor on covariance.
\end{quote}

\textbf{l1, l2, lw, x0} : scalar floats
\begin{quote}

Parameters of length scale warping function, passed to
\code{warp\_function}.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{k} : \code{Array} or \code{mpf}
\begin{quote}

Covariance between the given points.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{GibbsKernel1dTanhArb (class in gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.GibbsKernel1dTanhArb}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.gibbs.}\bfcode{GibbsKernel1dTanhArb}}{\emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.ArbitraryKernel]{\code{gptools.kernel.core.ArbitraryKernel}}}

Gibbs warped squared exponential covariance function in 1d.

Computes derivatives using \code{mpmath.diff()} and is hence in general
much slower than a hard-coded implementation of a given kernel.

The covariance function is given by
\begin{gather}
\begin{split}k = \left ( \frac{2l(x)l(x')}{l^2(x)+l^2(x')} \right )^{1/2}\exp\left ( -\frac{(x-x')^2}{l^2(x)+l^2(x')} \right )\end{split}\notag
\end{gather}
Warps the length scale using a hyperbolic tangent:
\begin{gather}
\begin{split}l = \frac{l_1 + l_2}{2} - \frac{l_1 - l_2}{2}\tanh\frac{x-x_0}{l_w}\end{split}\notag
\end{gather}
The order of the hyperparameters is:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigmaf
 & 
Amplitude of the covariance function
\\

1
 & 
l1
 & 
Small-X saturation value of the length scale.
\\

2
 & 
l2
 & 
Large-X saturation value of the length scale.
\\

3
 & 
lw
 & 
Length scale of the transition between the two length scales.
\\

4
 & 
x0
 & 
Location of the center of the transition between the two length scales.
\\
\hline\end{tabulary}

\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{**kwargs} :
\begin{quote}

All parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{GibbsKernel1dGaussArb (class in gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.GibbsKernel1dGaussArb}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.gibbs.}\bfcode{GibbsKernel1dGaussArb}}{\emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.ArbitraryKernel]{\code{gptools.kernel.core.ArbitraryKernel}}}

Gibbs warped squared exponential covariance function in 1d.

Computes derivatives using \code{mpmath.diff()} and is hence in general
much slower than a hard-coded implementation of a given kernel.

The covariance function is given by
\begin{gather}
\begin{split}k = \left ( \frac{2l(x)l(x')}{l^2(x)+l^2(x')} \right )^{1/2}\exp\left ( -\frac{(x-x')^2}{l^2(x)+l^2(x')} \right )\end{split}\notag
\end{gather}
Warps the length scale using a gaussian:
\begin{gather}
\begin{split}l = l_1 - (l_1 - l_2) \exp\left ( -4\ln 2\frac{(X-x_0)^2}{l_{w}^{2}} \right )\end{split}\notag
\end{gather}
The order of the hyperparameters is:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigmaf
 & 
Amplitude of the covariance function
\\

1
 & 
l1
 & 
Global value of the length scale.
\\

2
 & 
l2
 & 
Pedestal value of the length scale.
\\

3
 & 
lw
 & 
Width of the dip.
\\

4
 & 
x0
 & 
Location of the center of the dip in length scale.
\\
\hline\end{tabulary}

\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{**kwargs} :
\begin{quote}

All parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{GibbsKernel1d (class in gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.GibbsKernel1d}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.gibbs.}\bfcode{GibbsKernel1d}}{\emph{l\_func}, \emph{num\_params=None}, \emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{gptools.kernel.core.Kernel}}}

Univariate Gibbs kernel with arbitrary length scale warping for low derivatives.

The covariance function is given by
\begin{gather}
\begin{split}k = \left ( \frac{2l(x)l(x')}{l^2(x)+l^2(x')} \right )^{1/2}\exp\left ( -\frac{(x-x')^2}{l^2(x)+l^2(x')} \right )\end{split}\notag
\end{gather}
The derivatives are hard-coded using expressions obtained from Mathematica.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{l\_func} : callable
\begin{quote}

Function that dictates the length scale warping and its derivative.
Must have fingerprint (\emph{x}, \emph{n}, \emph{p1}, \emph{p2}, ...) where \emph{p1} is element
one of the kernel's parameters (i.e., element zero is skipped).
\end{quote}

\textbf{num\_params} : int, optional
\begin{quote}

The number of parameters of the length scale function. If not passed,
introspection will be used to determine this. This will fail if you have
used the *args syntax in your function definition. This count should
include sigma\_f, even though it is not passed to the length scale
function.
\end{quote}

\textbf{**kwargs} :
\begin{quote}

All remaining arguments are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.kernel.gibbs.GibbsKernel1d method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.GibbsKernel1d.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. If None, no derivatives are taken. Hyperparameter
derivatives are not supported at this point. Default is None.
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If the \emph{hyper\_deriv} keyword is not None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{tanh\_warp() (in module gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.tanh_warp}\pysiglinewithargsret{\code{gptools.kernel.gibbs.}\bfcode{tanh\_warp}}{\emph{x}, \emph{n}, \emph{l1}, \emph{l2}, \emph{lw}, \emph{x0}}{}
Implements a tanh warping function and its derivative.
\begin{gather}
\begin{split}l = \frac{l_1 + l_2}{2} - \frac{l_1 - l_2}{2}\tanh\frac{x-x_0}{l_w}\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{x} : float or array of float
\begin{quote}

Locations to evaluate the function at.
\end{quote}

\textbf{n} : int
\begin{quote}

Derivative order to take. Used for ALL of the points.
\end{quote}

\textbf{l1} : positive float
\begin{quote}

Left saturation value.
\end{quote}

\textbf{l2} : positive float
\begin{quote}

Right saturation value.
\end{quote}

\textbf{lw} : positive float
\begin{quote}

Transition width.
\end{quote}

\textbf{x0} : float
\begin{quote}

Transition location.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{l} : float or array
\begin{quote}

Warped length scale at the given locations.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If \emph{n} \textgreater{} 1.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{GibbsKernel1dTanh (class in gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.GibbsKernel1dTanh}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.gibbs.}\bfcode{GibbsKernel1dTanh}}{\emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.gibbs.GibbsKernel1d]{\code{gptools.kernel.gibbs.GibbsKernel1d}}}

Gibbs warped squared exponential covariance function in 1d.

Uses hard-coded implementation up to first derivatives.

The covariance function is given by
\begin{gather}
\begin{split}k = \left ( \frac{2l(x)l(x')}{l^2(x)+l^2(x')} \right )^{1/2}\exp\left ( -\frac{(x-x')^2}{l^2(x)+l^2(x')} \right )\end{split}\notag
\end{gather}
Warps the length scale using a hyperbolic tangent:
\begin{gather}
\begin{split}l = \frac{l_1 + l_2}{2} - \frac{l_1 - l_2}{2}\tanh\frac{x-x_0}{l_w}\end{split}\notag
\end{gather}
The order of the hyperparameters is:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigmaf
 & 
Amplitude of the covariance function
\\

1
 & 
l1
 & 
Small-X saturation value of the length scale.
\\

2
 & 
l2
 & 
Large-X saturation value of the length scale.
\\

3
 & 
lw
 & 
Length scale of the transition between the two length scales.
\\

4
 & 
x0
 & 
Location of the center of the transition between the two length scales.
\\
\hline\end{tabulary}

\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{**kwargs} :
\begin{quote}

All parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{double\_tanh\_warp() (in module gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.double_tanh_warp}\pysiglinewithargsret{\code{gptools.kernel.gibbs.}\bfcode{double\_tanh\_warp}}{\emph{x}, \emph{n}, \emph{lcore}, \emph{lmid}, \emph{ledge}, \emph{la}, \emph{lb}, \emph{xa}, \emph{xb}}{}
Implements a sum-of-tanh warping function and its derivative.
\begin{gather}
\begin{split}l = a\tanh\frac{x-x_a}{l_a} + b\tanh\frac{x-x_b}{l_b}\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{x} : float or array of float
\begin{quote}

Locations to evaluate the function at.
\end{quote}

\textbf{n} : int
\begin{quote}

Derivative order to take. Used for ALL of the points.
\end{quote}

\textbf{lcore} : float
\begin{quote}

Core length scale.
\end{quote}

\textbf{lmid} : float
\begin{quote}

Intermediate length scale.
\end{quote}

\textbf{ledge} : float
\begin{quote}

Edge length scale.
\end{quote}

\textbf{la} : positive float
\begin{quote}

Transition of first tanh.
\end{quote}

\textbf{lb} : positive float
\begin{quote}

Transition of second tanh.
\end{quote}

\textbf{xa} : float
\begin{quote}

Transition of first tanh.
\end{quote}

\textbf{xb} : float
\begin{quote}

Transition of second tanh.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{l} : float or array
\begin{quote}

Warped length scale at the given locations.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If \emph{n} \textgreater{} 1.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{GibbsKernel1dDoubleTanh (class in gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.GibbsKernel1dDoubleTanh}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.gibbs.}\bfcode{GibbsKernel1dDoubleTanh}}{\emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.gibbs.GibbsKernel1d]{\code{gptools.kernel.gibbs.GibbsKernel1d}}}

Gibbs warped squared exponential covariance function in 1d.

Uses hard-coded implementation up to first derivatives.

The covariance function is given by
\begin{gather}
\begin{split}k = \left ( \frac{2l(x)l(x')}{l^2(x)+l^2(x')} \right )^{1/2}\exp\left ( -\frac{(x-x')^2}{l^2(x)+l^2(x')} \right )\end{split}\notag
\end{gather}
Warps the length scale using two hyperbolic tangents:
\begin{gather}
\begin{split}l = a\tanh\frac{x-x_a}{l_a} + b\tanh\frac{x-x_b}{l_b}\end{split}\notag
\end{gather}
The order of the hyperparameters is:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigmaf
 & 
Amplitude of the covariance function
\\

1
 & 
lcore
 & 
Core length scale
\\

2
 & 
lmid
 & 
Intermediate length scale
\\

3
 & 
ledge
 & 
Edge length scale
\\

4
 & 
la
 & 
Width of first tanh
\\

5
 & 
lb
 & 
Width of second tanh
\\

6
 & 
xa
 & 
Center of first tanh
\\

7
 & 
xb
 & 
Center of second tanh
\\
\hline\end{tabulary}

\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{**kwargs} :
\begin{quote}

All parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{cubic\_bucket\_warp() (in module gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.cubic_bucket_warp}\pysiglinewithargsret{\code{gptools.kernel.gibbs.}\bfcode{cubic\_bucket\_warp}}{\emph{x}, \emph{n}, \emph{l1}, \emph{l2}, \emph{l3}, \emph{x0}, \emph{w1}, \emph{w2}, \emph{w3}}{}
Warps the length scale with a piecewise cubic ``bucket'' shape.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{x} : float or array-like of float
\begin{quote}

Locations to evaluate length scale at.
\end{quote}

\textbf{n} : non-negative int
\begin{quote}

Derivative order to evaluate. Only first derivatives are supported.
\end{quote}

\textbf{l1} : positive float
\begin{quote}

Length scale to the left of the bucket.
\end{quote}

\textbf{l2} : positive float
\begin{quote}

Length scale in the bucket.
\end{quote}

\textbf{l3} : positive float
\begin{quote}

Length scale to the right of the bucket.
\end{quote}

\textbf{x0} : float
\begin{quote}

Location of the center of the bucket.
\end{quote}

\textbf{w1} : positive float
\begin{quote}

Width of the left side cubic section.
\end{quote}

\textbf{w2} : positive float
\begin{quote}

Width of the bucket.
\end{quote}

\textbf{w3} : positive float
\begin{quote}

Width of the right side cubic section.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{GibbsKernel1dCubicBucket (class in gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.GibbsKernel1dCubicBucket}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.gibbs.}\bfcode{GibbsKernel1dCubicBucket}}{\emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.gibbs.GibbsKernel1d]{\code{gptools.kernel.gibbs.GibbsKernel1d}}}

Gibbs warped squared exponential covariance function in 1d.

Uses hard-coded implementation up to first derivatives.

The covariance function is given by
\begin{gather}
\begin{split}k = \left ( \frac{2l(x)l(x')}{l^2(x)+l^2(x')} \right )^{1/2}\exp\left ( -\frac{(x-x')^2}{l^2(x)+l^2(x')} \right )\end{split}\notag
\end{gather}
Warps the length scale using a ``bucket'' function with cubic joins.

The order of the hyperparameters is:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigmaf
 & 
Amplitude of the covariance function
\\

1
 & 
l1
 & 
Length scale to the left of the bucket.
\\

2
 & 
l2
 & 
Length scale in the bucket.
\\

3
 & 
l3
 & 
Length scale to the right of the bucket.
\\

4
 & 
x0
 & 
Location of the center of the bucket.
\\

5
 & 
w1
 & 
Width of the left side cubic section.
\\

6
 & 
w2
 & 
Width of the bucket.
\\

7
 & 
w3
 & 
Width of the right side cubic section.
\\
\hline\end{tabulary}

\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{**kwargs} :
\begin{quote}

All parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{quintic\_bucket\_warp() (in module gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.quintic_bucket_warp}\pysiglinewithargsret{\code{gptools.kernel.gibbs.}\bfcode{quintic\_bucket\_warp}}{\emph{x}, \emph{n}, \emph{l1}, \emph{l2}, \emph{l3}, \emph{x0}, \emph{w1}, \emph{w2}, \emph{w3}}{}
Warps the length scale with a piecewise quintic ``bucket'' shape.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{x} : float or array-like of float
\begin{quote}

Locations to evaluate length scale at.
\end{quote}

\textbf{n} : non-negative int
\begin{quote}

Derivative order to evaluate. Only first derivatives are supported.
\end{quote}

\textbf{l1} : positive float
\begin{quote}

Length scale to the left of the bucket.
\end{quote}

\textbf{l2} : positive float
\begin{quote}

Length scale in the bucket.
\end{quote}

\textbf{l3} : positive float
\begin{quote}

Length scale to the right of the bucket.
\end{quote}

\textbf{x0} : float
\begin{quote}

Location of the center of the bucket.
\end{quote}

\textbf{w1} : positive float
\begin{quote}

Width of the left side quintic section.
\end{quote}

\textbf{w2} : positive float
\begin{quote}

Width of the bucket.
\end{quote}

\textbf{w3} : positive float
\begin{quote}

Width of the right side quintic section.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{GibbsKernel1dQuinticBucket (class in gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.GibbsKernel1dQuinticBucket}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.gibbs.}\bfcode{GibbsKernel1dQuinticBucket}}{\emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.gibbs.GibbsKernel1d]{\code{gptools.kernel.gibbs.GibbsKernel1d}}}

Gibbs warped squared exponential covariance function in 1d.

Uses hard-coded implementation up to first derivatives.

The covariance function is given by
\begin{gather}
\begin{split}k = \left ( \frac{2l(x)l(x')}{l^2(x)+l^2(x')} \right )^{1/2}\exp\left ( -\frac{(x-x')^2}{l^2(x)+l^2(x')} \right )\end{split}\notag
\end{gather}
Warps the length scale using a ``bucket'' function with quintic joins.

The order of the hyperparameters is:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigmaf
 & 
Amplitude of the covariance function
\\

1
 & 
l1
 & 
Length scale to the left of the bucket.
\\

2
 & 
l2
 & 
Length scale in the bucket.
\\

3
 & 
l3
 & 
Length scale to the right of the bucket.
\\

4
 & 
x0
 & 
Location of the center of the bucket.
\\

5
 & 
w1
 & 
Width of the left side quintic section.
\\

6
 & 
w2
 & 
Width of the bucket.
\\

7
 & 
w3
 & 
Width of the right side quintic section.
\\
\hline\end{tabulary}

\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{**kwargs} :
\begin{quote}

All parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}



\paragraph{gptools.kernel.matern module}
\label{gptools.kernel:gptools-kernel-matern-module}\label{gptools.kernel:module-gptools.kernel.matern}\index{gptools.kernel.matern (module)}
Provides the {\hyperref[gptools.kernel:gptools.kernel.matern.MaternKernel]{\code{MaternKernel}}} class which implements the anisotropic Matern kernel.
\index{matern\_function() (in module gptools.kernel.matern)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.matern.matern_function}\pysiglinewithargsret{\code{gptools.kernel.matern.}\bfcode{matern\_function}}{\emph{Xi}, \emph{Xj}, \emph{*args}}{}
Matern covariance function of arbitrary dimension, for use with \code{ArbitraryKernel}.

The Matern kernel has the following hyperparameters, always referenced in
the order listed:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigma
 & 
prefactor
\\

1
 & 
nu
 & 
order of kernel
\\

2
 & 
l1
 & 
length scale for the first dimension
\\

3
 & 
l2
 & 
...and so on for all dimensions
\\
\hline\end{tabulary}


The kernel is defined as:
\begin{gather}
\begin{split}k_M = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}
\left (\sqrt{2\nu \sum_i\left (\frac{\tau_i^2}{l_i^2}\right )}\right )^\nu
K_\nu\left(\sqrt{2\nu \sum_i\left(\frac{\tau_i^2}{l_i^2}\right)}\right)\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xi, Xj} : \code{Array}, \code{mpf}, tuple or scalar float
\begin{quote}

Points to evaluate the covariance between. If they are \code{Array},
\code{scipy} functions are used, otherwise \code{mpmath}
functions are used.
\end{quote}

\textbf{*args} :
\begin{quote}

Remaining arguments are the 2+num\_dim hyperparameters as defined above.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{MaternKernelArb (class in gptools.kernel.matern)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.matern.MaternKernelArb}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.matern.}\bfcode{MaternKernelArb}}{\emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.ArbitraryKernel]{\code{gptools.kernel.core.ArbitraryKernel}}}

Matern covariance kernel. Supports arbitrary derivatives. Treats order as a hyperparameter.

This version of the Matern kernel is painfully slow, but uses \code{mpmath}
to ensure the derivatives are computed properly, since there may be issues
with the regular {\hyperref[gptools.kernel:gptools.kernel.matern.MaternKernel]{\code{MaternKernel}}}.

The Matern kernel has the following hyperparameters, always referenced in
the order listed:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigma
 & 
prefactor
\\

1
 & 
nu
 & 
order of kernel
\\

2
 & 
l1
 & 
length scale for the first dimension
\\

3
 & 
l2
 & 
...and so on for all dimensions
\\
\hline\end{tabulary}


The kernel is defined as:
\begin{gather}
\begin{split}k_M = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}
\left (\sqrt{2\nu \sum_i\left (\frac{\tau_i^2}{l_i^2}\right )}\right )^\nu
K_\nu\left(\sqrt{2\nu \sum_i\left(\frac{\tau_i^2}{l_i^2}\right)}\right)\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{**kwargs} :
\begin{quote}

All keyword parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.ArbitraryKernel]{\code{ArbitraryKernel}}}.
\end{quote}

\end{description}\end{quote}
\index{nu (gptools.kernel.matern.MaternKernelArb attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.matern.MaternKernelArb.nu}\pysigline{\bfcode{nu}}
Returns the value of the order \(\nu\).

\end{fulllineitems}


\end{fulllineitems}

\index{MaternKernel1d (class in gptools.kernel.matern)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.matern.MaternKernel1d}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.matern.}\bfcode{MaternKernel1d}}{\emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{gptools.kernel.core.Kernel}}}

Matern covariance kernel. Only for univariate data. Only supports up to first derivatives. Treats order as a hyperparameter.

The Matern kernel has the following hyperparameters, always referenced in
the order listed:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigma
 & 
prefactor
\\

1
 & 
nu
 & 
order of kernel
\\

2
 & 
l1
 & 
length scale
\\
\hline\end{tabulary}


The kernel is defined as:
\begin{gather}
\begin{split}k_M = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}
\left (\sqrt{2\nu \left (\frac{\tau^2}{l_1^2}\right )}\right )^\nu
K_\nu\left(\sqrt{2\nu \left(\frac{\tau^2}{l_1^2}\right)}\right)\end{split}\notag
\end{gather}
where \(\tau=X_i-X_j\).

Note that the expressions for the derivatives break down for \(\nu < 1\).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{**kwargs} :
\begin{quote}

All keyword parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.kernel.matern.MaternKernel1d method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.matern.MaternKernel1d.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. If None, no derivatives are taken. Hyperparameter
derivatives are not supported at this point. Default is None.
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If the \emph{hyper\_deriv} keyword is not None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{MaternKernel (class in gptools.kernel.matern)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.matern.MaternKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.matern.}\bfcode{MaternKernel}}{\emph{num\_dim=1}, \emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.ChainRuleKernel]{\code{gptools.kernel.core.ChainRuleKernel}}}

Matern covariance kernel. Supports arbitrary derivatives. Treats order as a hyperparameter.

EXPERIMENTAL IMPLEMENTATION! DO NOT TRUST FOR HIGHER DERIVATIVES!

The Matern kernel has the following hyperparameters, always referenced in
the order listed:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigma
 & 
prefactor
\\

1
 & 
nu
 & 
order of kernel
\\

2
 & 
l1
 & 
length scale for the first dimension
\\

3
 & 
l2
 & 
...and so on for all dimensions
\\
\hline\end{tabulary}


The kernel is defined as:
\begin{gather}
\begin{split}k_M = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}
\left (\sqrt{2\nu \sum_i\left (\frac{\tau_i^2}{l_i^2}\right )}\right )^\nu
K_\nu\left(\sqrt{2\nu \sum_i\left(\frac{\tau_i^2}{l_i^2}\right)}\right)\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{num\_dim} : int
\begin{quote}

Number of dimensions of the input data. Must be consistent with the \emph{X}
and \emph{Xstar} values passed to the {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}}
you wish to use the covariance kernel with.
\end{quote}

\textbf{**kwargs} :
\begin{quote}

All keyword parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.ChainRuleKernel]{\code{ChainRuleKernel}}}.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{ValueError} :
\begin{quote}

If \emph{num\_dim} is not a positive integer or the lengths of the input
vectors are inconsistent.
\end{quote}

\textbf{GPArgumentError} :
\begin{quote}

If \emph{fixed\_params} is passed but \emph{initial\_params} is not.
\end{quote}

\end{description}\end{quote}
\index{nu (gptools.kernel.matern.MaternKernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.matern.MaternKernel.nu}\pysigline{\bfcode{nu}}
Returns the value of the order \(\nu\).

\end{fulllineitems}


\end{fulllineitems}

\index{Matern52Kernel (class in gptools.kernel.matern)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.matern.Matern52Kernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.matern.}\bfcode{Matern52Kernel}}{\emph{num\_dim=1}, \emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{gptools.kernel.core.Kernel}}}

Matern 5/2 covariance kernel. Supports only 0th and 1st derivatives
and is fixed at nu=5/2. Because of these limitations, it is quite a bit
faster than the more general Matern kernels.

The Matern52 kernel has the following hyperparameters, always referenced in
the order listed:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigma
 & 
prefactor
\\

1
 & 
l1
 & 
length scale for the first dimension
\\

2
 & 
l2
 & 
...and so on for all dimensions
\\
\hline\end{tabulary}


The kernel is defined as:
\begin{gather}
\begin{split}k_M(x, x') = \sigma^2 \left(1 + \sqrt{5r^2} + \frac{5}{3}r^2\right) \exp(-\sqrt{5r^2}) \\
r^2 = \sum_{d=1}^D \frac{(x_d - x'_d)^2}{l_d^2}\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{num\_dim} : int
\begin{quote}

Number of dimensions of the input data. Must be consistent with the \emph{X}
and \emph{Xstar} values passed to the {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}}
you wish to use the covariance kernel with.
\end{quote}

\textbf{**kwargs} :
\begin{quote}

All keyword parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{ValueError} :
\begin{quote}

If \emph{num\_dim} is not a positive integer or the lengths of the input
vectors are inconsistent.
\end{quote}

\textbf{GPArgumentError} :
\begin{quote}

If \emph{fixed\_params} is passed but \emph{initial\_params} is not.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.kernel.matern.Matern52Kernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.matern.Matern52Kernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. If None, no derivatives are taken. Hyperparameter
derivatives are not supported at this point. Default is None.
\end{quote}

\textbf{symmetric} : bool
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If the \emph{hyper\_deriv} keyword is not None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{gptools.kernel.noise module}
\label{gptools.kernel:module-gptools.kernel.noise}\label{gptools.kernel:gptools-kernel-noise-module}\index{gptools.kernel.noise (module)}
Provides classes for implementing uncorrelated noise.
\index{DiagonalNoiseKernel (class in gptools.kernel.noise)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.noise.DiagonalNoiseKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.noise.}\bfcode{DiagonalNoiseKernel}}{\emph{num\_dim=1}, \emph{initial\_noise=None}, \emph{fixed\_noise=False}, \emph{noise\_bound=None}, \emph{n=0}, \emph{hyperprior=None}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{gptools.kernel.core.Kernel}}}

Kernel that has constant, independent noise (i.e., a diagonal kernel).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{num\_dim} : positive int
\begin{quote}

Number of dimensions of the input data.
\end{quote}

\textbf{initial\_noise} : float, optional
\begin{quote}

Initial value for the noise standard deviation. Default value is None
(noise gets set to 1).
\end{quote}

\textbf{fixed\_noise} : bool, optional
\begin{quote}

Whether or not the noise is taken to be fixed when optimizing the log
likelihood. Default is False (noise is not fixed).
\end{quote}

\textbf{noise\_bound} : 2-tuple, optional
\begin{quote}

The bounds for the noise when optimizing the log likelihood with
\code{scipy.optimize.minimize()}. Must be of the form
(\emph{lower}, \emph{upper}). Set a given entry to None to put no bound on
that side. Default is None, which gets set to (0, None).
\end{quote}

\textbf{n} : non-negative int or tuple of non-negative ints with length equal to \emph{num\_dim}, optional
\begin{quote}

Indicates which derivative this noise is with respect to. Default is 0
(noise applies to value).
\end{quote}

\textbf{hyperprior} : callable, optional
\begin{quote}

Function that returns the prior log-density for a possible value of
noise when called. Must also have an attribute called \code{bounds}
that is the bounds on the noise and a method called
\code{random\_draw()} that yields a random draw. Default behavior is
to assign a uniform prior.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.kernel.noise.DiagonalNoiseKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.noise.DiagonalNoiseKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. Since this kernel only has one hyperparameter, 0
is the only valid value. If None, no derivatives are taken. Default
is None (no hyperparameter derivatives).
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{ZeroKernel (class in gptools.kernel.noise)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.noise.ZeroKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.noise.}\bfcode{ZeroKernel}}{\emph{num\_dim=1}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.noise.DiagonalNoiseKernel]{\code{gptools.kernel.noise.DiagonalNoiseKernel}}}

Kernel that always evaluates to zero, used as the default noise kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{num\_dim} : positive int
\begin{quote}

The number of dimensions of the inputs.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.kernel.noise.ZeroKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.noise.ZeroKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
Return zeros the same length as the input Xi.

Ignores all other arguments.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. Since this kernel only has one hyperparameter, 0
is the only valid value. If None, no derivatives are taken. Default
is None (no hyperparameter derivatives).
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{gptools.kernel.rational\_quadratic module}
\label{gptools.kernel:gptools-kernel-rational-quadratic-module}\label{gptools.kernel:module-gptools.kernel.rational_quadratic}\index{gptools.kernel.rational\_quadratic (module)}
Provides the {\hyperref[gptools.kernel:gptools.kernel.rational_quadratic.RationalQuadraticKernel]{\code{RationalQuadraticKernel}}} class which implements the anisotropic rational quadratic (RQ) kernel.
\index{RationalQuadraticKernel (class in gptools.kernel.rational\_quadratic)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.rational_quadratic.RationalQuadraticKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.rational\_quadratic.}\bfcode{RationalQuadraticKernel}}{\emph{num\_dim=1}, \emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.ChainRuleKernel]{\code{gptools.kernel.core.ChainRuleKernel}}}

Rational quadratic (RQ) covariance kernel. Supports arbitrary derivatives.

The RQ kernel has the following hyperparameters, always referenced
in the order listed:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigma
 & 
prefactor.
\\

1
 & 
alpha
 & 
order of kernel.
\\

2
 & 
l1
 & 
length scale for the first dimension.
\\

3
 & 
l2
 & 
...and so on for all dimensions.
\\
\hline\end{tabulary}


The kernel is defined as:
\begin{gather}
\begin{split}k_{RQ} = \sigma^2 \left(1 + \frac{1}{2\alpha} \sum_i\frac{\tau_i^2}{l_i^2}\right)^{-\alpha}\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{num\_dim} : int
\begin{quote}

Number of dimensions of the input data. Must be consistent
with the \emph{X} and \emph{Xstar} values passed to the
{\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} you
wish to use the covariance kernel with.
\end{quote}

\textbf{**kwargs} :
\begin{quote}

All keyword parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.ChainRuleKernel]{\code{ChainRuleKernel}}}.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{ValueError} :
\begin{quote}

If \emph{num\_dim} is not a positive integer or the lengths of
the input vectors are inconsistent.
\end{quote}

\textbf{GPArgumentError} :
\begin{quote}

If \emph{fixed\_params} is passed but \emph{initial\_params} is not.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}



\paragraph{gptools.kernel.squared\_exponential module}
\label{gptools.kernel:module-gptools.kernel.squared_exponential}\label{gptools.kernel:gptools-kernel-squared-exponential-module}\index{gptools.kernel.squared\_exponential (module)}
Provides the {\hyperref[gptools.kernel:gptools.kernel.squared_exponential.SquaredExponentialKernel]{\code{SquaredExponentialKernel}}} class that implements the anisotropic SE kernel.
\index{SquaredExponentialKernel (class in gptools.kernel.squared\_exponential)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.squared_exponential.SquaredExponentialKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.squared\_exponential.}\bfcode{SquaredExponentialKernel}}{\emph{num\_dim=1}, \emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{gptools.kernel.core.Kernel}}}

Squared exponential covariance kernel. Supports arbitrary derivatives.

The squared exponential has the following hyperparameters, always
referenced in the order listed:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigma
 & 
prefactor on the SE
\\

1
 & 
l1
 & 
length scale for the first dimension
\\

2
 & 
l2
 & 
...and so on for all dimensions
\\
\hline\end{tabulary}


The kernel is defined as:
\begin{gather}
\begin{split}k_{SE} = \sigma^2 \exp\left(-\frac{1}{2}\sum_i\frac{\tau_i^2}{l_i^2}\right)\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{num\_dim} : int
\begin{quote}

Number of dimensions of the input data. Must be consistent
with the \emph{X} and \emph{Xstar} values passed to the
{\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} you
wish to use the covariance kernel with.
\end{quote}

\textbf{**kwargs} :
\begin{quote}

All keyword parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{ValueError} :
\begin{quote}

If \emph{num\_dim} is not a positive integer or the lengths of
the input vectors are inconsistent.
\end{quote}

\textbf{GPArgumentError} :
\begin{quote}

If \emph{fixed\_params} is passed but \emph{initial\_params} is not.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.kernel.squared\_exponential.SquaredExponentialKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.squared_exponential.SquaredExponentialKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} inputs with dimension \emph{D}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. If None, no derivatives are taken. Default is None
(no hyperparameter derivatives). Hyperparameter derivatives are not
support for \emph{n} \textgreater{} 0 at this time.
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If hyper\_deriv is not None and \emph{n} \textgreater{} 0.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{gptools.kernel.warping module}
\label{gptools.kernel:module-gptools.kernel.warping}\label{gptools.kernel:gptools-kernel-warping-module}\index{gptools.kernel.warping (module)}
Classes and functions to warp inputs to kernels to enable fitting of
nonstationary data. Note that this accomplishes a similar goal as the Gibbs
kernel (which is a nonstationary version of the squared exponential kernel), but
with the difference that the warpings in this module can be applied to any
existing kernel. Furthermore, whereas the Gibbs kernel implements
nonstationarity by changing the length scale of the covariance function, the
warpings in the module act by transforming the input values directly.

The module contains two core classes that work together.
{\hyperref[gptools.kernel:gptools.kernel.warping.WarpingFunction]{\code{WarpingFunction}}} gives you a way to wrap a given function in a way
that allows you to optimize/integrate over the hyperparameters that describe the
warping. {\hyperref[gptools.kernel:gptools.kernel.warping.WarpedKernel]{\code{WarpedKernel}}} is an extension of the \code{Kernel} base
class and is how you apply a {\hyperref[gptools.kernel:gptools.kernel.warping.WarpingFunction]{\code{WarpingFunction}}} to whatever kernel you
want to warp.

Two useful warpings have been implemented at this point:
{\hyperref[gptools.kernel:gptools.kernel.warping.BetaWarpedKernel]{\code{BetaWarpedKernel}}} warps the inputs using the CDF of the beta
distribution (i.e., the regularized incomplete beta function). This requires
that the starting inputs be constrained to the unit hypercube {[}0, 1{]}. In order
to get arbitrary data to this form, {\hyperref[gptools.kernel:gptools.kernel.warping.LinearWarpedKernel]{\code{LinearWarpedKernel}}} allows you to
apply a linear transformation based on the known bounds of your data.

So, for example, to make a beta-warped squared exponential kernel, you simply type:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{k\PYGZus{}SE} \PYG{o}{=} \PYG{n}{gptools}\PYG{o}{.}\PYG{n}{SquaredExponentialKernel}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{k\PYGZus{}SE\PYGZus{}beta} \PYG{o}{=} \PYG{n}{gptools}\PYG{o}{.}\PYG{n}{BetaWarpedKernel}\PYG{p}{(}\PYG{n}{k\PYGZus{}SE}\PYG{p}{)}
\end{Verbatim}

Furthermore, if your inputs \emph{X} are not confined to the unit hypercube {[}0, 1{]},
you should use a linear transformation to map them to it:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{k\PYGZus{}SE\PYGZus{}beta\PYGZus{}unit} \PYG{o}{=} \PYG{n}{gptools}\PYG{o}{.}\PYG{n}{LinearWarpedKernel}\PYG{p}{(}\PYG{n}{k\PYGZus{}SE\PYGZus{}beta}\PYG{p}{,} \PYG{n}{X}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{n}{X}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{WarpingFunction (class in gptools.kernel.warping)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpingFunction}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.warping.}\bfcode{WarpingFunction}}{\emph{fun}, \emph{num\_dim=1}, \emph{num\_params=None}, \emph{initial\_params=None}, \emph{fixed\_params=None}, \emph{param\_bounds=None}, \emph{param\_names=None}, \emph{enforce\_bounds=False}, \emph{hyperprior=None}}{}
Bases: \code{object}

Wrapper to interface a function with {\hyperref[gptools.kernel:gptools.kernel.warping.WarpedKernel]{\code{WarpedKernel}}}.

This lets you define a simple function \emph{fun(X, d, n, p1, p2, ...)} that
operates on one dimension of \emph{X} at a time and has several hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{fun} : callable
\begin{quote}

Must have fingerprint \emph{fun(X, d, n, p1, p2, ...)} where \emph{X} is an array
of length \emph{M}, \emph{d} is the index of the dimension \emph{X} is from, \emph{n} is a
non-negative integer representing the order of derivative to take and
\emph{p1}, \emph{p2}, ... are the parameters of the warping. Note that this form
assumes that the warping is applied independently to each dimension.
\end{quote}

\textbf{num\_dim} : positive int, optional
\begin{quote}

Number of dimensions of the input data. Must be consistent with the \emph{X}
and \emph{Xstar} values passed to the
{\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} you wish to use
the warping function with. Default is 1.
\end{quote}

\textbf{num\_params} : Non-negative int, optional
\begin{quote}

Number of parameters in the model. Default is to determine the number of
parameters by inspection of \emph{fun} or the other arguments provided.
\end{quote}

\textbf{initial\_params} : Array, (\emph{num\_params},), optional
\begin{quote}

Initial values to set for the hyperparameters. Default is None, in
which case 1 is used for the initial values.
\end{quote}

\textbf{fixed\_params} : Array of bool, (\emph{num\_params},), optional
\begin{quote}

Sets which hyperparameters are considered fixed when optimizing the log
likelihood. A True entry corresponds to that element being
fixed (where the element ordering is as defined in the class).
Default value is None (no hyperparameters are fixed).
\end{quote}

\textbf{param\_bounds} : list of 2-tuples (\emph{num\_params},), optional
\begin{quote}

List of bounds for each of the hyperparameters. Each 2-tuple is of the
form (lower{}`, \emph{upper}). If there is no bound in a given direction, it
works best to set it to something big like 1e16. Default is (0.0, 1e16)
for each hyperparameter. Note that this is overridden by the \emph{hyperprior}
keyword, if present.
\end{quote}

\textbf{param\_names} : list of str (\emph{num\_params},), optional
\begin{quote}

List of labels for the hyperparameters. Default is all empty strings.
\end{quote}

\textbf{enforce\_bounds} : bool, optional
\begin{quote}

If True, an attempt to set a hyperparameter outside of its bounds will
result in the hyperparameter being set right at its bound. If False,
bounds are not enforced inside the kernel. Default is False (do not
enforce bounds).
\end{quote}

\textbf{hyperprior} : \code{JointPrior} instance or list, optional
\begin{quote}

Joint prior distribution for all hyperparameters. Can either be given
as a \code{JointPrior} instance or a list of \emph{num\_params}
callables or \code{rv\_frozen} instances from \code{scipy.stats},
in which case a \code{IndependentJointPrior} is constructed with
these as the independent priors on each hyperparameter. Default is a
uniform PDF on all hyperparameters.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.kernel.warping.WarpingFunction method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpingFunction.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{X}, \emph{d}, \emph{n}}{}
Evaluate the warping function.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{X} : Array, (\emph{M},)
\begin{quote}

\emph{M} inputs corresponding to dimension \emph{d}.
\end{quote}

\textbf{d} : non-negative int
\begin{quote}

Index of the dimension that \emph{X} is from.
\end{quote}

\textbf{n} : non-negative int
\begin{quote}

Derivative order to compute.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{param\_bounds (gptools.kernel.warping.WarpingFunction attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpingFunction.param_bounds}\pysigline{\bfcode{param\_bounds}}
\end{fulllineitems}

\index{set\_hyperparams() (gptools.kernel.warping.WarpingFunction method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpingFunction.set_hyperparams}\pysiglinewithargsret{\bfcode{set\_hyperparams}}{\emph{new\_params}}{}
Sets the free hyperparameters to the new parameter values in new\_params.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{new\_params} : \code{Array} or other Array-like, (len(\code{self.params}),)
\begin{quote}

New parameter values, ordered as dictated by the docstring for the
class.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{num\_free\_params (gptools.kernel.warping.WarpingFunction attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpingFunction.num_free_params}\pysigline{\bfcode{num\_free\_params}}
Returns the number of free parameters.

\end{fulllineitems}

\index{free\_param\_idxs (gptools.kernel.warping.WarpingFunction attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpingFunction.free_param_idxs}\pysigline{\bfcode{free\_param\_idxs}}
Returns the indices of the free parameters in the main arrays of parameters, etc.

\end{fulllineitems}

\index{free\_params (gptools.kernel.warping.WarpingFunction attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpingFunction.free_params}\pysigline{\bfcode{free\_params}}
Returns the values of the free hyperparameters.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\textbf{free\_params} : \code{Array}
\begin{quote}

Array of the free parameters, in order.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{free\_param\_bounds (gptools.kernel.warping.WarpingFunction attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpingFunction.free_param_bounds}\pysigline{\bfcode{free\_param\_bounds}}
Returns the bounds of the free hyperparameters.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\textbf{free\_param\_bounds} : \code{Array}
\begin{quote}

Array of the bounds of the free parameters, in order.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{free\_param\_names (gptools.kernel.warping.WarpingFunction attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpingFunction.free_param_names}\pysigline{\bfcode{free\_param\_names}}
Returns the names of the free hyperparameters.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\textbf{free\_param\_names} : \code{Array}
\begin{quote}

Array of the names of the free parameters, in order.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{beta\_cdf\_warp() (in module gptools.kernel.warping)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.beta_cdf_warp}\pysiglinewithargsret{\code{gptools.kernel.warping.}\bfcode{beta\_cdf\_warp}}{\emph{X}, \emph{d}, \emph{n}, \emph{*args}}{}
Warp inputs that are confined to the unit hypercube using the regularized incomplete beta function.

Applies separately to each dimension, designed for use with
{\hyperref[gptools.kernel:gptools.kernel.warping.WarpingFunction]{\code{WarpingFunction}}}.

Assumes that your inputs \emph{X} lie entirely within the unit hypercube {[}0, 1{]}.

Note that you may experience some issues with constraining and computing
derivatives at \(x=0\) when \(\alpha < 1\) and at \(x=1\) when
\(\beta < 1\). As a workaround, try mapping your data to not touch the
boundaries of the unit hypercube.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{X} : array, (\emph{M},)
\begin{quote}

\emph{M} inputs from dimension \emph{d}.
\end{quote}

\textbf{d} : non-negative int
\begin{quote}

The index (starting from zero) of the dimension to apply the warping to.
\end{quote}

\textbf{n} : non-negative int
\begin{quote}

The derivative order to compute.
\end{quote}

\textbf{*args} : 2N scalars
\begin{quote}

The remaining parameters to describe the warping, given as scalars.
These are given as \emph{alpha\_i}, \emph{beta\_i} for each of the \emph{D} dimensions.
Note that these must ALL be provided for each call.
\end{quote}

\end{description}\end{quote}
\paragraph{References}

{\hyperref[gptools.kernel:r1]{{[}R1{]}}}

\end{fulllineitems}

\index{linear\_warp() (in module gptools.kernel.warping)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.linear_warp}\pysiglinewithargsret{\code{gptools.kernel.warping.}\bfcode{linear\_warp}}{\emph{X}, \emph{d}, \emph{n}, \emph{*args}}{}
Warp inputs with a linear transformation.

Applies the warping
\begin{gather}
\begin{split}w(x) = \frac{x-a}{b-a}\end{split}\notag
\end{gather}
to each dimension. If you set \emph{a=min(X)} and \emph{b=max(X)} then this is a
convenient way to map your inputs to the unit hypercube.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{X} : array, (\emph{M},)
\begin{quote}

\emph{M} inputs from dimension \emph{d}.
\end{quote}

\textbf{d} : non-negative int
\begin{quote}

The index (starting from zero) of the dimension to apply the warping to.
\end{quote}

\textbf{n} : non-negative int
\begin{quote}

The derivative order to compute.
\end{quote}

\textbf{*args} : 2N scalars
\begin{quote}

The remaining parameters to describe the warping, given as scalars.
These are given as \emph{a\_i}, \emph{b\_i} for each of the \emph{D} dimensions. Note
that these must ALL be provided for each call.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{WarpedKernel (class in gptools.kernel.warping)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpedKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.warping.}\bfcode{WarpedKernel}}{\emph{k}, \emph{w}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{gptools.kernel.core.Kernel}}}

Kernel which has had its inputs warped through a basic, elementwise warping function.

In other words, takes \(k(x_1, x_2, x'_1, x'_2)\) and turns it into
\(k(w_1(x_1), w_2(x_2), w_1(x'_1), w_2(x'_2))\).
\index{\_\_call\_\_() (gptools.kernel.warping.WarpedKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpedKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
\end{fulllineitems}

\index{enforce\_bounds (gptools.kernel.warping.WarpedKernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpedKernel.enforce_bounds}\pysigline{\bfcode{enforce\_bounds}}
Boolean indicating whether or not the kernel will explicitly enforce the bounds defined by the hyperprior.

\end{fulllineitems}

\index{fixed\_params (gptools.kernel.warping.WarpedKernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpedKernel.fixed_params}\pysigline{\bfcode{fixed\_params}}
\end{fulllineitems}

\index{params (gptools.kernel.warping.WarpedKernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpedKernel.params}\pysigline{\bfcode{params}}
\end{fulllineitems}

\index{param\_names (gptools.kernel.warping.WarpedKernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpedKernel.param_names}\pysigline{\bfcode{param\_names}}
\end{fulllineitems}

\index{free\_params (gptools.kernel.warping.WarpedKernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpedKernel.free_params}\pysigline{\bfcode{free\_params}}
\end{fulllineitems}

\index{free\_param\_bounds (gptools.kernel.warping.WarpedKernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpedKernel.free_param_bounds}\pysigline{\bfcode{free\_param\_bounds}}
\end{fulllineitems}

\index{free\_param\_names (gptools.kernel.warping.WarpedKernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpedKernel.free_param_names}\pysigline{\bfcode{free\_param\_names}}
\end{fulllineitems}

\index{set\_hyperparams() (gptools.kernel.warping.WarpedKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.WarpedKernel.set_hyperparams}\pysiglinewithargsret{\bfcode{set\_hyperparams}}{\emph{new\_params}}{}
Set the (free) hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{new\_params} : \code{Array} or other Array-like
\begin{quote}

New values of the free parameters.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{ValueError} :
\begin{quote}

If the length of \emph{new\_params} is not consistent with \code{self.params}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{BetaWarpedKernel (class in gptools.kernel.warping)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.BetaWarpedKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.warping.}\bfcode{BetaWarpedKernel}}{\emph{k}, \emph{**w\_kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.warping.WarpedKernel]{\code{gptools.kernel.warping.WarpedKernel}}}

Class to warp any existing \code{Kernel} with the beta CDF.

Assumes that your inputs \emph{X} lie entirely within the unit hypercube {[}0, 1{]}.

Note that you may experience some issues with constraining and computing
derivatives at \(x=0\) when \(\alpha < 1\) and at \(x=1\) when
\(\beta < 1\). As a workaround, try mapping your data to not touch the
boundaries of the unit hypercube.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{k} : \code{Kernel}
\begin{quote}

The \code{Kernel} to warp.
\end{quote}

\textbf{**w\_kwargs} : optional kwargs
\begin{quote}

All additional kwargs are passed to the constructor of
{\hyperref[gptools.kernel:gptools.kernel.warping.WarpingFunction]{\code{WarpingFunction}}}. If no hyperprior or param\_bounds are
provided, takes each \(\alpha\), \(\beta\) to follow the
log-normal distribution.
\end{quote}

\end{description}\end{quote}
\paragraph{References}

{\hyperref[gptools.kernel:r2]{{[}R2{]}}}

\end{fulllineitems}

\index{LinearWarpedKernel (class in gptools.kernel.warping)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.warping.LinearWarpedKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.warping.}\bfcode{LinearWarpedKernel}}{\emph{k}, \emph{a}, \emph{b}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.warping.WarpedKernel]{\code{gptools.kernel.warping.WarpedKernel}}}

Class to warp any existing \code{Kernel} with the linear transformation given in {\hyperref[gptools.kernel:gptools.kernel.warping.linear_warp]{\code{linear\_warp()}}}.

If you set \emph{a} to be the minimum of your \emph{X} inputs in each dimension and \emph{b}
to be the maximum then you can use this to map data from an arbitrary domain
to the unit hypercube {[}0, 1{]}, as is required for application of the
{\hyperref[gptools.kernel:gptools.kernel.warping.BetaWarpedKernel]{\code{BetaWarpedKernel}}}, for instance.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{k} : \code{Kernel}
\begin{quote}

The \code{Kernel} to warp.
\end{quote}

\textbf{a} : list
\begin{quote}

The \emph{a} parameter in the linear warping defined in {\hyperref[gptools.kernel:gptools.kernel.warping.linear_warp]{\code{linear\_warp()}}}.
This list must have length equal to \emph{k.num\_dim}.
\end{quote}

\textbf{b} : list
\begin{quote}

The \emph{b} parameter in the linear warping defined in {\hyperref[gptools.kernel:gptools.kernel.warping.linear_warp]{\code{linear\_warp()}}}.
This list must have length equal to \emph{k.num\_dim}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}



\paragraph{Module contents}
\label{gptools.kernel:module-contents}\label{gptools.kernel:module-gptools.kernel}\index{gptools.kernel (module)}
Subpackage containing a variety of covariance kernels and associated helpers.


\subsection{Submodules}
\label{gptools:submodules}

\subsection{gptools.error\_handling module}
\label{gptools:gptools-error-handling-module}\label{gptools:module-gptools.error_handling}\index{gptools.error\_handling (module)}
Contains exceptions specific to the {\hyperref[gptools:module-gptools]{\code{gptools}}} package.
\index{GPArgumentError}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.error_handling.GPArgumentError}\pysigline{\strong{exception }\code{gptools.error\_handling.}\bfcode{GPArgumentError}}
Bases: \code{exceptions.Exception}

Exception class raised when an incorrect combination of keyword arguments is given.

\end{fulllineitems}



\subsection{gptools.gaussian\_process module}
\label{gptools:gptools-gaussian-process-module}\label{gptools:module-gptools.gaussian_process}\index{gptools.gaussian\_process (module)}
Provides the base {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} class.
\index{GaussianProcess (class in gptools.gaussian\_process)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess}\pysiglinewithargsret{\strong{class }\code{gptools.gaussian\_process.}\bfcode{GaussianProcess}}{\emph{k}, \emph{noise\_k=None}, \emph{X=None}, \emph{y=None}, \emph{err\_y=0}, \emph{n=0}, \emph{T=None}, \emph{diag\_factor=100.0}, \emph{mu=None}}{}
Bases: \code{object}

Gaussian process.

If called with one argument, an untrained Gaussian process is constructed
and data must be added with the {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.add_data]{\code{add\_data()}}} method. If called with
the optional keywords, the values given are used as the data. It is always
possible to add additional data with {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.add_data]{\code{add\_data()}}}.

Note that the attributes have no write protection, but you should always
add data with {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.add_data]{\code{add\_data()}}} to ensure internal consistency.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{k} : {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} instance
\begin{quote}

Kernel instance corresponding to the desired noise-free covariance
kernel of the Gaussian process. The noise is handled separately either
through specification of \emph{err\_y}, or in a separate kernel. This allows
noise-free predictions when needed.
\end{quote}

\textbf{noise\_k} : {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} instance
\begin{quote}

Kernel instance corresponding to the noise portion of the desired
covariance kernel of the Gaussian process. Note that you DO NOT need to
specify this if the extent of the noise you want to represent is
contained in \emph{err\_y} (or if your data are noiseless). Default value is
None, which results in the {\hyperref[gptools.kernel:gptools.kernel.noise.ZeroKernel]{\code{ZeroKernel}}}
(noise specified elsewhere or not present).
\end{quote}

\textbf{diag\_factor} : float, optional
\begin{quote}

Factor of \code{sys.float\_info.epsilon} which is added to the
diagonal of the total \emph{K} matrix to improve the stability of the
Cholesky decomposition. If you are having issues, try increasing this by
a factor of 10 at a time. Default is 1e2.
\end{quote}

\textbf{mu} : {\hyperref[gptools:gptools.mean.MeanFunction]{\code{MeanFunction}}} instance
\begin{quote}

The mean function of the Gaussian process. Default is None (zero mean
prior).
\end{quote}

\textbf{NOTE} :
\begin{quote}

The following are all passed to {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.add_data]{\code{add\_data()}}}, refer to its
docstring.
\end{quote}

\textbf{X} : array, (\emph{M}, \emph{D}), optional
\begin{quote}

\emph{M} input values of dimension \emph{D}. Default value is None (no data).
\end{quote}

\textbf{y} : array, (\emph{M},), optional
\begin{quote}

\emph{M} data target values. Default value is None (no data).
\end{quote}

\textbf{err\_y} : array, (\emph{M},), optional
\begin{quote}

Error (given as standard deviation) in the \emph{M} training target values.
Default value is 0 (noiseless observations).
\end{quote}

\textbf{n} : array, (\emph{M}, \emph{D}) or scalar float, optional
\begin{quote}

Non-negative integer values only. Degree of derivative for each target.
If \emph{n} is a scalar it is taken to be the value for all points in \emph{y}.
Otherwise, the length of n must equal the length of \emph{y}. Default value
is 0 (observation of target value). If non-integer values are passed,
they will be silently rounded.
\end{quote}

\textbf{T} : array, (\emph{M}, \emph{N}), optional
\begin{quote}

Linear transformation to get from latent variables to data in the
argument \emph{y}. When \emph{T} is passed the argument \emph{y} holds the transformed
quantities \emph{y=TY(X)} where \emph{y} are the observed values of the
transformed quantities, \emph{T} is the transformation matrix and \emph{Y(X)} is
the underlying (untransformed) values of the function to be fit that
enter into the transformation. When \emph{T} is \emph{M}-by-\emph{N} and \emph{y} has \emph{M}
elements, \emph{X} and \emph{n} will both be \emph{N}-by-\emph{D}. Default is None (no
transformation).
\end{quote}

\item[{Raises}] \leavevmode
\textbf{GPArgumentError} :
\begin{quote}

Gave \emph{X} but not \emph{y} (or vice versa).
\end{quote}

\textbf{ValueError} :
\begin{quote}

Training data rejected by {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.add_data]{\code{add\_data()}}}.
\end{quote}

\end{description}\end{quote}


\strong{See also:}

\begin{description}
\item[{{\hyperref[gptools:gptools.gaussian_process.GaussianProcess.add_data]{\code{add\_data}}}}] \leavevmode
Used to process \emph{X}, \emph{y}, \emph{err\_y} and to add data to the process.

\end{description}


\paragraph{Attributes}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

k
 & 
{\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} instance
 & 
The non-noise portion of the covariance kernel.
\\

noise\_k
 & 
{\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} instance
 & 
The noise portion of the covariance kernel.
\\

X
 & 
array, (\emph{M}, \emph{D})
 & 
The \emph{M} training input values, each of which is of dimension \emph{D}.
\\

y
 & 
array, (\emph{M},)
 & 
The \emph{M} training target values.
\\

err\_y
 & 
array, (\emph{M},)
 & 
The error in the \emph{M} training input values.
\\

n
 & 
array, (\emph{M}, \emph{D})
 & 
The orders of derivatives that each of the \emph{M} training points represent, indicating the order of derivative with respect to each of the \emph{D} dimensions.
\\

T
 & 
array, (\emph{M}, \emph{N})
 & 
The transformation matrix applied to the data. If this is not None, \emph{X} and \emph{n} will be \emph{N}-by-\emph{D}.
\\

K\_up\_to\_date
 & 
bool
 & 
True if no data have been added since the last time the internal state was updated with a call to {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.compute_K_L_alpha_ll]{\code{compute\_K\_L\_alpha\_ll()}}}.
\\

K
 & 
array, (\emph{M}, \emph{M})
 & 
Covariance matrix between all of the training inputs.
\\

noise\_K
 & 
array, (\emph{M}, \emph{M})
 & 
Noise portion of the covariance matrix between all of the training inputs. Only includes the noise from \code{noise\_k}, not from \code{err\_y}.
\\

L
 & 
array, (\emph{M}, \emph{M})
 & 
Cholesky decomposition of the combined covariance matrix between all of the training inputs.
\\

alpha
 & 
array, (\emph{M}, 1)
 & 
Solution to \(K\alpha=y\).
\\

ll
 & 
float
 & 
Log-likelihood of the data given the model.
\\

diag\_factor
 & 
float
 & 
The factor of \code{sys.float\_info.epsilon} which is added to the diagonal of the \emph{K} matrix to improve stability.
\\

mu
 & 
{\hyperref[gptools:gptools.mean.MeanFunction]{\code{MeanFunction}}} instance
 & 
The mean function.
\\
\hline\end{tabulary}

\index{hyperprior (gptools.gaussian\_process.GaussianProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.hyperprior}\pysigline{\bfcode{hyperprior}}
Combined hyperprior for the kernel, noise kernel and (if present) mean function.

\end{fulllineitems}

\index{fixed\_params (gptools.gaussian\_process.GaussianProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.fixed_params}\pysigline{\bfcode{fixed\_params}}
\end{fulllineitems}

\index{params (gptools.gaussian\_process.GaussianProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.params}\pysigline{\bfcode{params}}
\end{fulllineitems}

\index{param\_bounds (gptools.gaussian\_process.GaussianProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.param_bounds}\pysigline{\bfcode{param\_bounds}}
\end{fulllineitems}

\index{param\_names (gptools.gaussian\_process.GaussianProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.param_names}\pysigline{\bfcode{param\_names}}
\end{fulllineitems}

\index{free\_params (gptools.gaussian\_process.GaussianProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.free_params}\pysigline{\bfcode{free\_params}}
\end{fulllineitems}

\index{free\_param\_bounds (gptools.gaussian\_process.GaussianProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.free_param_bounds}\pysigline{\bfcode{free\_param\_bounds}}
\end{fulllineitems}

\index{free\_param\_names (gptools.gaussian\_process.GaussianProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.free_param_names}\pysigline{\bfcode{free\_param\_names}}
\end{fulllineitems}

\index{add\_data() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.add_data}\pysiglinewithargsret{\bfcode{add\_data}}{\emph{X}, \emph{y}, \emph{err\_y=0}, \emph{n=0}, \emph{T=None}}{}
Add data to the training data set of the GaussianProcess instance.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{X} : array, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} input values of dimension \emph{D}.
\end{quote}

\textbf{y} : array, (\emph{M},)
\begin{quote}

\emph{M} target values.
\end{quote}

\textbf{err\_y} : array, (\emph{M},) or scalar float, optional
\begin{quote}

Non-negative values only. Error given as standard deviation) in the
\emph{M} target values. If \emph{err\_y} is a scalar, the data set is taken to
be homoscedastic (constant error). Otherwise, the length of \emph{err\_y}
must equal the length of \emph{y}. Default value is 0 (noiseless
observations).
\end{quote}

\textbf{n} : array, (\emph{M}, \emph{D}) or scalar float, optional
\begin{quote}

Non-negative integer values only. Degree of derivative for each
target. If \emph{n} is a scalar it is taken to be the value for all
points in \emph{y}. Otherwise, the length of n must equal the length of
\emph{y}. Default value is 0 (observation of target value). If
non-integer values are passed, they will be silently rounded.
\end{quote}

\textbf{T} : array, (\emph{M}, \emph{N}), optional
\begin{quote}

Linear transformation to get from latent variables to data in the
argument \emph{y}. When \emph{T} is passed the argument \emph{y} holds the
transformed quantities \emph{y=TY(X)} where \emph{y} are the observed values
of the transformed quantities, \emph{T} is the transformation matrix and
\emph{Y(X)} is the underlying (untransformed) values of the function to
be fit that enter into the transformation. When \emph{T} is \emph{M}-by-\emph{N}
and \emph{y} has \emph{M} elements, \emph{X} and \emph{n} will both be \emph{N}-by-\emph{D}.
Default is None (no transformation).
\end{quote}

\item[{Raises}] \leavevmode
\textbf{ValueError} :
\begin{quote}

Bad shapes for any of the inputs, negative values for \emph{err\_y} or \emph{n}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{condense\_duplicates() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.condense_duplicates}\pysiglinewithargsret{\bfcode{condense\_duplicates}}{}{}
Condense duplicate points using a transformation matrix.

This is useful if you have multiple non-transformed points at the same
location or multiple transformed points that use the same quadrature
points.

Won't change the GP if all of the rows of {[}X, n{]} are unique. Will create
a transformation matrix T if necessary. Note that the order of the
points in {[}X, n{]} will be arbitrary after this operation.

\end{fulllineitems}

\index{remove\_outliers() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.remove_outliers}\pysiglinewithargsret{\bfcode{remove\_outliers}}{\emph{thresh=3}, \emph{**predict\_kwargs}}{}
Remove outliers from the GP.

Removes points that are more than \emph{thresh} * \emph{err\_y} away from the GP
mean. Note that this is only very rough in that it ignores the
uncertainty in the GP mean at any given point. But you should only be
using this as a rough way of removing bad channels, anyways!

Returns the values that were removed and a boolean array indicating
where the removed points were.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{thresh} : float, optional
\begin{quote}

The threshold as a multiplier times \emph{err\_y}. Default is 3 (i.e.,
throw away all 3-sigma points).
\end{quote}

\textbf{**predict\_kwargs} : optional kwargs
\begin{quote}

All additional kwargs are passed to {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.predict]{\code{predict()}}}. You can, for
instance, use this to make it use MCMC to evaluate the mean. (If you
don't use MCMC, then the current value of the hyperparameters is
used.)
\end{quote}

\item[{Returns}] \leavevmode
\textbf{X\_bad} : array
\begin{quote}

Input values of the bad points.
\end{quote}

\textbf{y\_bad} : array
\begin{quote}

Bad values.
\end{quote}

\textbf{err\_y\_bad} : array
\begin{quote}

Uncertainties on the bad values.
\end{quote}

\textbf{n\_bad} : array
\begin{quote}

Derivative order of the bad values.
\end{quote}

\textbf{bad\_idxs} : array
\begin{quote}

Array of booleans with the original shape of X with True wherever
a point was taken to be bad and subsequently removed.
\end{quote}

\textbf{T\_bad} : array
\begin{quote}

Transformation matrix of returned points. Only returned if
\code{T} is not None for the instance.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{optimize\_hyperparameters() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.optimize_hyperparameters}\pysiglinewithargsret{\bfcode{optimize\_hyperparameters}}{\emph{method='SLSQP'}, \emph{opt\_kwargs=\{\}}, \emph{verbose=False}, \emph{random\_starts=None}, \emph{num\_proc=None}}{}
Optimize the hyperparameters by maximizing the log likelihood.

Leaves the {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} instance in the optimized state.

If \code{scipy.optimize.minimize()} is not available (i.e., if your
\code{scipy} version is older than 0.11.0) then \code{fmin\_slsqp()}
is used independent of what you set for the \emph{method} keyword.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{method} : str, optional
\begin{quote}

The method to pass to \code{scipy.optimize.minimize()}.
Refer to that function's docstring for valid options. Default
is `SLSQP'. See note above about behavior with older versions of
\code{scipy}.
\end{quote}

\textbf{opt\_kwargs} : dict, optional
\begin{quote}

Dictionary of extra keywords to pass to
\code{scipy.optimize.minimize()}. Refer to that function's
docstring for valid options. Note that if you use \emph{jac} = True (i.e.,
optimization function returns Jacobian) you should also set \emph{args}
= (True,) to tell {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.update_hyperparameters]{\code{update\_hyperparameters()}}} to compute and
return the Jacobian. Default is: \{\}.
\end{quote}

\textbf{verbose} : bool, optional
\begin{quote}

Whether or not the output should be verbose. If True, the entire
\code{Result} object from \code{scipy.optimize.minimize()} is
printed. If False, status information is only printed if the
\emph{success} flag from \code{minimize()} is False. Default is False.
\end{quote}

\textbf{random\_starts} : non-negative int, optional
\begin{quote}

Number of times to randomly perturb the starting guesses
(distributed uniformly within their bounds) in order to seek the
global minimum. If None, then \emph{num\_proc} random starts will be
performed. Default is None (do number of random starts equal to the
number of processors allocated). Note that for \emph{random\_starts} != 0,
the initial guesses provided are not actually used.
\end{quote}

\textbf{num\_proc} : non-negative int or None
\begin{quote}

Number of processors to use with random starts. If 0, processing is
not done in parallel. If None, all available processors are used.
Default is None (use all available processors).
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{Xstar}, \emph{n=0}, \emph{noise=False}, \emph{return\_std=True}, \emph{return\_cov=False}, \emph{full\_output=False}, \emph{return\_samples=False}, \emph{num\_samples=1}, \emph{samp\_kwargs=\{\}}, \emph{use\_MCMC=False}, \emph{full\_MC=False}, \emph{rejection\_func=None}, \emph{ddof=1}, \emph{output\_transform=None}, \emph{**kwargs}}{}
Predict the mean and covariance at the inputs \emph{Xstar}.

The order of the derivative is given by \emph{n}. The keyword \emph{noise} sets
whether or not noise is included in the prediction.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xstar} : array, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} test input values of dimension \emph{D}.
\end{quote}

\textbf{n} : array, (\emph{M}, \emph{D}) or scalar, non-negative int, optional
\begin{quote}

Order of derivative to predict (0 is the base quantity). If \emph{n} is
scalar, the value is used for all points in \emph{Xstar}. If non-integer
values are passed, they will be silently rounded. Default is 0
(return base quantity).
\end{quote}

\textbf{noise} : bool, optional
\begin{quote}

Whether or not noise should be included in the covariance. Default
is False (no noise in covariance).
\end{quote}

\textbf{return\_std} : bool, optional
\begin{quote}

Set to True to compute and return the standard deviation for the
predictions, False to skip this step. Default is True (return tuple
of (\emph{mean}, \emph{std})).
\end{quote}

\textbf{return\_cov} : bool, optional
\begin{quote}

Set to True to compute and return the full covariance matrix for the
predictions. This overrides the \emph{return\_std} keyword. If you want
both the standard deviation and covariance matrix pre-computed, use
the \emph{full\_output} keyword.
\end{quote}

\textbf{full\_output} : bool, optional
\begin{quote}

Set to True to return the full outputs in a dictionary with keys:
\begin{quote}

\begin{tabulary}{\linewidth}{|L|L|}
\hline

mean
 & 
mean of GP at requested points
\\

std
 & 
standard deviation of GP at requested points
\\

cov
 & 
covariance matrix for values of GP at requested points
\\

samp
 & 
random samples of GP at requested points (only if \emph{return\_sample} is True)
\\
\hline\end{tabulary}

\end{quote}
\end{quote}

\textbf{return\_samples} : bool, optional
\begin{quote}

Set to True to compute and return samples of the GP in addition to
computing the mean. Only done if \emph{full\_output} is True. Default is
False.
\end{quote}

\textbf{num\_samples} : int, optional
\begin{quote}

Number of samples to compute. If using MCMC this is the number of
samples per MCMC sample, if using present values of hyperparameters
this is the number of samples actually returned. Default is 1.
\end{quote}

\textbf{samp\_kwargs} : dict, optional
\begin{quote}

Additional keywords to pass to {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.draw_sample]{\code{draw\_sample()}}} if
\emph{return\_samples} is True. Default is \{\}.
\end{quote}

\textbf{use\_MCMC} : bool, optional
\begin{quote}

Set to True to use {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.predict_MCMC]{\code{predict\_MCMC()}}} to evaluate the prediction
marginalized over the hyperparameters.
\end{quote}

\textbf{full\_MC} : bool, optional
\begin{quote}

Set to True to compute the mean and covariance matrix using Monte
Carlo sampling of the posterior. The samples will also be returned
if full\_output is True. Default is False (don't use full sampling).
\end{quote}

\textbf{rejection\_func} : callable, optional
\begin{quote}

Any samples where this function evaluates False will be rejected,
where it evaluates True they will be kept. Default is None (no
rejection). Only has an effect if \emph{full\_MC} is True.
\end{quote}

\textbf{ddof} : int, optional
\begin{quote}

The degree of freedom correction to use when computing the covariance
matrix when \emph{full\_MC} is True. Default is 1 (unbiased estimator).
\end{quote}

\textbf{output\_transform: array, ({}`L{}`, {}`M{}`), optional} :
\begin{quote}

Matrix to use to transform the output vector of length \emph{M} to one of
length \emph{L}. This can, for instance, be used to compute integrals.
\end{quote}

\textbf{**kwargs} : optional kwargs
\begin{quote}

All additional kwargs are passed to {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.predict_MCMC]{\code{predict\_MCMC()}}} if
\emph{use\_MCMC} is True.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{mean} : array, (\emph{M},)
\begin{quote}

Predicted GP mean. Only returned if \emph{full\_output} is False.
\end{quote}

\textbf{std} : array, (\emph{M},)
\begin{quote}

Predicted standard deviation, only returned if \emph{return\_std} is True, \emph{return\_cov} is False and \emph{full\_output} is False.
\end{quote}

\textbf{cov} : array, (\emph{M}, \emph{M})
\begin{quote}

Predicted covariance matrix, only returned if \emph{return\_cov} is True and \emph{full\_output} is False.
\end{quote}

\textbf{full\_output} : dict
\begin{quote}

Dictionary with fields for mean, std, cov and possibly random samples. Only returned if \emph{full\_output} is True.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{ValueError} :
\begin{quote}

If \emph{n} is not consistent with the shape of \emph{Xstar} or is not entirely
composed of non-negative integers.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{plot() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.plot}\pysiglinewithargsret{\bfcode{plot}}{\emph{X=None, n=0, ax=None, envelopes={[}1, 3{]}, base\_alpha=0.375, return\_prediction=False, return\_std=True, full\_output=False, plot\_kwargs=\{\}, **kwargs}}{}
Plots the Gaussian process using the current hyperparameters. Only for num\_dim \textless{}= 2.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{X} : array-like (\emph{M},) or (\emph{M}, \emph{num\_dim}), optional
\begin{quote}

The values to evaluate the Gaussian process at. If None, then 100
points between the minimum and maximum of the data's X are used.
Default is None (use 100 points between min and max).
\end{quote}

\textbf{n} : int or list, optional
\begin{quote}

The order of derivative to compute. For num\_dim=1, this must be an
int. For num\_dim=2, this must be a list of ints of length 2.
Default is 0 (don't take derivative).
\end{quote}

\textbf{ax} : axis instance, optional
\begin{quote}

Axis to plot the result on. If no axis is passed, one is created.
If the string `gca' is passed, the current axis (from plt.gca())
is used. If X\_dim = 2, the axis must be 3d.
\end{quote}

\textbf{envelopes: list of float, optional} :
\begin{quote}

+/-n*sigma envelopes to plot. Default is {[}1, 3{]}.
\end{quote}

\textbf{base\_alpha} : float, optional
\begin{quote}

Alpha value to use for +/-1*sigma envelope. All other envelopes env
are drawn with base\_alpha/env. Default is 0.375.
\end{quote}

\textbf{return\_prediction} : bool, optional
\begin{quote}

If True, the predicted values are also returned. Default is False.
\end{quote}

\textbf{return\_std} : bool, optional
\begin{quote}

If True, the standard deviation is computed and returned along with
the mean when \emph{return\_prediction} is True. Default is True.
\end{quote}

\textbf{full\_output} : bool, optional
\begin{quote}

Set to True to return the full outputs in a dictionary with keys:
\begin{quote}

\begin{tabulary}{\linewidth}{|L|L|}
\hline

mean
 & 
mean of GP at requested points
\\

std
 & 
standard deviation of GP at requested points
\\

cov
 & 
covariance matrix for values of GP at requested points
\\

samp
 & 
random samples of GP at requested points (only if \emph{return\_sample} is True)
\\
\hline\end{tabulary}

\end{quote}
\end{quote}

\textbf{plot\_kwargs} : dict, optional
\begin{quote}

The entries in this dictionary are passed as kwargs to the plotting
command used to plot the mean. Use this to, for instance, change the
color, line width and line style.
\end{quote}

\textbf{**kwargs} : extra arguments for predict, optional
\begin{quote}

Extra arguments that are passed to {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.predict]{\code{predict()}}}.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{ax} : axis instance
\begin{quote}

The axis instance used.
\end{quote}

\textbf{mean} : \code{Array}, (\emph{M},)
\begin{quote}

Predicted GP mean. Only returned if \emph{return\_prediction} is True and \emph{full\_output} is False.
\end{quote}

\textbf{std} : \code{Array}, (\emph{M},)
\begin{quote}

Predicted standard deviation, only returned if \emph{return\_prediction} and \emph{return\_std} are True and \emph{full\_output} is False.
\end{quote}

\textbf{full\_output} : dict
\begin{quote}

Dictionary with fields for mean, std, cov and possibly random samples. Only returned if \emph{return\_prediction} and \emph{full\_output} are True.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{draw\_sample() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.draw_sample}\pysiglinewithargsret{\bfcode{draw\_sample}}{\emph{Xstar}, \emph{n=0}, \emph{num\_samp=1}, \emph{rand\_vars=None}, \emph{rand\_type='standard normal'}, \emph{diag\_factor=1000.0}, \emph{method='cholesky'}, \emph{num\_eig=None}, \emph{mean=None}, \emph{cov=None}, \emph{**kwargs}}{}
Draw a sample evaluated at the given points \emph{Xstar}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xstar} : array, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} test input values of dimension \emph{D}.
\end{quote}

\textbf{n} : array, (\emph{M}, \emph{D}) or scalar, non-negative int, optional
\begin{quote}

Derivative order to evaluate at. Default is 0 (evaluate value).
\end{quote}

\textbf{noise} : bool, optional
\begin{quote}

Whether or not to include the noise components of the kernel in the
sample. Default is False (no noise in samples).
\end{quote}

\textbf{num\_samp} : Positive int, optional
\begin{quote}

Number of samples to draw. Default is 1. Cannot be used in
conjunction with \emph{rand\_vars}: If you pass both \emph{num\_samp} and
\emph{rand\_vars}, \emph{num\_samp} will be silently ignored.
\end{quote}

\textbf{rand\_vars} : array, (\emph{M}, \emph{P}), optional
\begin{quote}

Vector of random variables \(u\) to use in constructing the
sample \(y_* = f_* + Lu\), where \(K=LL^T\). If None,
values will be produced using
\code{numpy.random.multivariate\_normal()}. This allows you to use
pseudo/quasi random numbers generated by an external routine.
Default is None (use \code{multivariate\_normal()} directly).
\end{quote}

\textbf{rand\_type} : \{`standard normal', `uniform'\}, optional
\begin{quote}

Type of distribution the inputs are given with.
\begin{itemize}
\item {} 
`standard normal': Standard (\emph{mu} = 0, \emph{sigma} = 1) normal
distribution (this is the default)

\item {} 
`uniform': Uniform distribution on {[}0, 1). In this case
the required Gaussian variables are produced with inversion.

\end{itemize}
\end{quote}

\textbf{diag\_factor} : float, optional
\begin{quote}

Number (times machine epsilon) added to the diagonal of the
covariance matrix prior to computing its Cholesky decomposition.
This is necessary as sometimes the decomposition will fail because,
to machine precision, the matrix appears to not be positive definite.
If you are getting errors from \code{scipy.linalg.cholesky()}, try
increasing this an order of magnitude at a time. This parameter only
has an effect when using rand\_vars. Default value is 1e3.
\end{quote}

\textbf{method} : \{`cholesky', `eig'\}, optional
\begin{quote}

Method to use for constructing the matrix square root. Default is
`cholesky' (use lower-triangular Cholesky decomposition).
\begin{itemize}
\item {} 
`cholesky': Perform Cholesky decomposition on the covariance
matrix: \(K=LL^T\), use \(L\) as the matrix square
root.

\item {} 
`eig': Perform an eigenvalue decomposition on the covariance
matrix: \(K=Q \Lambda Q^{-1}\), use \(Q\Lambda^{1/2}\)
as the matrix square root.

\end{itemize}
\end{quote}

\textbf{num\_eig} : int or None, optional
\begin{quote}

Number of eigenvalues to compute. Can range from 1 to \emph{M} (the
number of test points). If it is None, then all eigenvalues are
computed. Default is None (compute all eigenvalues). This keyword
only has an effect if \emph{method} is `eig'.
\end{quote}

\textbf{mean} : array, (\emph{M},)
\begin{quote}

If you have pre-computed the mean and covariance matrix, then you
can simply pass them in with the \emph{mean} and \emph{cov} keywords to save
on having to call {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.predict]{\code{predict()}}}.
\end{quote}

\textbf{cov} : array, (\emph{M}, \emph{M})
\begin{quote}

If you have pre-computed the mean and covariance matrix, then you
can simply pass them in with the \emph{mean} and \emph{cov} keywords to save
on having to call {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.predict]{\code{predict()}}}.
\end{quote}

\textbf{**kwargs} : optional kwargs
\begin{quote}

All extra keyword arguments are passed to {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.predict]{\code{predict()}}} when
evaluating the mean and covariance matrix of the GP.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{samples} : \code{Array} (\emph{M}, \emph{P}) or (\emph{M}, \emph{num\_samp})
\begin{quote}

Samples evaluated at the \emph{M} points.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{ValueError} :
\begin{quote}

If rand\_type or method is invalid.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{update\_hyperparameters() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.update_hyperparameters}\pysiglinewithargsret{\bfcode{update\_hyperparameters}}{\emph{new\_params}, \emph{exit\_on\_bounds=True}, \emph{inf\_on\_error=True}}{}
Update the kernel's hyperparameters to the new parameters.

This will call {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.compute_K_L_alpha_ll]{\code{compute\_K\_L\_alpha\_ll()}}} to update the state
accordingly.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{new\_params} : \code{Array} or other Array-like, length dictated by kernel
\begin{quote}

New parameters to use.
\end{quote}

\textbf{exit\_on\_bounds} : bool, optional
\begin{quote}

If True, the method will automatically exit if the hyperparameters
are impossible given the hyperprior, without trying to update the
internal state. This is useful during MCMC sampling and optimization.
Default is True (don't perform update for impossible hyperparameters).
\end{quote}

\textbf{inf\_on\_error} : bool, optional
\begin{quote}

If True, the method will return \emph{scipy.inf} if the hyperparameters
produce a linear algebra error upon trying to update the Gaussian
process. Default is True (catch errors and return infinity).
\end{quote}

\item[{Returns}] \leavevmode
\textbf{-1*ll} : float
\begin{quote}

The updated log likelihood.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_K\_L\_alpha\_ll() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.compute_K_L_alpha_ll}\pysiglinewithargsret{\bfcode{compute\_K\_L\_alpha\_ll}}{}{}
Compute \emph{K}, \emph{L}, \emph{alpha} and log-likelihood according to the first part of Algorithm 2.1 in R\&W.

Computes \emph{K} and the noise portion of \emph{K} using {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.compute_Kij]{\code{compute\_Kij()}}},
computes \emph{L} using \code{scipy.linalg.cholesky()}, then computes
\emph{alpha} as \emph{L.T\textbackslash{}(L\textbackslash{}y)}.

Only does the computation if \code{K\_up\_to\_date} is False --
otherwise leaves the existing values.

\end{fulllineitems}

\index{num\_dim (gptools.gaussian\_process.GaussianProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.num_dim}\pysigline{\bfcode{num\_dim}}
The number of dimensions of the input data.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\textbf{num\_dim: int} :
\begin{quote}

The number of dimensions of the input data as defined in the kernel.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_Kij() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.compute_Kij}\pysiglinewithargsret{\bfcode{compute\_Kij}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{noise=False}, \emph{hyper\_deriv=None}}{}
Compute covariance matrix between datasets \emph{Xi} and \emph{Xj}.

Specify the orders of derivatives at each location with the \emph{ni}, \emph{nj}
arrays. The \emph{include\_noise} flag is passed to the covariance kernel to
indicate whether noise is to be included (i.e., for evaluation of
\(K+\sigma I\) versus \(K_*\)).

If \emph{Xj} is None, the symmetric matrix \(K(X, X)\) is formed.

Note that type and dimension checking is NOT performed, as it is assumed
the data are from inside the instance and have hence been sanitized by
{\hyperref[gptools:gptools.gaussian_process.GaussianProcess.add_data]{\code{add\_data()}}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{Xi} : array, (\emph{M}, \emph{D})
\begin{quote}

\emph{M} input values of dimension \emph{D}.
\end{quote}

\textbf{Xj} : array, (\emph{P}, \emph{D})
\begin{quote}

\emph{P} input values of dimension \emph{D}.
\end{quote}

\textbf{ni} : array, (\emph{M},), non-negative integers
\begin{quote}

\emph{M} derivative orders with respect to the \emph{Xi} coordinates.
\end{quote}

\textbf{nj} : array, (\emph{P},), non-negative integers
\begin{quote}

\emph{P} derivative orders with respect to the \emph{Xj} coordinates.
\end{quote}

\textbf{noise} : bool, optional
\begin{quote}

If True, uses the noise kernel, otherwise uses the regular kernel.
Default is False (use regular kernel).
\end{quote}

\textbf{hyper\_deriv} : None or non-negative int
\begin{quote}

Index of the hyperparameter to compute the first derivative with
respect to. If None, no derivatives are taken. Default is None (no
hyperparameter derivatives).
\end{quote}

\item[{Returns}] \leavevmode
\textbf{Kij} : array, (\emph{M}, \emph{P})
\begin{quote}

Covariance matrix between \emph{Xi} and \emph{Xj}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_ll\_matrix() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.compute_ll_matrix}\pysiglinewithargsret{\bfcode{compute\_ll\_matrix}}{\emph{bounds}, \emph{num\_pts}}{}
Compute the log likelihood over the (free) parameter space.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{bounds} : 2-tuple or list of 2-tuples with length equal to the number of free parameters
\begin{quote}

Bounds on the range to use for each of the parameters. If a single
2-tuple is given, it will be used for each of the parameters.
\end{quote}

\textbf{num\_pts} : int or list of ints with length equal to the number of free parameters
\begin{quote}

If a single int is given, it will be used for each of the parameters.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{ll\_vals} : \code{Array}
\begin{quote}
\begin{quote}

The log likelihood for each of the parameter possibilities.
\end{quote}
\begin{description}
\item[{param\_vals}] \leavevmode{[}List of \code{Array}{]}
The parameter values used.

\end{description}
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample\_hyperparameter\_posterior() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.sample_hyperparameter_posterior}\pysiglinewithargsret{\bfcode{sample\_hyperparameter\_posterior}}{\emph{nwalkers=200}, \emph{nsamp=500}, \emph{burn=0}, \emph{thin=1}, \emph{num\_proc=None}, \emph{sampler=None}, \emph{plot\_posterior=False}, \emph{plot\_chains=False}, \emph{sampler\_type='ensemble'}, \emph{ntemps=20}, \emph{sampler\_a=2.0}}{}
Produce samples from the posterior for the hyperparameters using MCMC.

Returns the sampler created, because storing it stops the GP from being
pickleable. To add more samples to a previous sampler, pass the sampler
instance in the \emph{sampler} keyword.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{nwalkers} : int, optional
\begin{quote}

The number of walkers to use in the sampler. Should be on the order
of several hundred. Default is 200.
\end{quote}

\textbf{nsamp} : int, optional
\begin{quote}

Number of samples (per walker) to take. Default is 500.
\end{quote}

\textbf{burn} : int, optional
\begin{quote}

This keyword only has an effect on the corner plot produced when
\emph{plot\_posterior} is True and the flattened chain plot produced
when \emph{plot\_chains} is True. To perform computations with burn-in,
see {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.compute_from_MCMC]{\code{compute\_from\_MCMC()}}}. The number of samples to discard
at the beginning of the chain. Default is 0.
\end{quote}

\textbf{thin} : int, optional
\begin{quote}

This keyword only has an effect on the corner plot produced when
\emph{plot\_posterior} is True and the flattened chain plot produced
when \emph{plot\_chains} is True. To perform computations with thinning,
see {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.compute_from_MCMC]{\code{compute\_from\_MCMC()}}}. Every \emph{thin}-th sample is kept.
Default is 1.
\end{quote}

\textbf{num\_proc} : int or None, optional
\begin{quote}

Number of processors to use. If None, all available processors are
used. Default is None (use all available processors).
\end{quote}

\textbf{sampler} : \code{Sampler} instance
\begin{quote}

The sampler to use. If the sampler already has samples, the most
recent sample will be used as the starting point. Otherwise a
random sample from the hyperprior will be used.
\end{quote}

\textbf{plot\_posterior} : bool, optional
\begin{quote}

If True, a corner plot of the posterior for the hyperparameters
will be generated. Default is False.
\end{quote}

\textbf{plot\_chains} : bool, optional
\begin{quote}

If True, a plot showing the history and autocorrelation of the
chains will be produced.
\end{quote}

\textbf{sampler\_type} : str, optional
\begin{quote}

The type of sampler to use. Valid options are ``ensemble'' (affine-
invariant ensemble sampler) and ``pt'' (parallel-tempered ensemble
sampler).
\end{quote}

\textbf{ntemps} : int, optional
\begin{quote}

Number of temperatures to use with the parallel-tempered ensemble
sampler.
\end{quote}

\textbf{sampler\_a} : float, optional
\begin{quote}

Scale of the proposal distribution.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_from\_MCMC() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.compute_from_MCMC}\pysiglinewithargsret{\bfcode{compute\_from\_MCMC}}{\emph{X}, \emph{n=0}, \emph{return\_mean=True}, \emph{return\_std=True}, \emph{return\_cov=False}, \emph{return\_samples=False}, \emph{num\_samples=1}, \emph{noise=False}, \emph{samp\_kwargs=\{\}}, \emph{sampler=None}, \emph{flat\_trace=None}, \emph{burn=0}, \emph{thin=1}, \emph{**kwargs}}{}
Compute desired quantities from MCMC samples of the hyperparameter posterior.

The return will be a list with a number of rows equal to the number of
hyperparameter samples. The columns depend on the state of the boolean
flags, but will be some subset of (mean, stddev, cov, samples), in that
order. Samples will be the raw output of {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.draw_sample]{\code{draw\_sample()}}}, so you
will need to remember to convert to an array and flatten if you want to
work with a single sample.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{X} : array-like (\emph{M},) or (\emph{M}, \emph{num\_dim})
\begin{quote}

The values to evaluate the Gaussian process at.
\end{quote}

\textbf{n} : non-negative int or list, optional
\begin{quote}

The order of derivative to compute. For num\_dim=1, this must be an
int. For num\_dim=2, this must be a list of ints of length 2.
Default is 0 (don't take derivative).
\end{quote}

\textbf{return\_mean} : bool, optional
\begin{quote}

If True, the mean will be computed at each hyperparameter sample.
Default is True (compute mean).
\end{quote}

\textbf{return\_std} : bool, optional
\begin{quote}

If True, the standard deviation will be computed at each
hyperparameter sample. Default is True (compute stddev).
\end{quote}

\textbf{return\_cov} : bool, optional
\begin{quote}

If True, the covariance matrix will be computed at each
hyperparameter sample. Default is True (compute stddev).
\end{quote}

\textbf{return\_samples} : bool, optional
\begin{quote}

If True, random sample(s) will be computed at each hyperparameter
sample. Default is False (do not compute samples).
\end{quote}

\textbf{num\_samples} : int, optional
\begin{quote}

Compute this many samples if \emph{return\_sample} is True. Default is 1.
\end{quote}

\textbf{noise} : bool, optional
\begin{quote}

If True, noise is included in the predictions and samples. Default
is False (do not include noise).
\end{quote}

\textbf{samp\_kwargs} : dict, optional
\begin{quote}

If \emph{return\_sample} is True, the contents of this dictionary will be
passed as kwargs to {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.draw_sample]{\code{draw\_sample()}}}.
\end{quote}

\textbf{sampler} : \code{Sampler} instance or None, optional
\begin{quote}

\code{Sampler} instance that has already been run to the extent
desired on the hyperparameter posterior. If None, a new sampler will
be created with {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.sample_hyperparameter_posterior]{\code{sample\_hyperparameter\_posterior()}}}. In this
case, all extra kwargs will be passed on, allowing you to set the
number of samples, etc. Default is None (create sampler).
\end{quote}

\textbf{flat\_trace} : array-like (\emph{nsamp}, \emph{ndim}) or None, optional
\begin{quote}

Flattened trace with samples of the free hyperparameters. If present,
overrides \emph{sampler}. This allows you to use a sampler other than the
ones from \code{emcee}, or to specify arbitrary values you wish
to evaluate the curve at. Note that this WILL be thinned and burned
according to the following two kwargs. ``Flat'' refers to the fact
that you must have combined all chains into a single one. Default is
None (use \emph{sampler}).
\end{quote}

\textbf{burn} : int, optional
\begin{quote}

The number of samples to discard at the beginning of the chain.
Default is 0.
\end{quote}

\textbf{thin} : int, optional
\begin{quote}

Every \emph{thin}-th sample is kept. Default is 1.
\end{quote}

\textbf{num\_proc} : int, optional
\begin{quote}

The number of processors to use for evaluation. This is used both
when calling the sampler and when evaluating the Gaussian process.
If None, the number of available processors will be used. If zero,
evaluation will proceed in parallel. Default is to use all available
processors.
\end{quote}

\textbf{**kwargs} : extra optional kwargs
\begin{quote}

All additional kwargs are passed to
{\hyperref[gptools:gptools.gaussian_process.GaussianProcess.sample_hyperparameter_posterior]{\code{sample\_hyperparameter\_posterior()}}}.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{out} : dict
\begin{quote}

A dictionary having some or all of the fields `mean', `std', `cov'
and `samp'. Each entry is a list of array-like. The length of this
list is equal to the number of hyperparameter samples used, and the
entries have the following shapes:
\begin{quote}

\begin{tabulary}{\linewidth}{|L|L|}
\hline

mean
 & 
(\emph{M},)
\\

std
 & 
(\emph{M},)
\\

cov
 & 
(\emph{M}, \emph{M})
\\

samp
 & 
(\emph{M}, \emph{num\_samples})
\\
\hline\end{tabulary}

\end{quote}
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{predict\_MCMC() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.predict_MCMC}\pysiglinewithargsret{\bfcode{predict\_MCMC}}{\emph{X}, \emph{ddof=1}, \emph{full\_MC=False}, \emph{rejection\_func=None}, \emph{**kwargs}}{}
Make a prediction using MCMC samples.

This is essentially a convenient wrapper of {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.compute_from_MCMC]{\code{compute\_from\_MCMC()}}},
designed to act more or less interchangeably with {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.predict]{\code{predict()}}}.

Computes the mean of the GP posterior marginalized over the
hyperparameters using iterated expectations. If \emph{return\_std} is True,
uses the law of total variance to compute the variance of the GP
posterior marginalized over the hyperparameters. If \emph{return\_cov} is True,
uses the law of total covariance to compute the entire covariance of the
GP posterior marginalized over the hyperparameters. If both \emph{return\_cov}
and \emph{return\_std} are True, then both the covariance matrix and standard
deviation array will be returned.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{X} : array-like (\emph{M},) or (\emph{M}, \emph{num\_dim})
\begin{quote}

The values to evaluate the Gaussian process at.
\end{quote}

\textbf{ddof} : int, optional
\begin{quote}

The degree of freedom correction to use when computing the variance.
Default is 1 (standard Bessel correction for unbiased estimate).
\end{quote}

\textbf{return\_std} : bool, optional
\begin{quote}

If True, the standard deviation is also computed. Default is True.
\end{quote}

\textbf{full\_MC} : bool, optional
\begin{quote}

Set to True to compute the mean and covariance matrix using Monte
Carlo sampling of the posterior. The samples will also be returned
if full\_output is True. Default is False (don't use full sampling).
\end{quote}

\textbf{rejection\_func} : callable, optional
\begin{quote}

Any samples where this function evaluates False will be rejected,
where it evaluates True they will be kept. Default is None (no
rejection). Only has an effect if \emph{full\_MC} is True.
\end{quote}

\textbf{ddof} : int, optional

\textbf{**kwargs} : optional kwargs
\begin{quote}

All additional kwargs are passed directly to
{\hyperref[gptools:gptools.gaussian_process.GaussianProcess.compute_from_MCMC]{\code{compute\_from\_MCMC()}}}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{Constraint (class in gptools.gaussian\_process)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.Constraint}\pysiglinewithargsret{\strong{class }\code{gptools.gaussian\_process.}\bfcode{Constraint}}{\emph{gp}, \emph{boundary\_val=0.0}, \emph{n=0}, \emph{loc='min'}, \emph{type\_='gt'}, \emph{bounds=None}}{}
Bases: \code{object}

Implements an inequality constraint on the value of the mean or its derivatives.

Provides a callable such as can be passed to SLSQP or COBYLA to implement
the constraint when using \code{scipy.optimize.minimize()}.

The function defaults implement a constraint that forces the mean value to
be positive everywhere.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{gp} : {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}}
\begin{quote}

The {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} instance to create the constraint on.
\end{quote}

\textbf{boundary\_val} : float, optional
\begin{quote}

Boundary value for the constraint. For \emph{type\_} = `gt', this is the lower
bound, for \emph{type\_} = `lt', this is the upper bound. Default is 0.0.
\end{quote}

\textbf{n} : non-negative int, optional
\begin{quote}

Derivative order to evaluate. Default is 0 (value of the mean). Note
that non-int values are silently cast to int.
\end{quote}

\textbf{loc} : \{`min', `max'\}, float or Array-like of float (\emph{num\_dim},), optional
\begin{quote}

Which extreme of the mean to use, or location to evaluate at.
\begin{itemize}
\item {} 
If `min', the minimum of the mean (optionally over \emph{bounds}) is used.

\item {} 
If `max', the maximum of the mean (optionally over \emph{bounds}) is used.

\item {} 
If a float (valid for \emph{num\_dim} = 1 only) or Array of float, the mean
is evaluated at the given X value.

\end{itemize}

Default is `min' (use function minimum).
\end{quote}

\textbf{type\_} : \{`gt', `lt'\}, optional
\begin{quote}

What type of inequality constraint to implement.
\begin{itemize}
\item {} 
If `gt', a greater-than-or-equals constraint is used.

\item {} 
If `lt', a less-than-or-equals constraint is used.

\end{itemize}

Default is `gt' (greater-than-or-equals).
\end{quote}

\textbf{bounds} : 2-tuple of float or 2-tuple Array-like of float (\emph{num\_dim},) or None, optional
\begin{quote}

Bounds to use when \emph{loc} is `min' or `max'.
\begin{itemize}
\item {} 
If None, the bounds are taken to be the extremes of the training data.
For multivariate data, ``extremes'' essentially means the smallest
hypercube oriented parallel to the axes that encapsulates all of the
training inputs. (I.e., \code{(gp.X.min(axis=0), gp.X.max(axis=0))})

\item {} 
If \emph{bounds} is a 2-tuple, then this is used as (\emph{lower}, \emph{upper})
where lower{}` and \emph{upper} are Array-like with dimensions (\emph{num\_dim},).

\item {} 
If \emph{num\_dim} is 1 then \emph{lower} and \emph{upper} can be scalar floats.

\end{itemize}

Default is None (use extreme values of training data).
\end{quote}

\item[{Raises}] \leavevmode
\textbf{TypeError} :
\begin{quote}

If \emph{gp} is not an instance of {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}}.
\end{quote}

\textbf{ValueError} :
\begin{quote}

If \emph{n} is negative.
\end{quote}

\textbf{ValueError} :
\begin{quote}

If \emph{loc} is not `min', `max' or an Array-like of the correct dimensions.
\end{quote}

\textbf{ValueError} :
\begin{quote}

If \emph{type\_} is not `gt' or `lt'.
\end{quote}

\textbf{ValueError} :
\begin{quote}

If \emph{bounds} is not None or length 2 or if the elements of bounds don't
have the right dimensions.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.gaussian\_process.Constraint method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.Constraint.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{params}}{}
Returns a non-negative number if the constraint is satisfied.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{params} : Array-like, length dictated by kernel
\begin{quote}

New parameters to use.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{val} : float
\begin{quote}

Value of the constraint. \code{minimize} will attempt to keep
this non-negative.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsection{gptools.gp\_utils module}
\label{gptools:gptools-gp-utils-module}\label{gptools:module-gptools.gp_utils}\index{gptools.gp\_utils (module)}
Provides convenient utilities for working with the classes and results from {\hyperref[gptools:module-gptools]{\code{gptools}}}.

This module specifically contains utilities that need to interact directly with
the GaussianProcess object, and hence can present circular import problems when
incorporated in the main utils submodule.
\index{parallel\_compute\_ll\_matrix() (in module gptools.gp\_utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gp_utils.parallel_compute_ll_matrix}\pysiglinewithargsret{\code{gptools.gp\_utils.}\bfcode{parallel\_compute\_ll\_matrix}}{\emph{gp}, \emph{bounds}, \emph{num\_pts}, \emph{num\_proc=None}}{}
Compute matrix of the log likelihood over the parameter space in parallel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{bounds} : 2-tuple or list of 2-tuples with length equal to the number of free parameters
\begin{quote}

Bounds on the range to use for each of the parameters. If a single
2-tuple is given, it will be used for each of the parameters.
\end{quote}

\textbf{num\_pts} : int or list of ints with length equal to the number of free parameters
\begin{quote}

The number of points to use for each parameters. If a single int is
given, it will be used for each of the parameters.
\end{quote}

\textbf{num\_proc} : Positive int or None, optional
\begin{quote}

Number of processes to run the parallel computation with. If set to
None, ALL available cores are used. Default is None (use all available
cores).
\end{quote}

\item[{Returns}] \leavevmode
\textbf{ll\_vals} : array
\begin{quote}

The log likelihood for each of the parameter possibilities.
\end{quote}

\textbf{param\_vals} : list of array
\begin{quote}

The parameter values used.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{slice\_plot() (in module gptools.gp\_utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gp_utils.slice_plot}\pysiglinewithargsret{\code{gptools.gp\_utils.}\bfcode{slice\_plot}}{\emph{*args}, \emph{**kwargs}}{}
Constructs a plot that lets you look at slices through a multidimensional array.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{vals} : array, (\emph{M}, \emph{D}, \emph{P}, ...)
\begin{quote}

Multidimensional array to visualize.
\end{quote}

\textbf{x\_vals\_1} : array, (\emph{M},)
\begin{quote}

Values along the first dimension.
\end{quote}

\textbf{x\_vals\_2} : array, (\emph{D},)
\begin{quote}

Values along the second dimension.
\end{quote}

\textbf{x\_vals\_3} : array, (\emph{P},)
\begin{quote}

Values along the third dimension.

\textbf{...and so on. At least four arguments must be provided.}
\end{quote}

\textbf{names} : list of strings, optional
\begin{quote}

Names for each of the parameters at hand. If None, sequential numerical
identifiers will be used. Length must be equal to the number of
dimensions of \emph{vals}. Default is None.
\end{quote}

\textbf{n} : Positive int, optional
\begin{quote}

Number of contours to plot. Default is 100.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{f} : \code{Figure}
\begin{quote}

The Matplotlib figure instance created.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{GPArgumentError} :
\begin{quote}

If the number of arguments is less than 4.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{arrow\_respond() (in module gptools.gp\_utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gp_utils.arrow_respond}\pysiglinewithargsret{\code{gptools.gp\_utils.}\bfcode{arrow\_respond}}{\emph{slider}, \emph{event}}{}
Event handler for arrow key events in plot windows.

Pass the slider object to update as a masked argument using a lambda function:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{lambda} \PYG{n}{evt}\PYG{p}{:} \PYG{n}{arrow\PYGZus{}respond}\PYG{p}{(}\PYG{n}{my\PYGZus{}slider}\PYG{p}{,} \PYG{n}{evt}\PYG{p}{)}
\end{Verbatim}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{slider} : Slider instance associated with this handler.

\textbf{event} : Event to be handled.

\end{description}\end{quote}

\end{fulllineitems}



\subsection{gptools.mean module}
\label{gptools:gptools-mean-module}\label{gptools:module-gptools.mean}\index{gptools.mean (module)}
Provides classes for defining explicit, parametric mean functions.

To provide the necessary hooks to optimize/sample the hyperparameters, your mean
function must be wrapped with {\hyperref[gptools:gptools.mean.MeanFunction]{\code{MeanFunction}}} before being passed to
\code{GaussianProcess}. The function must have the calling fingerprint
\emph{fun(X, n, p1, p2, ...)}, where \emph{X} is an array with shape \emph{(M, N)}, \emph{n} is a
vector with length \emph{D} and \emph{p1}, \emph{p2}, ... are the (hyper)parameters of the mean
function, given as individual arguments.
\index{MeanFunction (class in gptools.mean)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.MeanFunction}\pysiglinewithargsret{\strong{class }\code{gptools.mean.}\bfcode{MeanFunction}}{\emph{fun}, \emph{num\_params=None}, \emph{initial\_params=None}, \emph{fixed\_params=None}, \emph{param\_bounds=None}, \emph{param\_names=None}, \emph{enforce\_bounds=False}, \emph{hyperprior=None}}{}
Bases: \code{object}

Wrapper to turn a function into a form useable by \code{GaussianProcess}.

This lets you define a simple function \emph{fun(X, n, p1, p2, ...)} that
operates on an (\emph{M}, \emph{D}) array \emph{X}, taking the derivatives indicated by the
vector \emph{n} with length \emph{D} (one derivative order for each dimension). The
function should evaluate this derivative at all points in \emph{X}, returning an
array of length \emph{M}. {\hyperref[gptools:gptools.mean.MeanFunction]{\code{MeanFunction}}} takes care of looping over the
different derivatives requested by \code{GaussianProcess}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{fun} : callable
\begin{quote}

Must have fingerprint \emph{fun(X, n, p1, p2, ...)} where \emph{X} is an array
with shape (\emph{M}, \emph{D}), \emph{n} is an array of non-negative integers with
length \emph{D} representing the order of derivative orders to take for each
dimension and \emph{p1}, \emph{p2}, ... are the parameters of the mean function.
\end{quote}

\textbf{num\_params} : Non-negative int, optional
\begin{quote}

Number of parameters in the model. Default is to determine the number of
parameters by inspection of \emph{fun} or the other arguments provided.
\end{quote}

\textbf{initial\_params} : Array, (\emph{num\_params},), optional
\begin{quote}

Initial values to set for the hyperparameters. Default is None, in
which case 1 is used for the initial values.
\end{quote}

\textbf{fixed\_params} : Array of bool, (\emph{num\_params},), optional
\begin{quote}

Sets which hyperparameters are considered fixed when optimizing the log
likelihood. A True entry corresponds to that element being
fixed (where the element ordering is as defined in the class).
Default value is None (no hyperparameters are fixed).
\end{quote}

\textbf{param\_bounds} : list of 2-tuples (\emph{num\_params},), optional
\begin{quote}

List of bounds for each of the hyperparameters. Each 2-tuple is of the
form (lower{}`, \emph{upper}). If there is no bound in a given direction, it
works best to set it to something big like 1e16. Default is (0.0, 1e16)
for each hyperparameter. Note that this is overridden by the \emph{hyperprior}
keyword, if present.
\end{quote}

\textbf{param\_names} : list of str (\emph{num\_params},), optional
\begin{quote}

List of labels for the hyperparameters. Default is all empty strings.
\end{quote}

\textbf{enforce\_bounds} : bool, optional
\begin{quote}

If True, an attempt to set a hyperparameter outside of its bounds will
result in the hyperparameter being set right at its bound. If False,
bounds are not enforced inside the kernel. Default is False (do not
enforce bounds).
\end{quote}

\textbf{hyperprior} : \code{JointPrior} instance or list, optional
\begin{quote}

Joint prior distribution for all hyperparameters. Can either be given
as a \code{JointPrior} instance or a list of \emph{num\_params}
callables or \code{rv\_frozen} instances from \code{scipy.stats},
in which case a \code{IndependentJointPrior} is constructed with
these as the independent priors on each hyperparameter. Default is a
uniform PDF on all hyperparameters.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.mean.MeanFunction method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.MeanFunction.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{X}, \emph{n}}{}
\end{fulllineitems}

\index{param\_bounds (gptools.mean.MeanFunction attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.MeanFunction.param_bounds}\pysigline{\bfcode{param\_bounds}}
\end{fulllineitems}

\index{set\_hyperparams() (gptools.mean.MeanFunction method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.MeanFunction.set_hyperparams}\pysiglinewithargsret{\bfcode{set\_hyperparams}}{\emph{new\_params}}{}
Sets the free hyperparameters to the new parameter values in new\_params.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{new\_params} : \code{Array} or other Array-like, (len(\code{self.params}),)
\begin{quote}

New parameter values, ordered as dictated by the docstring for the
class.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{num\_free\_params (gptools.mean.MeanFunction attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.MeanFunction.num_free_params}\pysigline{\bfcode{num\_free\_params}}
Returns the number of free parameters.

\end{fulllineitems}

\index{free\_param\_idxs (gptools.mean.MeanFunction attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.MeanFunction.free_param_idxs}\pysigline{\bfcode{free\_param\_idxs}}
Returns the indices of the free parameters in the main arrays of parameters, etc.

\end{fulllineitems}

\index{free\_params (gptools.mean.MeanFunction attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.MeanFunction.free_params}\pysigline{\bfcode{free\_params}}
Returns the values of the free hyperparameters.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\textbf{free\_params} : \code{Array}
\begin{quote}

Array of the free parameters, in order.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{free\_param\_bounds (gptools.mean.MeanFunction attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.MeanFunction.free_param_bounds}\pysigline{\bfcode{free\_param\_bounds}}
Returns the bounds of the free hyperparameters.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\textbf{free\_param\_bounds} : \code{Array}
\begin{quote}

Array of the bounds of the free parameters, in order.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{free\_param\_names (gptools.mean.MeanFunction attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.MeanFunction.free_param_names}\pysigline{\bfcode{free\_param\_names}}
Returns the names of the free hyperparameters.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\textbf{free\_param\_names} : \code{Array}
\begin{quote}

Array of the names of the free parameters, in order.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{constant() (in module gptools.mean)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.constant}\pysiglinewithargsret{\code{gptools.mean.}\bfcode{constant}}{\emph{X}, \emph{n}, \emph{mu}}{}
Function implementing a constant mean suitable for use with {\hyperref[gptools:gptools.mean.MeanFunction]{\code{MeanFunction}}}.

\end{fulllineitems}

\index{ConstantMeanFunction (class in gptools.mean)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.ConstantMeanFunction}\pysiglinewithargsret{\strong{class }\code{gptools.mean.}\bfcode{ConstantMeanFunction}}{\emph{**kwargs}}{}
Bases: {\hyperref[gptools:gptools.mean.MeanFunction]{\code{gptools.mean.MeanFunction}}}

Class implementing a constant mean function suitable for use with \code{GaussianProcess}.

All kwargs are passed to {\hyperref[gptools:gptools.mean.MeanFunction]{\code{MeanFunction}}}. If you do not pass
\emph{hyperprior} or \emph{param\_bounds}, the hyperprior for the mean is taken to be
uniform over {[}-1e3, 1e3{]}.

\end{fulllineitems}

\index{mtanh() (in module gptools.mean)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.mtanh}\pysiglinewithargsret{\code{gptools.mean.}\bfcode{mtanh}}{\emph{alpha}, \emph{z}}{}
Modified hyperbolic tangent function mtanh(z; alpha).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{alpha} : float
\begin{quote}

The core slope of the mtanh.
\end{quote}

\textbf{z} : float or array
\begin{quote}

The coordinate of the mtanh.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{mtanh\_profile() (in module gptools.mean)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.mtanh_profile}\pysiglinewithargsret{\code{gptools.mean.}\bfcode{mtanh\_profile}}{\emph{X}, \emph{n}, \emph{x0}, \emph{delta}, \emph{alpha}, \emph{h}, \emph{b}}{}
Profile used with the mtanh function to fit profiles, suitable for use with {\hyperref[gptools:gptools.mean.MeanFunction]{\code{MeanFunction}}}.

Only supports univariate data!
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{X} : array, (\emph{M}, 1)
\begin{quote}

The points to evaluate at.
\end{quote}

\textbf{n} : array, (1,)
\begin{quote}

The order of derivative to compute. Only up to first derivatives are
supported.
\end{quote}

\textbf{x0} : float
\begin{quote}

Pedestal center
\end{quote}

\textbf{delta} : float
\begin{quote}

Pedestal halfwidth
\end{quote}

\textbf{alpha} : float
\begin{quote}

Core slope
\end{quote}

\textbf{h} : float
\begin{quote}

Pedestal height
\end{quote}

\textbf{b} : float
\begin{quote}

Pedestal foot
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{MtanhMeanFunction1d (class in gptools.mean)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.MtanhMeanFunction1d}\pysiglinewithargsret{\strong{class }\code{gptools.mean.}\bfcode{MtanhMeanFunction1d}}{\emph{**kwargs}}{}
Bases: {\hyperref[gptools:gptools.mean.MeanFunction]{\code{gptools.mean.MeanFunction}}}

Profile with mtanh edge, suitable for use with \code{GaussianProcess}.

All kwargs are passed to {\hyperref[gptools:gptools.mean.MeanFunction]{\code{MeanFunction}}}. If \emph{hyperprior} and
\emph{param\_bounds} are not passed then the hyperprior is taken to be uniform
over the following intervals:
\begin{quote}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

x0
 & 
0.98
 & 
1.1
\\

delta
 & 
0.0
 & 
0.1
\\

alpha
 & 
-0.5
 & 
0.5
\\

h
 & 
0
 & 
5
\\

b
 & 
0
 & 
0.5
\\
\hline\end{tabulary}

\end{quote}

\end{fulllineitems}

\index{linear() (in module gptools.mean)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.linear}\pysiglinewithargsret{\code{gptools.mean.}\bfcode{linear}}{\emph{X}, \emph{n}, \emph{*args}}{}
Linear mean function of arbitrary dimension, suitable for use with {\hyperref[gptools:gptools.mean.MeanFunction]{\code{MeanFunction}}}.

The form is \(m_0 * X[:, 0] + m_1 * X[:, 1] + \dots + b\).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{X} : array, (\emph{M}, \emph{D})
\begin{quote}

The points to evaluate the model at.
\end{quote}

\textbf{n} : array of non-negative int, (\emph{D})
\begin{quote}

The derivative order to take, specified as an integer order for each
dimension in \emph{X}.
\end{quote}

\textbf{*args} : num\_dim+1 floats
\begin{quote}

The slopes for each dimension, plus the constant term. Must be of the
form \emph{m0, m1, ..., b}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{LinearMeanFunction (class in gptools.mean)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.mean.LinearMeanFunction}\pysiglinewithargsret{\strong{class }\code{gptools.mean.}\bfcode{LinearMeanFunction}}{\emph{num\_dim=1}, \emph{**kwargs}}{}
Bases: {\hyperref[gptools:gptools.mean.MeanFunction]{\code{gptools.mean.MeanFunction}}}

Linear mean function suitable for use with \code{GaussianProcess}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{num\_dim} : positive int, optional
\begin{quote}

The number of dimensions of the input data. Default is 1.
\end{quote}

\textbf{**kwargs} : optional kwargs
\begin{quote}

All extra kwargs are passed to {\hyperref[gptools:gptools.mean.MeanFunction]{\code{MeanFunction}}}. If \emph{hyperprior}
and \emph{param\_bounds} are not specified, all parameters are taken to have
a uniform hyperprior over {[}-1e3, 1e3{]}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}



\subsection{gptools.utils module}
\label{gptools:gptools-utils-module}\label{gptools:module-gptools.utils}\index{gptools.utils (module)}
Provides convenient utilities for working with the classes and results from {\hyperref[gptools:module-gptools]{\code{gptools}}}.
\index{LessThanUniformPotential (class in gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.LessThanUniformPotential}\pysiglinewithargsret{\strong{class }\code{gptools.utils.}\bfcode{LessThanUniformPotential}}{\emph{l\_idx}, \emph{g\_idx}}{}
Bases: \code{object}

Class to implement a potential to enforce an inequality constraint.

Specifically lets you change the param with l\_idx to have a uniform prior
between its lower bound and the param with g\_idx.

Returns log((ub-lb)/(theta{[}g\_idx{]}-lb)) if theta{[}l\_idx{]} \textless{}= theta{[}g\_idx{]},
double\_min otherwise.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{l\_idx} : int
\begin{quote}

Index of the parameter that is required to be lesser.
\end{quote}

\textbf{g\_idx} : int
\begin{quote}

Index of the parameter that is required to be greater.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.utils.LessThanUniformPotential method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.LessThanUniformPotential.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{theta}, \emph{k}}{}
Return the log-density of the potential.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{theta} : array-like
\begin{quote}

Array of the hyperparameters.
\end{quote}

\textbf{k} : Kernel instance
\begin{quote}

The kernel the hyperparameters apply to.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{f} : float
\begin{quote}

Returns log((ub-lb)/(theta{[}g\_idx{]}-lb)) if the condition is met, -inf if not.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{JeffreysPrior (class in gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.JeffreysPrior}\pysiglinewithargsret{\strong{class }\code{gptools.utils.}\bfcode{JeffreysPrior}}{\emph{idx}, \emph{bounds}}{}
Bases: \code{object}

Class to implement a Jeffreys prior over a finite range. Returns log-density.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{idx} : int
\begin{quote}

The index this prior applies to.
\end{quote}

\textbf{bounds} : 2-tuple
\begin{quote}

The bounds for the parameter this prior corresponds to: (lb, ub).
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.utils.JeffreysPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.JeffreysPrior.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{theta}}{}
\end{fulllineitems}

\index{interval() (gptools.utils.JeffreysPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.JeffreysPrior.interval}\pysiglinewithargsret{\bfcode{interval}}{\emph{alpha}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{LinearPrior (class in gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.LinearPrior}\pysiglinewithargsret{\strong{class }\code{gptools.utils.}\bfcode{LinearPrior}}{\emph{idx}, \emph{bounds}}{}
Bases: \code{object}

Class to implement a linear prior. Returns log-density.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{idx} : int
\begin{quote}

The index this prior applies to.
\end{quote}

\textbf{bounds} : 2-tuple
\begin{quote}

The bounds for the parameter this prior corresponds to: (lb, ub).
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.utils.LinearPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.LinearPrior.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{theta}}{}
Return the log-density of the uniform prior.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{theta} : array-like, or float
\begin{quote}

Value of values of the hyperparameter.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{f} : \code{Array} or float
\begin{quote}

Returns log(2/(b-a)\textasciicircum{}2) + log(b-theta) if theta is in bounds, -inf
if theta is out of bounds.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{interval() (gptools.utils.LinearPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.LinearPrior.interval}\pysiglinewithargsret{\bfcode{interval}}{\emph{alpha}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{UniformPrior (class in gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.UniformPrior}\pysiglinewithargsret{\strong{class }\code{gptools.utils.}\bfcode{UniformPrior}}{\emph{idx}, \emph{bounds}}{}
Bases: \code{object}

Class to implement a uniform prior. Returns log-density.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{idx} : int
\begin{quote}

The index this prior applies to.
\end{quote}

\textbf{bounds} : 2-tuple
\begin{quote}

The bounds for the parameter this prior corresponds to: (lb, ub).
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.utils.UniformPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.UniformPrior.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{theta}}{}
Return the log-PDF of the uniform prior.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{theta} : array-like
\begin{quote}

Values of the hyperparameters.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{f} : \code{Array} or float
\begin{quote}

Returns -log(ub - lb) if theta is scalar and in bounds, double\_min
if theta is scalar and out of bounds and an appropriately-shaped
array if theta is array-like.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{interval() (gptools.utils.UniformPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.UniformPrior.interval}\pysiglinewithargsret{\bfcode{interval}}{\emph{alpha}}{}
\end{fulllineitems}

\index{rvs() (gptools.utils.UniformPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.UniformPrior.rvs}\pysiglinewithargsret{\bfcode{rvs}}{\emph{size=None}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{JointPrior (class in gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.JointPrior}\pysigline{\strong{class }\code{gptools.utils.}\bfcode{JointPrior}}
Bases: \code{object}

Abstract class for objects implementing joint priors over hyperparameters.
\index{\_\_call\_\_() (gptools.utils.JointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.JointPrior.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{theta}}{}
Evaluate the prior log-PDF at the given values of the hyperparameters, theta.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{theta} : array-like, (\emph{num\_params},)
\begin{quote}

The hyperparameters to evaluate the log-PDF at.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{random\_draw() (gptools.utils.JointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.JointPrior.random_draw}\pysiglinewithargsret{\bfcode{random\_draw}}{\emph{size=None}}{}
Draw random samples of the hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{size} : None, int or array-like, optional
\begin{quote}

The number/shape of samples to draw. If None, only one sample is
returned. Default is None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{\_\_mul\_\_() (gptools.utils.JointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.JointPrior.__mul__}\pysiglinewithargsret{\bfcode{\_\_mul\_\_}}{\emph{other}}{}
Multiply two {\hyperref[gptools:gptools.utils.JointPrior]{\code{JointPrior}}} instances together.

\end{fulllineitems}


\end{fulllineitems}

\index{CombinedBounds (class in gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.CombinedBounds}\pysiglinewithargsret{\strong{class }\code{gptools.utils.}\bfcode{CombinedBounds}}{\emph{l1}, \emph{l2}}{}
Bases: \code{object}

Object to support reassignment of the bounds from a combined prior.
\index{\_\_getitem\_\_() (gptools.utils.CombinedBounds method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.CombinedBounds.__getitem__}\pysiglinewithargsret{\bfcode{\_\_getitem\_\_}}{\emph{pos}}{}
\end{fulllineitems}

\index{\_\_setitem\_\_() (gptools.utils.CombinedBounds method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.CombinedBounds.__setitem__}\pysiglinewithargsret{\bfcode{\_\_setitem\_\_}}{\emph{pos}, \emph{value}}{}
\end{fulllineitems}

\index{\_\_len\_\_() (gptools.utils.CombinedBounds method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.CombinedBounds.__len__}\pysiglinewithargsret{\bfcode{\_\_len\_\_}}{}{}
\end{fulllineitems}

\index{\_\_invert\_\_() (gptools.utils.CombinedBounds method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.CombinedBounds.__invert__}\pysiglinewithargsret{\bfcode{\_\_invert\_\_}}{}{}
\end{fulllineitems}


\end{fulllineitems}

\index{MaskedBounds (class in gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.MaskedBounds}\pysiglinewithargsret{\strong{class }\code{gptools.utils.}\bfcode{MaskedBounds}}{\emph{a}, \emph{m}}{}
Bases: \code{object}

Object to support reassignment of free parameter bounds.
\index{\_\_getitem\_\_() (gptools.utils.MaskedBounds method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.MaskedBounds.__getitem__}\pysiglinewithargsret{\bfcode{\_\_getitem\_\_}}{\emph{pos}}{}
\end{fulllineitems}

\index{\_\_setitem\_\_() (gptools.utils.MaskedBounds method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.MaskedBounds.__setitem__}\pysiglinewithargsret{\bfcode{\_\_setitem\_\_}}{\emph{pos}, \emph{value}}{}
\end{fulllineitems}

\index{\_\_len\_\_() (gptools.utils.MaskedBounds method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.MaskedBounds.__len__}\pysiglinewithargsret{\bfcode{\_\_len\_\_}}{}{}
\end{fulllineitems}


\end{fulllineitems}

\index{ProductJointPrior (class in gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.ProductJointPrior}\pysiglinewithargsret{\strong{class }\code{gptools.utils.}\bfcode{ProductJointPrior}}{\emph{p1}, \emph{p2}}{}
Bases: {\hyperref[gptools:gptools.utils.JointPrior]{\code{gptools.utils.JointPrior}}}

Product of two independent priors.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{p1, p2: :py:class:{}`JointPrior{}` instances} :
\begin{quote}

The two priors to merge.
\end{quote}

\end{description}\end{quote}
\index{bounds (gptools.utils.ProductJointPrior attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.ProductJointPrior.bounds}\pysigline{\bfcode{bounds}}
\end{fulllineitems}

\index{\_\_call\_\_() (gptools.utils.ProductJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.ProductJointPrior.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{theta}}{}
Evaluate the prior log-PDF at the given values of the hyperparameters, theta.

The log-PDFs of the two priors are summed.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{theta} : array-like, (\emph{num\_params},)
\begin{quote}

The hyperparameters to evaluate the log-PDF at.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{random\_draw() (gptools.utils.ProductJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.ProductJointPrior.random_draw}\pysiglinewithargsret{\bfcode{random\_draw}}{\emph{size=None}}{}
Draw random samples of the hyperparameters.

The outputs of the two priors are stacked vertically.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{size} : None, int or array-like, optional
\begin{quote}

The number/shape of samples to draw. If None, only one sample is
returned. Default is None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{UniformJointPrior (class in gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.UniformJointPrior}\pysiglinewithargsret{\strong{class }\code{gptools.utils.}\bfcode{UniformJointPrior}}{\emph{bounds}}{}
Bases: {\hyperref[gptools:gptools.utils.JointPrior]{\code{gptools.utils.JointPrior}}}

Uniform prior over the specified bounds.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{bounds} : list of tuples, (\emph{num\_params},)
\begin{quote}

The bounds for each of the random variables.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.utils.UniformJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.UniformJointPrior.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{theta}}{}
Evaluate the prior log-PDF at the given values of the hyperparameters, theta.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{theta} : array-like, (\emph{num\_params},)
\begin{quote}

The hyperparameters to evaluate the log-PDF at.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{random\_draw() (gptools.utils.UniformJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.UniformJointPrior.random_draw}\pysiglinewithargsret{\bfcode{random\_draw}}{\emph{size=None}}{}
Draw random samples of the hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{size} : None, int or array-like, optional
\begin{quote}

The number/shape of samples to draw. If None, only one sample is
returned. Default is None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{CoreEdgeJointPrior (class in gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.CoreEdgeJointPrior}\pysiglinewithargsret{\strong{class }\code{gptools.utils.}\bfcode{CoreEdgeJointPrior}}{\emph{bounds}}{}
Bases: {\hyperref[gptools:gptools.utils.UniformJointPrior]{\code{gptools.utils.UniformJointPrior}}}

Prior for use with Gibbs kernel warping functions with an inequality constraint between the core and edge length scales.
\index{\_\_call\_\_() (gptools.utils.CoreEdgeJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.CoreEdgeJointPrior.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{theta}}{}
Evaluate the prior log-PDF at the given values of the hyperparameters, theta.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{theta} : array-like, (\emph{num\_params},)
\begin{quote}

The hyperparameters to evaluate the log-PDF at.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{random\_draw() (gptools.utils.CoreEdgeJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.CoreEdgeJointPrior.random_draw}\pysiglinewithargsret{\bfcode{random\_draw}}{\emph{size=None}}{}
Draw random samples of the hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{size} : None, int or array-like, optional
\begin{quote}

The number/shape of samples to draw. If None, only one sample is
returned. Default is None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{CoreMidEdgeJointPrior (class in gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.CoreMidEdgeJointPrior}\pysiglinewithargsret{\strong{class }\code{gptools.utils.}\bfcode{CoreMidEdgeJointPrior}}{\emph{bounds}}{}
Bases: {\hyperref[gptools:gptools.utils.UniformJointPrior]{\code{gptools.utils.UniformJointPrior}}}

Prior for use with Gibbs kernel warping functions with an inequality constraint between the core, mid and edge length scales and the core-mid and mid-edge joins.
\index{\_\_call\_\_() (gptools.utils.CoreMidEdgeJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.CoreMidEdgeJointPrior.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{theta}}{}
Evaluate the prior log-PDF at the given values of the hyperparameters, theta.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{theta} : array-like, (\emph{num\_params},)
\begin{quote}

The hyperparameters to evaluate the log-PDF at.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{random\_draw() (gptools.utils.CoreMidEdgeJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.CoreMidEdgeJointPrior.random_draw}\pysiglinewithargsret{\bfcode{random\_draw}}{\emph{size=None}}{}
Draw random samples of the hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{size} : None, int or array-like, optional
\begin{quote}

The number/shape of samples to draw. If None, only one sample is
returned. Default is None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{IndependentJointPrior (class in gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.IndependentJointPrior}\pysiglinewithargsret{\strong{class }\code{gptools.utils.}\bfcode{IndependentJointPrior}}{\emph{univariate\_priors}}{}
Bases: {\hyperref[gptools:gptools.utils.JointPrior]{\code{gptools.utils.JointPrior}}}

Joint prior for which each hyperparameter is independent.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{univariate\_priors} : list of callables or rv\_frozen, (\emph{num\_params},)
\begin{quote}

The univariate priors for each hyperparameter. Entries in this list
can either be a callable that takes as an argument the entire list of
hyperparameters or a frozen instance of a distribution from
\code{scipy.stats}.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.utils.IndependentJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.IndependentJointPrior.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{theta}}{}
Evaluate the prior log-PDF at the given values of the hyperparameters, theta.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{theta} : array-like, (\emph{num\_params},)
\begin{quote}

The hyperparameters to evaluate the log-PDF at.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{bounds (gptools.utils.IndependentJointPrior attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.IndependentJointPrior.bounds}\pysigline{\bfcode{bounds}}
The bounds of the random variable.

\end{fulllineitems}

\index{random\_draw() (gptools.utils.IndependentJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.IndependentJointPrior.random_draw}\pysiglinewithargsret{\bfcode{random\_draw}}{\emph{size=None}}{}
Draw random samples of the hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{size} : None, int or array-like, optional
\begin{quote}

The number/shape of samples to draw. If None, only one sample is
returned. Default is None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{NormalJointPrior (class in gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.NormalJointPrior}\pysiglinewithargsret{\strong{class }\code{gptools.utils.}\bfcode{NormalJointPrior}}{\emph{mu}, \emph{sigma}}{}
Bases: {\hyperref[gptools:gptools.utils.JointPrior]{\code{gptools.utils.JointPrior}}}

Joint prior for which each hyperparameter has a normal prior with fixed hyper-hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{mu} : list of float, same size as \emph{sigma}
\begin{quote}

Means of the hyperparameters.
\end{quote}

\textbf{sigma} : list of float
\begin{quote}

Standard deviations of the hyperparameters.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.utils.NormalJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.NormalJointPrior.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{theta}}{}
Evaluate the prior log-PDF at the given values of the hyperparameters, theta.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{theta} : array-like, (\emph{num\_params},)
\begin{quote}

The hyperparameters to evaluate the log-PDF at.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{bounds (gptools.utils.NormalJointPrior attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.NormalJointPrior.bounds}\pysigline{\bfcode{bounds}}
The bounds of the random variable.

\end{fulllineitems}

\index{random\_draw() (gptools.utils.NormalJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.NormalJointPrior.random_draw}\pysiglinewithargsret{\bfcode{random\_draw}}{\emph{size=None}}{}
Draw random samples of the hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{size} : None, int or array-like, optional
\begin{quote}

The number/shape of samples to draw. If None, only one sample is
returned. Default is None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{LogNormalJointPrior (class in gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.LogNormalJointPrior}\pysiglinewithargsret{\strong{class }\code{gptools.utils.}\bfcode{LogNormalJointPrior}}{\emph{mu}, \emph{sigma}}{}
Bases: {\hyperref[gptools:gptools.utils.JointPrior]{\code{gptools.utils.JointPrior}}}

Joint prior for which each hyperparameter has a log-normal prior with fixed hyper-hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{mu} : list of float, same size as \emph{sigma}
\begin{quote}

Means of the logarithms of the hyperparameters.
\end{quote}

\textbf{sigma} : list of float
\begin{quote}

Standard deviations of the logarithms of the hyperparameters.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.utils.LogNormalJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.LogNormalJointPrior.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{theta}}{}
Evaluate the prior log-PDF at the given values of the hyperparameters, theta.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{theta} : array-like, (\emph{num\_params},)
\begin{quote}

The hyperparameters to evaluate the log-PDF at.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{bounds (gptools.utils.LogNormalJointPrior attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.LogNormalJointPrior.bounds}\pysigline{\bfcode{bounds}}
The bounds of the random variable.

\end{fulllineitems}

\index{random\_draw() (gptools.utils.LogNormalJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.LogNormalJointPrior.random_draw}\pysiglinewithargsret{\bfcode{random\_draw}}{\emph{size=None}}{}
Draw random samples of the hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{size} : None, int or array-like, optional
\begin{quote}

The number/shape of samples to draw. If None, only one sample is
returned. Default is None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{GammaJointPrior (class in gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.GammaJointPrior}\pysiglinewithargsret{\strong{class }\code{gptools.utils.}\bfcode{GammaJointPrior}}{\emph{a}, \emph{b}}{}
Bases: {\hyperref[gptools:gptools.utils.JointPrior]{\code{gptools.utils.JointPrior}}}

Joint prior for which each hyperparameter has a gamma prior with fixed hyper-hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{a} : list of float, same size as \emph{b}
\begin{quote}

Shape parameters.
\end{quote}

\textbf{b} : list of float
\begin{quote}

Rate parameters.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.utils.GammaJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.GammaJointPrior.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{theta}}{}
Evaluate the prior log-PDF at the given values of the hyperparameters, theta.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{theta} : array-like, (\emph{num\_params},)
\begin{quote}

The hyperparameters to evaluate the log-PDF at.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{bounds (gptools.utils.GammaJointPrior attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.GammaJointPrior.bounds}\pysigline{\bfcode{bounds}}
The bounds of the random variable.

\end{fulllineitems}

\index{random\_draw() (gptools.utils.GammaJointPrior method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.GammaJointPrior.random_draw}\pysiglinewithargsret{\bfcode{random\_draw}}{\emph{size=None}}{}
Draw random samples of the hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{size} : None, int or array-like, optional
\begin{quote}

The number/shape of samples to draw. If None, only one sample is
returned. Default is None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{wrap\_fmin\_slsqp() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.wrap_fmin_slsqp}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{wrap\_fmin\_slsqp}}{\emph{fun}, \emph{guess}, \emph{opt\_kwargs=\{\}}}{}
Wrapper for \code{fmin\_slsqp()} to allow it to be called with \code{minimize()}-like syntax.

This is included to enable the code to run with \code{scipy} versions
older than 0.11.0.

Accepts \emph{opt\_kwargs} in the same format as used by
\code{scipy.optimize.minimize()}, with the additional precondition
that the keyword \emph{method} has already been removed by the calling code.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{fun} : callable
\begin{quote}

The function to minimize.
\end{quote}

\textbf{guess} : sequence
\begin{quote}

The initial guess for the parameters.
\end{quote}

\textbf{opt\_kwargs} : dict, optional
\begin{quote}

Dictionary of extra keywords to pass to
\code{scipy.optimize.minimize()}. Refer to that function's
docstring for valid options. The keywords `jac', `hess' and `hessp'
are ignored. Note that if you were planning to use \emph{jac} = True
(i.e., optimization function returns Jacobian) and have set
\emph{args} = (True,) to tell \code{update\_hyperparameters()} to
compute and return the Jacobian this may cause unexpected behavior.
Default is: \{\}.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{Result} : namedtuple
\begin{quote}

\code{namedtuple} that mimics the fields of the
\code{Result} object returned by
\code{scipy.optimize.minimize()}. Has the following fields:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

status
 & 
int
 & 
Code indicating the exit mode of the optimizer (\emph{imode} from \code{fmin\_slsqp()})
\\

success
 & 
bool
 & 
Boolean indicating whether or not the optimizer thinks a minimum was found.
\\

fun
 & 
float
 & 
Value of the optimized function (-1*LL).
\\

x
 & 
ndarray
 & 
Optimal values of the hyperparameters.
\\

message
 & 
str
 & 
String describing the exit state (\emph{smode} from \code{fmin\_slsqp()})
\\

nit
 & 
int
 & 
Number of iterations.
\\
\hline\end{tabulary}

\end{quote}

\item[{Raises}] \leavevmode
\textbf{ValueError} :
\begin{quote}

Invalid constraint type in \emph{constraints}. (See documentation for \code{scipy.optimize.minimize()}.)
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{incomplete\_bell\_poly() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.incomplete_bell_poly}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{incomplete\_bell\_poly}}{\emph{n}, \emph{k}, \emph{x}}{}
Recursive evaluation of the incomplete Bell polynomial \(B_{n, k}(x)\).

Evaluates the incomplete Bell polynomial \(B_{n, k}(x_1, x_2, \dots, x_{n-k+1})\),
also known as the partial Bell polynomial or the Bell polynomial of the
second kind. This polynomial is useful in the evaluation of (the univariate)
Faa di Bruno's formula which generalizes the chain rule to higher order
derivatives.

The implementation here is based on the implementation in:
\code{sympy.functions.combinatorial.numbers.bell.\_bell\_incomplete\_poly()}
Following that function's documentation, the polynomial is computed
according to the recurrence formula:
\begin{gather}
\begin{split}B_{n, k}(x_1, x_2, \dots, x_{n-k+1}) = \sum_{m=1}^{n-k+1}x_m\binom{n-1}{m-1}B_{n-m, k-1}(x_1, x_2, \dots, x_{n-m-k})\end{split}\notag
\end{gather}
\begin{DUlineblock}{0em}
\item[] The end cases are:
\item[] \(B_{0, 0} = 1\)
\item[] \(B_{n, 0} = 0\) for \(n \ge 1\)
\item[] \(B_{0, k} = 0\) for \(k \ge 1\)
\end{DUlineblock}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{n} : scalar int
\begin{quote}

The first subscript of the polynomial.
\end{quote}

\textbf{k} : scalar int
\begin{quote}

The second subscript of the polynomial.
\end{quote}

\textbf{x} : \code{Array} of floats, (\emph{p}, \emph{n} - \emph{k} + 1)
\begin{quote}

\emph{p} sets of \emph{n} - \emph{k} + 1 points to use as the arguments to
\(B_{n,k}\). The second dimension can be longer than
required, in which case the extra entries are silently ignored
(this facilitates recursion without needing to subset the array \emph{x}).
\end{quote}

\item[{Returns}] \leavevmode
\textbf{result} : \code{Array}, (\emph{p},)
\begin{quote}

Incomplete Bell polynomial evaluated at the desired values.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_set\_partition\_strings() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.generate_set_partition_strings}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{generate\_set\_partition\_strings}}{\emph{n}}{}
Generate the restricted growth strings for all of the partitions of an \emph{n}-member set.

Uses Algorithm H from page 416 of volume 4A of Knuth's \emph{The Art of Computer
Programming}. Returns the partitions in lexicographical order.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{n} : scalar int, non-negative
\begin{quote}

Number of (unique) elements in the set to be partitioned.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{partitions} : list of \code{Array}
\begin{quote}

List has a number of elements equal to the \emph{n}-th Bell number (i.e.,
the number of partitions for a set of size \emph{n}). Each element has
length \emph{n}, the elements of which are the restricted growth strings
describing the partitions of the set. The strings are returned in
lexicographic order.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_set\_partitions() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.generate_set_partitions}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{generate\_set\_partitions}}{\emph{set\_}}{}
Generate all of the partitions of a set.

This is a helper function that utilizes the restricted growth strings from
{\hyperref[gptools:gptools.utils.generate_set_partition_strings]{\code{generate\_set\_partition\_strings()}}}. The partitions are returned in
lexicographic order.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{set\_} : \code{Array} or other Array-like, (\emph{m},)
\begin{quote}

The set to find the partitions of.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{partitions} : list of lists of \code{Array}
\begin{quote}

The number of elements in the outer list is equal to the number of
partitions, which is the len(\emph{m})\textasciicircum{}th Bell number. Each of the inner lists
corresponds to a single possible partition. The length of an inner list
is therefore equal to the number of blocks. Each of the arrays in an
inner list is hence a block.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{powerset() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.powerset}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{powerset}}{\emph{{[}1,2,3{]}) --\textgreater{} () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3}}{}
From itertools documentation.

\end{fulllineitems}

\index{unique\_rows() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.unique_rows}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{unique\_rows}}{\emph{arr}, \emph{return\_index=False}, \emph{return\_inverse=False}}{}
Returns a copy of arr with duplicate rows removed.

From Stackoverflow ``Find unique rows in numpy.array.''
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{arr} : \code{Array}, (\emph{m}, \emph{n})
\begin{quote}

The array to find the unique rows of.
\end{quote}

\textbf{return\_index} : bool, optional
\begin{quote}

If True, the indices of the unique rows in the array will also be
returned. I.e., unique = arr{[}idx{]}. Default is False (don't return
indices).
\end{quote}

\textbf{return\_inverse: bool, optional} :
\begin{quote}

If True, the indices in the unique array to reconstruct the original
array will also be returned. I.e., arr = unique{[}inv{]}. Default is False
(don't return inverse).
\end{quote}

\item[{Returns}] \leavevmode
\textbf{unique} : \code{Array}, (\emph{p}, \emph{n}) where \emph{p} \textless{}= \emph{m}
\begin{quote}

The array \emph{arr} with duplicate rows removed.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_stats() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.compute_stats}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{compute\_stats}}{\emph{vals}, \emph{check\_nan=False}, \emph{robust=False}, \emph{axis=1}, \emph{plot\_QQ=False}, \emph{bins=15}, \emph{name='`}}{}
Compute the average statistics (mean, std dev) for the given values.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{vals} : array-like, (\emph{M}, \emph{D})
\begin{quote}

Values to compute the average statistics along the specified axis of.
\end{quote}

\textbf{check\_nan} : bool, optional
\begin{quote}

Whether or not to check for (and exclude) NaN's. Default is False (do
not attempt to handle NaN's).
\end{quote}

\textbf{robust} : bool, optional
\begin{quote}

Whether or not to use robust estimators (median for mean, IQR for
standard deviation). Default is False (use non-robust estimators).
\end{quote}

\textbf{axis} : int, optional
\begin{quote}

Axis to compute the statistics along. Presently only supported if
\emph{robust} is False. Default is 1.
\end{quote}

\textbf{plot\_QQ} : bool, optional
\begin{quote}

Whether or not a QQ plot and histogram should be drawn for each channel.
Default is False (do not draw QQ plots).
\end{quote}

\textbf{bins} : int, optional
\begin{quote}

Number of bins to use when plotting histogram (for plot\_QQ=True).
Default is 15
\end{quote}

\textbf{name} : str, optional
\begin{quote}

Name to put in the title of the QQ/histogram plot.
\end{quote}

\item[{Returns}] \leavevmode
\textbf{mean} : ndarray, (\emph{M},)
\begin{quote}

Estimator for the mean of \emph{vals}.
\end{quote}

\textbf{std} : ndarray, (\emph{M},)
\begin{quote}

Estimator for the standard deviation of \emph{vals}.
\end{quote}

\item[{Raises}] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If \emph{axis} != 1 when \emph{robust} is True.
\end{quote}

\textbf{NotImplementedError} :
\begin{quote}

If \emph{plot\_QQ} is True.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{univariate\_envelope\_plot() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.univariate_envelope_plot}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{univariate\_envelope\_plot}}{\emph{x, mean, std, ax=None, base\_alpha=0.375, envelopes={[}1, 3{]}, **kwargs}}{}
Make a plot of a mean curve with uncertainty envelopes.

\end{fulllineitems}

\index{summarize\_sampler() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.summarize_sampler}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{summarize\_sampler}}{\emph{sampler}, \emph{burn=0}, \emph{thin=1}, \emph{ci=0.95}}{}
Create summary statistics of the flattened chain of the sampler.

The confidence regions are computed from the quantiles of the data.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{sampler} : \code{emcee.EnsembleSampler} instance
\begin{quote}

The sampler to summarize the chains of.
\end{quote}

\textbf{burn} : int, optional
\begin{quote}

The number of samples to burn from the beginning of the chain. Default
is 0 (no burn).
\end{quote}

\textbf{thin} : int, optional
\begin{quote}

The step size to thin with. Default is 1 (no thinning).
\end{quote}

\textbf{ci} : float, optional
\begin{quote}

A number between 0 and 1 indicating the confidence region to compute.
Default is 0.95 (return upper and lower bounds of the 95\% confidence
interval).
\end{quote}

\item[{Returns}] \leavevmode
\textbf{mean} : array, (num\_params,)
\begin{quote}

Mean values of each of the parameters sampled.
\end{quote}

\textbf{ci\_l} : array, (num\_params,)
\begin{quote}

Lower bounds of the \emph{ci*100\%} confidence intervals.
\end{quote}

\textbf{ci\_u} : array, (num\_params,)
\begin{quote}

Upper bounds of the \emph{ci*100\%} confidence intervals.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_sampler() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.plot_sampler}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{plot\_sampler}}{\emph{sampler}, \emph{labels=None}, \emph{burn=0}, \emph{chain\_mask=None}}{}
Plot the results of MCMC sampler (posterior and chains).

Loosely based on triangle.py.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{sampler} : \code{emcee.EnsembleSampler} instance
\begin{quote}

The sampler to plot the chains/marginals of.
\end{quote}

\textbf{labels} : list of str, optional
\begin{quote}

The labels to use for each of the free parameters. Default is to leave
the axes unlabeled.
\end{quote}

\textbf{burn} : int, optional
\begin{quote}

The number of samples to burn before making the marginal histograms.
Default is zero (use all samples).
\end{quote}

\textbf{chain\_mask} : (index) array
\begin{quote}

Mask identifying the chains to keep before plotting, in case there are
bad chains. Default is to use all chains.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}



\subsection{Module contents}
\label{gptools:module-contents}\label{gptools:module-gptools}\index{gptools (module)}
{\hyperref[gptools:module-gptools]{\code{gptools}}} - Gaussian process regression with support for arbitrary derivatives


\chapter{Indices and tables}
\label{index:indices-and-tables}\begin{itemize}
\item {} 
\emph{genindex}

\item {} 
\emph{modindex}

\item {} 
\emph{search}

\end{itemize}

\begin{thebibliography}{R1}
\bibitem[R1]{R1}{\phantomsection\label{gptools.kernel:r1} 
J. Snoek, K. Swersky, R. Zemel, R. P. Adams, ``Input Warping for
Bayesian Optimization of Non-stationary Functions'' ICML (2014)
}
\bibitem[R2]{R2}{\phantomsection\label{gptools.kernel:r2} 
J. Snoek, K. Swersky, R. Zemel, R. P. Adams, ``Input Warping for
Bayesian Optimization of Non-stationary Functions'' ICML (2014)
}
\end{thebibliography}


\renewcommand{\indexname}{Python Module Index}
\begin{theindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{g}
\item {\texttt{gptools}}, \pageref{gptools:module-gptools}
\item {\texttt{gptools.error\_handling}}, \pageref{gptools:module-gptools.error_handling}
\item {\texttt{gptools.gaussian\_process}}, \pageref{gptools:module-gptools.gaussian_process}
\item {\texttt{gptools.gp\_utils}}, \pageref{gptools:module-gptools.gp_utils}
\item {\texttt{gptools.kernel}}, \pageref{gptools.kernel:module-gptools.kernel}
\item {\texttt{gptools.kernel.core}}, \pageref{gptools.kernel:module-gptools.kernel.core}
\item {\texttt{gptools.kernel.gibbs}}, \pageref{gptools.kernel:module-gptools.kernel.gibbs}
\item {\texttt{gptools.kernel.matern}}, \pageref{gptools.kernel:module-gptools.kernel.matern}
\item {\texttt{gptools.kernel.noise}}, \pageref{gptools.kernel:module-gptools.kernel.noise}
\item {\texttt{gptools.kernel.rational\_quadratic}}, \pageref{gptools.kernel:module-gptools.kernel.rational_quadratic}
\item {\texttt{gptools.kernel.squared\_exponential}}, \pageref{gptools.kernel:module-gptools.kernel.squared_exponential}
\item {\texttt{gptools.kernel.warping}}, \pageref{gptools.kernel:module-gptools.kernel.warping}
\item {\texttt{gptools.mean}}, \pageref{gptools:module-gptools.mean}
\item {\texttt{gptools.utils}}, \pageref{gptools:module-gptools.utils}
\end{theindex}
\renewcommand{\indexname}{Python Module Index}
\begin{theindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{g}
\item {\texttt{gptools}}, \pageref{gptools:module-gptools}
\item {\texttt{gptools.error\_handling}}, \pageref{gptools:module-gptools.error_handling}
\item {\texttt{gptools.gaussian\_process}}, \pageref{gptools:module-gptools.gaussian_process}
\item {\texttt{gptools.gp\_utils}}, \pageref{gptools:module-gptools.gp_utils}
\item {\texttt{gptools.kernel}}, \pageref{gptools.kernel:module-gptools.kernel}
\item {\texttt{gptools.kernel.core}}, \pageref{gptools.kernel:module-gptools.kernel.core}
\item {\texttt{gptools.kernel.gibbs}}, \pageref{gptools.kernel:module-gptools.kernel.gibbs}
\item {\texttt{gptools.kernel.matern}}, \pageref{gptools.kernel:module-gptools.kernel.matern}
\item {\texttt{gptools.kernel.noise}}, \pageref{gptools.kernel:module-gptools.kernel.noise}
\item {\texttt{gptools.kernel.rational\_quadratic}}, \pageref{gptools.kernel:module-gptools.kernel.rational_quadratic}
\item {\texttt{gptools.kernel.squared\_exponential}}, \pageref{gptools.kernel:module-gptools.kernel.squared_exponential}
\item {\texttt{gptools.kernel.warping}}, \pageref{gptools.kernel:module-gptools.kernel.warping}
\item {\texttt{gptools.mean}}, \pageref{gptools:module-gptools.mean}
\item {\texttt{gptools.utils}}, \pageref{gptools:module-gptools.utils}
\end{theindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}
