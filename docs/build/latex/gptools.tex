% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}
\hypersetup{bookmarksdepth=4}

\title{gptools Documentation}
\date{September 24, 2013}
\release{0.0}
\author{Mark Chilenski}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}



\chapter{Overview}
\label{index:overview}\label{index:gptools-gaussian-process-regression-with-support-for-arbitrary-derivatives}
\code{gptools} is a Python package that provides a convenient, powerful and extensible implementation of Gaussian process regression (GPR). Central to \code{gptool}`s implementation is support for derivatives and their variances. A number of kernels are provided to allow many types of data to be fit:
\begin{itemize}
\item {} 
{\hyperref[gptools.kernel:gptools.kernel.noise.DiagonalNoiseKernel]{\code{DiagonalNoiseKernel}}} implements homoscedastic noise. The noise is tied to a specific derivative order. This allows you to, for instance, have noise on your observations but have noiseless derivative constraints, or to have different noise levels for observations and derivatives. Note that you can also specify potentially heteroscedastic noise explicitly when adding data to the process.

\item {} 
{\hyperref[gptools.kernel:gptools.kernel.squared_exponential.SquaredExponentialKernel]{\code{SquaredExponentialKernel}}} implements the SE kernel which is infinitely differentiable.

\item {} 
{\hyperref[gptools.kernel:gptools.kernel.matern.MaternKernel]{\code{MaternKernel}}} implements the entire Matern class of covariance functions, which are characterized by a hyperparameter $\nu$. A process having the Matern kernel is only mean-square differentiable for derivative order $n<\nu$.

\item {} 
{\hyperref[gptools.kernel:gptools.kernel.rational_quadratic.RationalQuadraticKernel]{\code{RationalQuadraticKernel}}} implements the rational quadratic kernel, which is a scale mixture over SE kernels.

\end{itemize}

In all cases, these kernels have been constructed in a way to allow inputs of arbitrary dimension. Each dimension has a length scale hyperparameter that can be separately optimized over or held fixed. Arbitrary derivatives with respect to each dimension can be taken, including computation of the covariance for those observations.

Other kernels can be implemented by extending the {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} class. Furthermore, kernels may be added or multiplied together to yield a new, valid kernel.


\chapter{Contents}
\label{index:contents}

\section{gptools Package}
\label{gptools:gptools-package}\label{gptools::doc}

\subsection{\texttt{gptools} Package}
\label{gptools:id1}\phantomsection\label{gptools:module-gptools.__init__}\index{gptools.\_\_init\_\_ (module)}
\code{gptools} - Gaussian process regression with support for arbitrary derivatives


\subsection{\texttt{error\_handling} Module}
\label{gptools:error-handling-module}\label{gptools:module-gptools.error_handling}\index{gptools.error\_handling (module)}
Contains exceptions specific to the \code{gptools} package.
\index{GPArgumentError}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.error_handling.GPArgumentError}\pysigline{\strong{exception }\code{gptools.error\_handling.}\bfcode{GPArgumentError}}
Bases: \code{exceptions.Exception}

Exception class raised when an incorrect combination of keyword arguments is given.

\end{fulllineitems}



\subsection{\texttt{gaussian\_process} Module}
\label{gptools:module-gptools.gaussian_process}\label{gptools:gaussian-process-module}\index{gptools.gaussian\_process (module)}
Provides the base {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} class.
\index{GaussianProcess (class in gptools.gaussian\_process)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess}\pysiglinewithargsret{\strong{class }\code{gptools.gaussian\_process.}\bfcode{GaussianProcess}}{\emph{k}, \emph{noise\_k=None}, \emph{X=None}, \emph{y=None}, \emph{err\_y=0}}{}
Bases: \code{object}

Gaussian process.

If called with one argument, an untrained Gaussian process is
constructed and training data must be added with the {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.add_data]{\code{add\_data()}}} method.
If called with the optional keywords, the values given are used as the
training data. It is always possible to add additional training data
with {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.add_data]{\code{add\_data()}}}.

Note that the attributes have no write protection, but you should always
add data with {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.add_data]{\code{add\_data()}}} to ensure internal consistency.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{k} : {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} instance
\begin{quote}

Kernel instance corresponding to the desired noise-free
covariance kernel of the Gaussian process. The noise is handled
separately either through specification of \emph{err\_y}, or in a
separate kernel. This allows noise-free predictions when needed.
\end{quote}

\textbf{noise\_k} : {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} instance
\begin{quote}

Kernel instance corresponding to the noise portion of the
desired covariance kernel of the Gaussian process. Note that you
DO NOT need to specify this if the extent of the noise you want
to represent is contained in \emph{err\_y} (or if your data are
noiseless). Default value is None, which results in the
{\hyperref[gptools.kernel:gptools.kernel.noise.ZeroKernel]{\code{ZeroKernel}}} (noise specified elsewhere
or not present).
\end{quote}

\textbf{NOTE} :
\begin{quote}

The following are all passed to {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.add_data]{\code{add\_data()}}}, refer to its docstring.
\end{quote}

\textbf{X} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N}), optional
\begin{quote}

\emph{M} training input values of dimension \emph{N}. Default value is None (no
training data).
\end{quote}

\textbf{y} : \code{Array} or other Array-like, (\emph{M},), optional
\begin{quote}

\emph{M} training target values. Default value is None (no training data).
\end{quote}

\textbf{err\_y} : \code{Array} or other Array-like, (\emph{M},), optional
\begin{quote}

Error (given as standard deviation) in the \emph{M} training target values.
Default value is 0 (noiseless observations).
\end{quote}

\item[{Raises }] \leavevmode
\textbf{GPArgumentError} :
\begin{quote}

Gave \emph{X} but not \emph{y} (or vice versa).
\end{quote}

\textbf{ValueError} :
\begin{quote}

Training data rejected by {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.add_data]{\code{add\_data()}}}.
\end{quote}

\end{description}\end{quote}


\strong{See Also:}

\begin{description}
\item[{{\hyperref[gptools:gptools.gaussian_process.GaussianProcess.add_data]{\code{add\_data}}}}] \leavevmode
Used to process \emph{X}, \emph{y}, \emph{err\_y} and to add data to the process.

\end{description}


\paragraph{Attributes}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

k
 & 
{\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} instance
 & 
The non-noise portion of the covariance kernel.
\\\hline

noise\_k
 & 
{\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} instance
 & 
The noise portion of the covariance kernel.
\\\hline

X
 & 
\code{Matrix}, (\emph{M}, \emph{N})
 & 
The \emph{M} training input values, each of which is of dimension \emph{N}.
\\\hline

y
 & 
\code{Array}, (\emph{M},)
 & 
The \emph{M} training target values.
\\\hline

err\_y
 & 
\code{Array}, (\emph{M},)
 & 
The error in the \emph{M} training input values.
\\\hline

n
 & 
\code{Matrix}, (\emph{M}, \emph{N})
 & 
The orders of derivatives that each of the M training points represent, indicating the order of derivative with respect to each of the \emph{N} dimensions.
\\\hline

K\_up\_to\_date
 & 
bool
 & 
True if no data have been added since the last time the internal state was updated with a call to {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.compute_K_L_alpha_ll]{\code{compute\_K\_L\_alpha\_ll()}}}.
\\\hline

K
 & 
\code{Matrix}, (\emph{M}, \emph{M})
 & 
Covariance matrix between all of the training inputs.
\\\hline

noise\_K
 & 
\code{Matrix}, (\emph{M}, \emph{M})
 & 
Noise portion of the covariance matrix between all of the training inputs. Only includes the noise from \code{noise\_k}, not from \code{err\_y}.
\\\hline

L
 & 
\code{Matrix}, (\emph{M}, \emph{M})
 & 
Cholesky decomposition of the combined covariance matrix between all of the training inputs.
\\\hline

alpha
 & 
\code{Matrix}, (\emph{M}, 1)
 & 
Solution to $K\alpha=y$.
\\\hline

ll
 & 
float
 & 
Log-likelihood of the data given the model.
\\\hline
\end{tabulary}

\index{num\_dim (gptools.gaussian\_process.GaussianProcess attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.num_dim}\pysigline{\bfcode{num\_dim}}
The number of dimensions of the input data.
\begin{quote}\begin{description}
\item[{Returns }] \leavevmode
\textbf{num\_dim: int} :
\begin{quote}

The number of dimensions of the input data as defined in the kernel.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{add\_data() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.add_data}\pysiglinewithargsret{\bfcode{add\_data}}{\emph{X}, \emph{y}, \emph{err\_y=0}, \emph{n=0}}{}
Add data to the training data set of the GaussianProcess instance.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{X} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} training input values of dimension \emph{N}.
\end{quote}

\textbf{y} : \code{Array} or other Array-like, (\emph{M},)
\begin{quote}

\emph{M} training target values.
\end{quote}

\textbf{err\_y} : \code{Array} or other Array-like (\emph{M},) or scalar float, optional
\begin{quote}

Non-negative values only. Error given as standard deviation) in the
\emph{M} training target values. If \emph{err\_y} is a scalar, the data set is
taken to be homoscedastic (constant error). Otherwise, the length
of \emph{err\_y} must equal the length of \emph{y}. Default value is 0
(noiseless observations).
\end{quote}

\textbf{n} : \code{Matrix} or other Array-like (\emph{M}, \emph{N}) or scalar float, optional
\begin{quote}

Non-negative integer values only. Degree of derivative for each
training target. If \emph{n} is a scalar it is taken to be the value for
all points in \emph{y}. Otherwise, the length of n must equal the length
of \emph{y}. Default value is 0 (observation of target value). If
non-integer values are passed, they will be silently rounded.
\end{quote}

\item[{Raises }] \leavevmode
\textbf{ValueError} :
\begin{quote}

Bad shapes for any of the inputs, negative values for \emph{err\_y} or \emph{n}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_Kij() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.compute_Kij}\pysiglinewithargsret{\bfcode{compute\_Kij}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{noise=False}, \emph{hyper\_deriv=None}}{}
Compute covariance matrix between datasets \emph{Xi} and \emph{Xj}.

Specify the orders of derivatives at each location with the \emph{ni}, \emph{nj}
arrays. The \emph{include\_noise} flag is passed to the covariance kernel to
indicate whether noise is to be included (i.e., for evaluation of
$K+\sigma I$ versus $K_*$).

If \emph{Xj} is None, the symmetric matrix $K(X, X)$ is formed.

Note that type and dimension checking is NOT performed, as it is assumed
the data are from inside the instance and have hence been sanitized by
{\hyperref[gptools:gptools.gaussian_process.GaussianProcess.add_data]{\code{add\_data()}}}.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{Xi} : \code{Matrix}, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} input values of dimension \emph{N}.
\end{quote}

\textbf{Xj} : \code{Matrix}, (\emph{P}, \emph{N})
\begin{quote}

\emph{P} input values of dimension \emph{N}.
\end{quote}

\textbf{ni} : \code{Array}, (\emph{M},), non-negative integers
\begin{quote}

\emph{M} derivative orders with respect to the \emph{Xi} coordinates.
\end{quote}

\textbf{nj} : \code{Array}, (\emph{P},), non-negative integers
\begin{quote}

\emph{P} derivative orders with respect to the \emph{Xj} coordinates.
\end{quote}

\textbf{noise} : bool, optional
\begin{quote}

If True, uses the noise kernel, otherwise uses the regular kernel.
Default is False (use regular kernel).
\end{quote}

\textbf{hyper\_deriv} : None or non-negative int
\begin{quote}

Index of the hyperparameter to compute the first derivative with
respect to. If None, no derivatives are taken. Default is None (no
hyperparameter derivatives).
\end{quote}

\item[{Returns }] \leavevmode
\textbf{Kij} : \code{Matrix}, (\emph{M}, \emph{P})
\begin{quote}

Covariance matrix between \emph{Xi} and \emph{Xj}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_K\_L\_alpha\_ll() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.compute_K_L_alpha_ll}\pysiglinewithargsret{\bfcode{compute\_K\_L\_alpha\_ll}}{\emph{diag\_factor=100.0}}{}
Compute \emph{K}, \emph{L}, \emph{alpha} and log-likelihood according to the first part of Algorithm 2.1 in R\&W.

Computes \emph{K} and the noise portion of \emph{K} using {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.compute_Kij]{\code{compute\_Kij()}}},
computes \emph{L} using \code{scipy.linalg.cholesky()}, then computes
\emph{alpha} as \emph{L.T\textbackslash{}(L\textbackslash{}y)}.

Only does the computation if \code{K\_up\_to\_date} is False --
otherwise leaves the existing values.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{diag\_factor} : float, optional
\begin{quote}

Factor of \code{sys.float\_info.epsilon} which is added to
the diagonal of the total \emph{K} matrix to improve the stability of
the Cholesky decomposition. If you are having issues, try increasing
this by a factor of 10 at a time. Default is 1e2.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{update\_hyperparameters() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.update_hyperparameters}\pysiglinewithargsret{\bfcode{update\_hyperparameters}}{\emph{new\_params}, \emph{return\_jacobian=False}}{}
Update the kernel's hyperparameters to the new parameters.

This will call {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.compute_K_L_alpha_ll]{\code{compute\_K\_L\_alpha\_ll()}}} to update the state
accordingly.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{new\_params} : \code{Array} or other Array-like, length dictated by kernel
\begin{quote}

New parameters to use.
\end{quote}

\textbf{return\_jacobian} : bool, optional
\begin{quote}

If True, the return is (\emph{ll}, \emph{jac}). Otherwise, return is \emph{ll}
only and the execution is faster. Default is False (do not
compute Jacobian).
\end{quote}

\item[{Returns }] \leavevmode
\textbf{-1*ll} : float
\begin{quote}

The updated log likelihood.
\end{quote}

\textbf{-1*jac} : \code{Array}, length equal to the number of parameters
\begin{quote}

The derivative of \emph{ll} with respect to each of the parameters, in
order. Only computed and returned if \emph{return\_jacobian} is True.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{optimize\_hyperparameters() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.optimize_hyperparameters}\pysiglinewithargsret{\bfcode{optimize\_hyperparameters}}{\emph{method='SLSQP'}, \emph{opt\_kwargs=\{\}}, \emph{verbose=False}}{}
Optimize the hyperparameters by maximizing the log likelihood.

Leaves the {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} instance in the optimized state.

If \code{scipy.optimize.minimize()} is not available (i.e., if your
\code{scipy} version is older than 0.11.0) then \code{fmin\_slsqp()}
is used independent of what you set for the \emph{method} keyword.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{method} : str, optional
\begin{quote}

The method to pass to \code{scipy.optimize.minimize()}.
Refer to that function's docstring for valid options. Default
is `SLSQP'. See note above about behavior with older versions of
\code{scipy}.
\end{quote}

\textbf{opt\_kwargs} : dict, optional
\begin{quote}

Dictionary of extra keywords to pass to
\code{scipy.optimize.minimize()}. Refer to that function's docstring for
valid options. Note that if you use \emph{jac} = True (i.e., optimization
function returns Jacobian) you should also set \emph{args} = (True,) to
tell {\hyperref[gptools:gptools.gaussian_process.GaussianProcess.update_hyperparameters]{\code{update\_hyperparameters()}}} to compute and return the
Jacobian. Default is: \{\}.
\end{quote}

\textbf{verbose} : bool, optional
\begin{quote}

Whether or not the output should be verbose. If
True, the entire \code{Result} object from
\code{scipy.optimize.minimize()} is printed. If False, status
information is only printed if the \emph{success} flag from
\code{minimize()} is False. Default is False.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{Xstar}, \emph{n=0}, \emph{noise=False}, \emph{return\_cov=True}}{}
Predict the mean and covariance at the inputs \emph{Xstar}.

The order of the derivative is given by \emph{n}. The keyword \emph{noise} sets
whether or not noise is included in the prediction.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{Xstar} : \code{Array} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} test input values of dimension \emph{N}.
\end{quote}

\textbf{n} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N}) or scalar, non-negative int, optional
\begin{quote}

Order of derivative to predict (0 is the base quantity). If \emph{n} is
scalar, the value is used for all points in \emph{Xstar}. If non-integer
values are passed, they will be silently rounded. Default is 0
(return base quantity).
\end{quote}

\textbf{noise} : bool, optional
\begin{quote}

Whether or not noise should be included in the covariance. Default
is False (no noise in covariance).
\end{quote}

\textbf{return\_cov} : bool, optional
\begin{quote}

Set to True to compute and return the covariance matrix for the
predictions, False to skip this step. Default is True (return tuple
of (\emph{mean}, \emph{cov})).
\end{quote}

\item[{Returns }] \leavevmode
\textbf{mean} : \code{Array}, (\emph{M},)
\begin{quote}

Predicted GP mean.
\end{quote}

\textbf{covariance} : \code{Matrix}, (\emph{M}, \emph{M})
\begin{quote}

Predicted covariance matrix, only returned if \emph{return\_cov} is True.
\end{quote}

\item[{Raises }] \leavevmode
\textbf{ValueError} :
\begin{quote}

If \emph{n} is not consistent with the shape of \emph{Xstar} or is not entirely
composed of non-negative integers.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_ll\_matrix() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.compute_ll_matrix}\pysiglinewithargsret{\bfcode{compute\_ll\_matrix}}{\emph{bounds}, \emph{num\_pts}}{}
Compute the log likelihood over the (free) parameter space.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{bounds} : 2-tuple or list of 2-tuples with length equal to the number of free parameters
\begin{quote}

Bounds on the range to use for each of the parameters. If a single
2-tuple is given, it will be used for each of the parameters.
\end{quote}

\textbf{num\_pts} : int or list of ints with length equal to the number of free parameters
\begin{quote}

If a single int is given, it will be used for each of the parameters.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{ll\_vals} : \code{Array}
\begin{quote}
\begin{quote}

The log likelihood for each of the parameter possibilities.
\end{quote}
\begin{description}
\item[{param\_vals}] \leavevmode{[}List of \code{Array}{]}
The parameter values used.

\end{description}
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{draw\_sample() (gptools.gaussian\_process.GaussianProcess method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.GaussianProcess.draw_sample}\pysiglinewithargsret{\bfcode{draw\_sample}}{\emph{Xstar}, \emph{n=0}, \emph{noise=False}, \emph{num\_samp=1}, \emph{rand\_vars=None}, \emph{rand\_type='standard normal'}, \emph{diag\_factor=1000.0}}{}
Draw a sample evaluated at the given points \emph{Xstar}.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{Xstar} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} test input values of dimension \emph{N}.
\end{quote}

\textbf{n} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N}) or scalar, non-negative int, optional
\begin{quote}

Derivative order to evaluate at. Default is 0 (evaluate value).
\end{quote}

\textbf{noise} : bool, optional
\begin{quote}

Whether or not to include the noise components of the kernel in the
sample. Default is False (no noise in samples).
\end{quote}

\textbf{num\_samp} : Positive int, optional
\begin{quote}

Number of samples to draw. Default is 1. Cannot be used in
conjunction with \emph{rand\_vars}: If you pass both \emph{num\_samp} and
\emph{rand\_vars}, \emph{num\_samp} will be silently ignored.
\end{quote}

\textbf{rand\_vars} : \code{Matrix} or other Array-like (\emph{M}, \emph{P}), optional
\begin{quote}

Vector of random variables $u$ to use in constructing the
sample $y_* = f_* + Lu$, where $K=LL^T$. If None,
values will be produced using \code{numpy.random.multivariate\_normal()}.
This allows you to use pseudo/quasi random numbers generated by
an external routine. Default is None (use \code{multivariate\_normal()}
directly).
\end{quote}

\textbf{rand\_type} : \{`standard normal', `uniform'\}, optional
\begin{quote}

Type of distribution the inputs are given with.
\begin{itemize}
\item {} 
`standard normal': Standard (\emph{mu} = 0, \emph{sigma} = 1) normal
distribution (this is the default)

\item {} 
`uniform': Uniform distribution on {[}0, 1). In this case
the required Gaussian variables are produced with inversion.

\end{itemize}
\end{quote}

\textbf{diag\_factor} : float, optional
\begin{quote}

Number (times machine epsilon) added to the diagonal of the
covariance matrix prior to computing its Cholesky decomposition.
This is necessary as sometimes the decomposition will fail because,
to machine precision, the matrix appears to not be positive definite.
If you are getting errors from \code{scipy.linalg.cholesky()}, try increasing
this an order of magnitude at a time. This parameter only has an
effect when using rand\_vars. Default value is 1e3.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{samples} : \code{Array} (\emph{M}, \emph{P}) or (\emph{M}, \emph{num\_samp})
\begin{quote}

Samples evaluated at the \emph{M} points.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{Constraint (class in gptools.gaussian\_process)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.Constraint}\pysiglinewithargsret{\strong{class }\code{gptools.gaussian\_process.}\bfcode{Constraint}}{\emph{gp}, \emph{boundary\_val=0.0}, \emph{n=0}, \emph{loc='min'}, \emph{type\_='gt'}, \emph{bounds=None}}{}
Bases: \code{object}

Implements an inequality constraint on the value of the mean or its derivatives.

Provides a callable such as can be passed to SLSQP or COBYLA to implement
the constraint when using \code{scipy.optimize.minimize()}.

The function defaults implement a constraint that forces the mean value to
be positive everywhere.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{gp} : {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}}
\begin{quote}

The {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} instance to create the constraint on.
\end{quote}

\textbf{boundary\_val} : float, optional
\begin{quote}

Boundary value for the constraint. For \emph{type\_} = `gt', this is the lower
bound, for \emph{type\_} = `lt', this is the upper bound. Default is 0.0.
\end{quote}

\textbf{n} : non-negative int, optional
\begin{quote}

Derivative order to evaluate. Default is 0 (value of the mean). Note
that non-int values are silently cast to int.
\end{quote}

\textbf{loc} : \{`min', `max'\}, float or Array-like of float (\emph{num\_dim},), optional
\begin{quote}

Which extreme of the mean to use, or location to evaluate at.
\begin{itemize}
\item {} 
If `min', the minimum of the mean (optionally over \emph{bounds}) is used.

\item {} 
If `max', the maximum of the mean (optionally over \emph{bounds}) is used.

\item {} 
If a float (valid for \emph{num\_dim} = 1 only) or Array of float, the mean
is evaluated at the given X value.

\end{itemize}

Default is `min' (use function minimum).
\end{quote}

\textbf{type\_} : \{`gt', `lt'\}, optional
\begin{quote}

What type of inequality constraint to implement.
\begin{itemize}
\item {} 
If `gt', a greater-than-or-equals constraint is used.

\item {} 
If `lt', a less-than-or-equals constraint is used.

\end{itemize}

Default is `gt' (greater-than-or-equals).
\end{quote}

\textbf{bounds} : 2-tuple of float or 2-tuple Array-like of float (\emph{num\_dim},) or None, optional
\begin{quote}

Bounds to use when \emph{loc} is `min' or `max'.
\begin{itemize}
\item {} 
If None, the bounds are taken to be the extremes of the training data.
For multivariate data, ``extremes'' essentially means the smallest
hypercube oriented parallel to the axes that encapsulates all of the
training inputs. (I.e., \code{(gp.X.min(axis=0), gp.X.max(axis=0))})

\item {} 
If \emph{bounds} is a 2-tuple, then this is used as (\emph{lower}, \emph{upper})
where lower{}` and \emph{upper} are Array-like with dimensions (\emph{num\_dim},).

\item {} 
If \emph{num\_dim} is 1 then \emph{lower} and \emph{upper} can be scalar floats.

\end{itemize}

Default is None (use extreme values of training data).
\end{quote}

\item[{Raises }] \leavevmode
\textbf{TypeError} :
\begin{quote}

If \emph{gp} is not an instance of {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}}.
\end{quote}

\textbf{ValueError} :
\begin{quote}

If \emph{n} is negative.
\end{quote}

\textbf{ValueError} :
\begin{quote}

If \emph{loc} is not `min', `max' or an Array-like of the correct dimensions.
\end{quote}

\textbf{ValueError} :
\begin{quote}

If \emph{type\_} is not `gt' or `lt'.
\end{quote}

\textbf{ValueError} :
\begin{quote}

If \emph{bounds} is not None or length 2 or if the elements of bounds don't
have the right dimensions.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.gaussian\_process.Constraint method)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.gaussian_process.Constraint.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{params}}{}
Returns a non-negative number if the constraint is satisfied.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{params} : Array-like, length dictated by kernel
\begin{quote}

New parameters to use.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{val} : float
\begin{quote}

Value of the constraint. \code{minimize} will attempt to keep
this non-negative.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsection{\texttt{utils} Module}
\label{gptools:utils-module}\label{gptools:module-gptools.utils}\index{gptools.utils (module)}
Provides convenient utilities for working with the classes and results from \code{gptools}.
\index{parallel\_compute\_ll\_matrix() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.parallel_compute_ll_matrix}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{parallel\_compute\_ll\_matrix}}{\emph{gp}, \emph{bounds}, \emph{num\_pts}, \emph{num\_proc=None}}{}
Compute matrix of the log likelihood over the parameter space in parallel.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{bounds} : 2-tuple or list of 2-tuples with length equal to the number of free parameters
\begin{quote}

Bounds on the range to use for each of the parameters. If a single
2-tuple is given, it will be used for each of the parameters.
\end{quote}

\textbf{num\_pts} : int or list of ints with length equal to the number of free parameters
\begin{quote}

The number of points to use for each parameters. If a single int is
given, it will be used for each of the parameters.
\end{quote}

\textbf{num\_proc} : Positive int or None, optional
\begin{quote}

Number of processes to run the parallel computation with. If set to
None, ALL available cores are used. Default is None (use all available
cores).
\end{quote}

\item[{Returns }] \leavevmode
\textbf{ll\_vals} : \code{Array}
\begin{quote}

The log likelihood for each of the parameter possibilities.
\end{quote}

\textbf{param\_vals} : list of \code{Array}
\begin{quote}

The parameter values used.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{slice\_plot() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.slice_plot}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{slice\_plot}}{\emph{*args}, \emph{**kwargs}}{}
Constructs a plot that lets you look at slices through a multidimensional array.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{vals} : \code{Array}, (\emph{M}, \emph{N}, \emph{P}, ...)
\begin{quote}

Multidimensional array to visualize.
\end{quote}

\textbf{x\_vals\_1} : \code{Array}, (\emph{M},)
\begin{quote}

Values along the first dimension.
\end{quote}

\textbf{x\_vals\_2} : \code{Array}, (\emph{N},)
\begin{quote}

Values along the second dimension.
\end{quote}

\textbf{x\_vals\_3} : \code{Array}, (\emph{P},)
\begin{quote}

Values along the third dimension.

\textbf{...and so on. At least four arguments must be provided.}
\end{quote}

\textbf{names} : list of strings, optional
\begin{quote}

Names for each of the parameters at hand. If None, sequential numerical
identifiers will be used. Length must be equal to the number of
dimensions of \emph{vals}. Default is None.
\end{quote}

\textbf{n} : Positive int, optional
\begin{quote}

Number of contours to plot. Default is 100.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{f} : \code{Figure}
\begin{quote}

The Matplotlib figure instance created.
\end{quote}

\item[{Raises }] \leavevmode
\textbf{GPArgumentError} :
\begin{quote}

If the number of arguments is less than 4.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{arrow\_respond() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.arrow_respond}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{arrow\_respond}}{\emph{slider}, \emph{event}}{}
Event handler for arrow key events in plot windows.

Pass the slider object to update as a masked argument using a lambda function:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{lambda} \PYG{n}{evt}\PYG{p}{:} \PYG{n}{arrow\PYGZus{}respond}\PYG{p}{(}\PYG{n}{my\PYGZus{}slider}\PYG{p}{,} \PYG{n}{evt}\PYG{p}{)}
\end{Verbatim}
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{slider} : Slider instance associated with this handler.

\textbf{event} : Event to be handled.

\end{description}\end{quote}

\end{fulllineitems}

\index{incomplete\_bell\_poly() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.incomplete_bell_poly}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{incomplete\_bell\_poly}}{\emph{n}, \emph{k}, \emph{x}}{}
Recursive evaluation of the incomplete Bell polynomial $B_{n, k}(x)$.

Evaluates the incomplete Bell polynomial $B_{n, k}(x_1, x_2, \dots, x_{n-k+1})$,
also known as the partial Bell polynomial or the Bell polynomial of the
second kind. This polynomial is useful in the evaluation of (the univariate)
Faa di Bruno's formula which generalizes the chain rule to higher order
derivatives.

The implementation here is based on the implementation in:
\code{sympy.functions.combinatorial.numbers.bell.\_bell\_incomplete\_poly()}
Following that function's documentation, the polynomial is computed
according to the recurrence formula:
\begin{gather}
\begin{split}B_{n, k}(x_1, x_2, \dots, x_{n-k+1}) = \sum_{m=1}^{n-k+1}x_m\binom{n-1}{m-1}B_{n-m, k-1}(x_1, x_2, \dots, x_{n-m-k})\end{split}\notag
\end{gather}
\begin{DUlineblock}{0em}
\item[] The end cases are:
\item[] $B_{0, 0} = 1$
\item[] $B_{n, 0} = 0$ for $n \ge 1$
\item[] $B_{0, k} = 0$ for $k \ge 1$
\end{DUlineblock}
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{n} : scalar int
\begin{quote}

The first subscript of the polynomial.
\end{quote}

\textbf{k} : scalar int
\begin{quote}

The second subscript of the polynomial.
\end{quote}

\textbf{x} : \code{Array} of floats, (\emph{p}, \emph{n} - \emph{k} + 1)
\begin{quote}

\emph{p} sets of \emph{n} - \emph{k} + 1 points to use as the arguments to
$B_{n,k}$. The second dimension can be longer than
required, in which case the extra entries are silently ignored
(this facilitates recursion without needing to subset the array \emph{x}).
\end{quote}

\item[{Returns }] \leavevmode
\textbf{result} : \code{Array}, (\emph{p},)
\begin{quote}

Incomplete Bell polynomial evaluated at the desired values.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_set\_partition\_strings() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.generate_set_partition_strings}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{generate\_set\_partition\_strings}}{\emph{n}}{}
Generate the restricted growth strings for all of the partitions of an \emph{n}-member set.

Uses Algorithm H from page 416 of volume 4A of Knuth's \emph{The Art of Computer
Programming}. Returns the partitions in lexicographical order.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{n} : scalar int, non-negative
\begin{quote}

Number of (unique) elements in the set to be partitioned.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{partitions} : list of \code{Array}
\begin{quote}

List has a number of elements equal to the \emph{n}-th Bell number (i.e.,
the number of partitions for a set of size \emph{n}). Each element has
length \emph{n}, the elements of which are the restricted growth strings
describing the partitions of the set. The strings are returned in
lexicographic order.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_set\_partitions() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.generate_set_partitions}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{generate\_set\_partitions}}{\emph{set\_}}{}
Generate all of the partitions of a set.

This is a helper function that utilizes the restricted growth strings from
{\hyperref[gptools:gptools.utils.generate_set_partition_strings]{\code{generate\_set\_partition\_strings()}}}. The partitions are returned in
lexicographic order.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{set\_} : \code{Array} or other Array-like, (\emph{m},)
\begin{quote}

The set to find the partitions of.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{partitions} : list of lists of \code{Array}
\begin{quote}

The number of elements in the outer list is equal to the number of
partitions, which is the len(\emph{m})\textasciicircum{}th Bell number. Each of the inner lists
corresponds to a single possible partition. The length of an inner list
is therefore equal to the number of blocks. Each of the arrays in an
inner list is hence a block.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{unique\_rows() (in module gptools.utils)}

\begin{fulllineitems}
\phantomsection\label{gptools:gptools.utils.unique_rows}\pysiglinewithargsret{\code{gptools.utils.}\bfcode{unique\_rows}}{\emph{arr}}{}
Returns a copy of arr with duplicate rows removed.

From Stackoverflow ``Find unique rows in numpy.array.''
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{arr} : \code{Array}, (\emph{m}, \emph{n}). The array to find the unique rows of.

\item[{Returns }] \leavevmode
\textbf{unique} : \code{Array}, (\emph{p}, \emph{n}) where \emph{p} \textless{}= \emph{m}
\begin{quote}

The array \emph{arr} with duplicate rows removed.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}



\subsection{Subpackages}
\label{gptools:subpackages}

\subsubsection{kernel Package}
\label{gptools.kernel::doc}\label{gptools.kernel:kernel-package}

\paragraph{\texttt{kernel} Package}
\label{gptools.kernel:id1}\phantomsection\label{gptools.kernel:module-gptools.kernel}\index{gptools.kernel (module)}
Subpackage containing a variety of covariance kernels and associated helpers.


\paragraph{\texttt{core} Module}
\label{gptools.kernel:core-module}\label{gptools.kernel:module-gptools.kernel.core}\index{gptools.kernel.core (module)}
Core kernel classes: contains the base {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} class and helper subclasses.
\index{Kernel (class in gptools.kernel.core)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.core.}\bfcode{Kernel}}{\emph{num\_dim}, \emph{num\_params}, \emph{initial\_params=None}, \emph{fixed\_params=None}, \emph{param\_bounds=None}, \emph{enforce\_bounds=False}}{}
Bases: \code{object}

Covariance kernel base class. Not meant to be explicitly instantiated!

Initialize the kernel with the given number of input dimensions.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{num\_dim} : positive int
\begin{quote}

Number of dimensions of the input data. Must be consistent with the \emph{X}
and \emph{Xstar} values passed to the {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} you wish
to use the covariance kernel with.
\end{quote}

\textbf{num\_params} : Non-negative int
\begin{quote}

Number of parameters in the model.
\end{quote}

\textbf{initial\_params} : \code{Array} or other Array-like, (\emph{num\_params},), optional
\begin{quote}

Initial values to set for the hyperparameters. Default is None, in
which case 1 is used for the initial values.
\end{quote}

\textbf{fixed\_params} : \code{Array} or other Array-like of bool, (\emph{num\_params},), optional
\begin{quote}

Sets which parameters are considered fixed when optimizing the log
likelihood. A True entry corresponds to that element being
fixed (where the element ordering is as defined in the class).
Default value is None (no parameters are fixed).
\end{quote}

\textbf{param\_bounds} : list of 2-tuples (\emph{num\_params},), optional
\begin{quote}

List of bounds for each of the parameters. Each 2-tuple is of the form
(\emph{lower}, \emph{upper}). If there is no bound in a given direction, set it
to None. Default is (0.0, None) for each parameter.
\end{quote}

\textbf{enforce\_bounds} : bool, optional
\begin{quote}

If True, an attempt to set a parameter outside of its bounds will
result in the parameter being set right at its bound. If False, bounds
are not enforced inside the kernel. Default is False (do not enforce
bounds).
\end{quote}

\item[{Raises }] \leavevmode
\textbf{ValueError} :
\begin{quote}

If \emph{num\_dim} is not a positive integer or the lengths of the input
vectors are inconsistent.
\end{quote}

\textbf{GPArgumentError} :
\begin{quote}

if \emph{fixed\_params} is passed but \emph{initial\_params} is not.
\end{quote}

\end{description}\end{quote}
\paragraph{Attributes}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

num\_params
 & 
int
 & 
Number of parameters
\\\hline

num\_dim
 & 
int
 & 
Number of dimensions
\\\hline

params
 & 
\code{Array} of float, (\emph{num\_params},)
 & 
Array of parameters.
\\\hline

fixed\_params
 & 
\code{Array} of bool, (\emph{num\_params},)
 & 
Array of booleans indicated which parameters in params are fixed.
\\\hline

param\_bounds
 & 
list of 2-tuples, (\emph{num\_params},)
 & 
List of bounds for the parameters in params.
\\\hline
\end{tabulary}

\index{\_\_call\_\_() (gptools.kernel.core.Kernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. If None, no derivatives are taken. Default is None
(no hyperparameter derivatives).
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\end{description}\end{quote}
\paragraph{Notes}

THIS IS ONLY A METHOD STUB TO DEFINE THE NEEDED CALLING FINGERPRINT!

\end{fulllineitems}

\index{set\_hyperparams() (gptools.kernel.core.Kernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.set_hyperparams}\pysiglinewithargsret{\bfcode{set\_hyperparams}}{\emph{new\_params}}{}
Sets the free hyperparameters to the new parameter values in new\_params.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{new\_params} : \code{Array} or other Array-like, (len(\code{self.params}),)
\begin{quote}

New parameter values, ordered as dictated by the docstring for the
class.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{free\_params (gptools.kernel.core.Kernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.free_params}\pysigline{\bfcode{free\_params}}
Returns the values of the free hyperparameters.
\begin{quote}\begin{description}
\item[{Returns }] \leavevmode
\textbf{free\_params} : \code{Array}
\begin{quote}

Array of the free parameters, in order.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{free\_param\_bounds (gptools.kernel.core.Kernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.free_param_bounds}\pysigline{\bfcode{free\_param\_bounds}}
Returns the bounds of the free hyperparameters.
\begin{quote}\begin{description}
\item[{Returns }] \leavevmode
\textbf{free\_param\_bounds} : \code{Array}
\begin{quote}

Array of the bounds of the free parameters, in order.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{\_\_add\_\_() (gptools.kernel.core.Kernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.__add__}\pysiglinewithargsret{\bfcode{\_\_add\_\_}}{\emph{other}}{}
Add two Kernels together.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{other} : {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}
\begin{quote}

Kernel to be added to this one.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{sum} : {\hyperref[gptools.kernel:gptools.kernel.core.SumKernel]{\code{SumKernel}}}
\begin{quote}

Instance representing the sum of the two kernels.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{\_\_mul\_\_() (gptools.kernel.core.Kernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.Kernel.__mul__}\pysiglinewithargsret{\bfcode{\_\_mul\_\_}}{\emph{other}}{}
Multiply two Kernels together.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{other} : {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}
\begin{quote}

Kernel to be multiplied by this one.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{prod} : {\hyperref[gptools.kernel:gptools.kernel.core.ProductKernel]{\code{ProductKernel}}}
\begin{quote}

Instance representing the product of the two kernels.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{BinaryKernel (class in gptools.kernel.core)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.BinaryKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.core.}\bfcode{BinaryKernel}}{\emph{k1}, \emph{k2}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{gptools.kernel.core.Kernel}}}

Abstract class for binary operations on kernels (addition, multiplication, etc.).
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{k1, k2} : {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}} instances to be combined

\end{description}\end{quote}
\paragraph{Notes}

\emph{k1} and \emph{k2} must have the same number of dimensions.
\index{set\_hyperparams() (gptools.kernel.core.BinaryKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.BinaryKernel.set_hyperparams}\pysiglinewithargsret{\bfcode{set\_hyperparams}}{\emph{new\_params}}{}
Set the (free) hyperparameters.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{new\_params} : \code{Array} or other Array-like
\begin{quote}

New values of the free parameters.
\end{quote}

\item[{Raises }] \leavevmode
\textbf{ValueError} :
\begin{quote}

If the length of \emph{new\_params} is not consistent with \code{self.params}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{SumKernel (class in gptools.kernel.core)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.SumKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.core.}\bfcode{SumKernel}}{\emph{k1}, \emph{k2}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.BinaryKernel]{\code{gptools.kernel.core.BinaryKernel}}}

The sum of two kernels.
\index{\_\_call\_\_() (gptools.kernel.core.SumKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.SumKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{*args}, \emph{**kwargs}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\item[{Raises }] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If the \emph{hyper\_deriv} keyword is given and is not None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{ProductKernel (class in gptools.kernel.core)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.ProductKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.core.}\bfcode{ProductKernel}}{\emph{k1}, \emph{k2}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.BinaryKernel]{\code{gptools.kernel.core.BinaryKernel}}}

The product of two kernels.
\index{\_\_call\_\_() (gptools.kernel.core.ProductKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.ProductKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{*args}, \emph{**kwargs}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\item[{Raises }] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If the \emph{hyper\_deriv} keyword is given and is not None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{ChainRuleKernel (class in gptools.kernel.core)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.ChainRuleKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.core.}\bfcode{ChainRuleKernel}}{\emph{num\_dim}, \emph{num\_params}, \emph{initial\_params=None}, \emph{fixed\_params=None}, \emph{param\_bounds=None}, \emph{enforce\_bounds=False}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{gptools.kernel.core.Kernel}}}

Abstract class for the common methods in creating kernels that require application of Faa di Bruno's formula.
\index{\_\_call\_\_() (gptools.kernel.core.ChainRuleKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.ChainRuleKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. If None, no derivatives are taken. Hyperparameter
derivatives are not supported at this point. Default is None.
\end{quote}

\textbf{symmetric} : bool
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\item[{Raises }] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If the \emph{hyper\_deriv} keyword is not None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{ArbitraryKernel (class in gptools.kernel.core)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.ArbitraryKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.core.}\bfcode{ArbitraryKernel}}{\emph{num\_dim}, \emph{cov\_func}, \emph{num\_proc=0}, \emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{gptools.kernel.core.Kernel}}}

Covariance kernel from an arbitrary covariance function.

Computes derivatives using \code{mpmath.diff()} and is hence in general
much slower than a hard-coded implementation of a given kernel.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{num\_dim} : positive int
\begin{quote}

Number of dimensions of the input data. Must be consistent with the \emph{X}
and \emph{Xstar} values passed to the
{\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} you wish to use
the covariance kernel with.
\end{quote}

\textbf{cov\_func} : callable, takes \textgreater{}= 2 args
\begin{quote}

Covariance function. Must take arrays of \emph{Xi} and \emph{Xj} as the
first two arguments. The subsequent (scalar) arguments are the
hyperparameters. The number of parameters is found by inspection of
\emph{cov\_func} itself.
\end{quote}

\textbf{num\_proc} : int or None, optional
\begin{quote}

Number of procs to use in evaluating covariance derivatives. 0 means
to do it in serial, None means to use all available cores. Default is
0 (serial evaluation).
\end{quote}

\textbf{**kwargs} :
\begin{quote}

All other keyword parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\end{description}\end{quote}
\paragraph{Attributes}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

cov\_func
 & 
callable
 & 
The covariance function
\\\hline

num\_proc
 & 
non-negative int
 & 
Number of processors to use in evaluating covariance derivatives. 0 means serial.
\\\hline
\end{tabulary}

\index{\_\_call\_\_() (gptools.kernel.core.ArbitraryKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.core.ArbitraryKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. If None, no derivatives are taken. Hyperparameter
derivatives are not supported at this point. Default is None.
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\item[{Raises }] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If the \emph{hyper\_deriv} keyword is not None.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{\texttt{gibbs} Module}
\label{gptools.kernel:module-gptools.kernel.gibbs}\label{gptools.kernel:gibbs-module}\index{gptools.kernel.gibbs (module)}
Provides classes and functions for creating SE kernels with warped length scales.
\index{tanh\_warp() (in module gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.tanh_warp}\pysiglinewithargsret{\code{gptools.kernel.gibbs.}\bfcode{tanh\_warp}}{\emph{X}, \emph{l1}, \emph{l2}, \emph{lw}, \emph{x0}}{}
Warps the \emph{X} coordinate with the tanh model
\begin{gather}
\begin{split}l = \frac{l_1 + l_2}{2} - \frac{l_1 - l_2}{2}\tanh\frac{x-x_0}{l_w}\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{X} : \code{Array}, (\emph{M},) or scalar float
\begin{quote}

\emph{M} locations to evaluate length scale at.
\end{quote}

\textbf{l1} : positive float
\begin{quote}

Small-\emph{X} saturation value of the length scale.
\end{quote}

\textbf{l2} : positive float
\begin{quote}

Large-\emph{X} saturation value of the length scale.
\end{quote}

\textbf{lw} : positive float
\begin{quote}

Length scale of the transition between the two length scales.
\end{quote}

\textbf{x0} : float
\begin{quote}

Location of the center of the transition between the two length scales.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{l} : \code{Array}, (\emph{M},) or scalar float
\begin{quote}

The value of the length scale at the specified point.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{spline\_warp() (in module gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.spline_warp}\pysiglinewithargsret{\code{gptools.kernel.gibbs.}\bfcode{spline\_warp}}{\emph{X}, \emph{l1}, \emph{l2}, \emph{lw}, \emph{x0}}{}
Warps the \emph{X} coordinate with a ``divot'' spline shape.

\begin{notice}{warning}{Warning:}
Broken! Do not use!
\end{notice}
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{X} : \code{Array}, (\emph{M},) or scalar float
\begin{quote}

\emph{M} locations to evaluate length scale at.
\end{quote}

\textbf{l1} : positive float
\begin{quote}

Global value of the length scale.
\end{quote}

\textbf{l2} : positive float
\begin{quote}

Pedestal value of the length scale.
\end{quote}

\textbf{lw} : positive float
\begin{quote}

Width of the dip.
\end{quote}

\textbf{x0} : float
\begin{quote}

Location of the center of the dip in length scale.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{l} : \code{Array}, (\emph{M},) or scalar float
\begin{quote}

The value of the length scale at the specified point.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{gauss\_warp() (in module gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.gauss_warp}\pysiglinewithargsret{\code{gptools.kernel.gibbs.}\bfcode{gauss\_warp}}{\emph{X}, \emph{l1}, \emph{l2}, \emph{lw}, \emph{x0}}{}
Warps the \emph{X} coordinate with a Gaussian-shaped divot.
\begin{gather}
\begin{split}l = l_1 - (l_1 - l_2) \exp\left ( -4\ln 2\frac{(X-x_0)^2}{l_{w}^{2}} \right )\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{X} : \code{Array}, (\emph{M},) or scalar float
\begin{quote}

\emph{M} locations to evaluate length scale at.
\end{quote}

\textbf{l1} : positive float
\begin{quote}

Global value of the length scale.
\end{quote}

\textbf{l2} : positive float
\begin{quote}

Pedestal value of the length scale.
\end{quote}

\textbf{lw} : positive float
\begin{quote}

Width of the dip.
\end{quote}

\textbf{x0} : float
\begin{quote}

Location of the center of the dip in length scale.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{l} : \code{Array}, (\emph{M},) or scalar float
\begin{quote}

The value of the length scale at the specified point.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{GibbsFunction1d (class in gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.GibbsFunction1d}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.gibbs.}\bfcode{GibbsFunction1d}}{\emph{warp\_function}}{}
Bases: \code{object}

Wrapper class for the Gibbs covariance function, permits the use of arbitrary warping.

The covariance function is given by
\begin{gather}
\begin{split}k = \left ( \frac{2l(x)l(x')}{l^2(x)+l^2(x')} \right )^{1/2}\exp\left ( -\frac{(x-x')^2}{l^2(x)+l^2(x')} \right )\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{warp\_function} : callable
\begin{quote}

The function that warps the length scale as a function of X. Must have
the fingerprint (\emph{Xi}, \emph{l1}, \emph{l2}, \emph{lw}, \emph{x0}).
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.kernel.gibbs.GibbsFunction1d method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.GibbsFunction1d.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{sigmaf}, \emph{l1}, \emph{l2}, \emph{lw}, \emph{x0}}{}
Evaluate the covariance function between points \emph{Xi} and \emph{Xj}.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{Xi, Xj} : \code{Array}, \code{mpf} or scalar float
\begin{quote}

Points to evaluate covariance between. If they are \code{Array},
\code{scipy} functions are used, otherwise \code{mpmath}
functions are used.
\end{quote}

\textbf{sigmaf} : scalar float
\begin{quote}

Prefactor on covariance.
\end{quote}

\textbf{l1, l2, lw, x0} : scalar floats
\begin{quote}

Parameters of length scale warping function, passed to
\code{warp\_function}.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{k} : \code{Array} or \code{mpf}
\begin{quote}

Covariance between the given points.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{GibbsKernel1dtanh (class in gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.GibbsKernel1dtanh}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.gibbs.}\bfcode{GibbsKernel1dtanh}}{\emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.ArbitraryKernel]{\code{gptools.kernel.core.ArbitraryKernel}}}

Gibbs warped squared exponential covariance function in 1d.

Computes derivatives using \code{mpmath.diff()} and is hence in general
much slower than a hard-coded implementation of a given kernel.

The covariance function is given by
\begin{gather}
\begin{split}k = \left ( \frac{2l(x)l(x')}{l^2(x)+l^2(x')} \right )^{1/2}\exp\left ( -\frac{(x-x')^2}{l^2(x)+l^2(x')} \right )\end{split}\notag
\end{gather}
Warps the length scale using a hyperbolic tangent:
\begin{gather}
\begin{split}l = \frac{l_1 + l_2}{2} - \frac{l_1 - l_2}{2}\tanh\frac{x-x_0}{l_w}\end{split}\notag
\end{gather}
The order of the hyperparameters is:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigmaf
 & 
Amplitude of the covariance function
\\\hline

1
 & 
l1
 & 
Small-X saturation value of the length scale.
\\\hline

2
 & 
l2
 & 
Large-X saturation value of the length scale.
\\\hline

3
 & 
lw
 & 
Length scale of the transition between the two length scales.
\\\hline

4
 & 
x0
 & 
Location of the center of the transition between the two length scales.
\\\hline
\end{tabulary}

\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{**kwargs} :
\begin{quote}

All parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{GibbsKernel1dSpline (class in gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.GibbsKernel1dSpline}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.gibbs.}\bfcode{GibbsKernel1dSpline}}{\emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.ArbitraryKernel]{\code{gptools.kernel.core.ArbitraryKernel}}}

Gibbs warped squared exponential covariance function in 1d.

\begin{notice}{warning}{Warning:}
Broken! Do not use!
\end{notice}

Computes derivatives using \code{mpmath.diff()} and is hence in general
much slower than a hard-coded implementation of a given kernel.

The covariance function is given by
\begin{gather}
\begin{split}k = \left ( \frac{2l(x)l(x')}{l^2(x)+l^2(x')} \right )^{1/2}\exp\left ( -\frac{(x-x')^2}{l^2(x)+l^2(x')} \right )\end{split}\notag
\end{gather}
Warps the length scale using a spline.

The order of the hyperparameters is:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigmaf
 & 
Amplitude of the covariance function
\\\hline

1
 & 
l1
 & 
Global value of the length scale.
\\\hline

2
 & 
l2
 & 
Pedestal value of the length scale.
\\\hline

3
 & 
lw
 & 
Width of the dip.
\\\hline

4
 & 
x0
 & 
Location of the center of the dip in length scale.
\\\hline
\end{tabulary}

\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{**kwargs} :
\begin{quote}

All parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}

\index{GibbsKernel1dGauss (class in gptools.kernel.gibbs)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.gibbs.GibbsKernel1dGauss}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.gibbs.}\bfcode{GibbsKernel1dGauss}}{\emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.ArbitraryKernel]{\code{gptools.kernel.core.ArbitraryKernel}}}

Gibbs warped squared exponential covariance function in 1d.

Computes derivatives using \code{mpmath.diff()} and is hence in general
much slower than a hard-coded implementation of a given kernel.

The covariance function is given by
\begin{gather}
\begin{split}k = \left ( \frac{2l(x)l(x')}{l^2(x)+l^2(x')} \right )^{1/2}\exp\left ( -\frac{(x-x')^2}{l^2(x)+l^2(x')} \right )\end{split}\notag
\end{gather}
Warps the length scale using a gaussian:
\begin{gather}
\begin{split}l = l_1 - (l_1 - l_2) \exp\left ( -4\ln 2\frac{(X-x_0)^2}{l_{w}^{2}} \right )\end{split}\notag
\end{gather}
The order of the hyperparameters is:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigmaf
 & 
Amplitude of the covariance function
\\\hline

1
 & 
l1
 & 
Global value of the length scale.
\\\hline

2
 & 
l2
 & 
Pedestal value of the length scale.
\\\hline

3
 & 
lw
 & 
Width of the dip.
\\\hline

4
 & 
x0
 & 
Location of the center of the dip in length scale.
\\\hline
\end{tabulary}

\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{**kwargs} :
\begin{quote}

All parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}



\paragraph{\texttt{matern} Module}
\label{gptools.kernel:matern-module}\label{gptools.kernel:module-gptools.kernel.matern}\index{gptools.kernel.matern (module)}
Provides the {\hyperref[gptools.kernel:gptools.kernel.matern.MaternKernel]{\code{MaternKernel}}} class which implements the anisotropic Matern kernel.
\index{MaternKernel (class in gptools.kernel.matern)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.matern.MaternKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.matern.}\bfcode{MaternKernel}}{\emph{num\_dim}, \emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.ChainRuleKernel]{\code{gptools.kernel.core.ChainRuleKernel}}}

Matern covariance kernel. Supports arbitrary derivatives. Treats order as a hyperparameter.

The Matern kernel has the following hyperparameters, always referenced in
the order listed:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigma
 & 
prefactor
\\\hline

1
 & 
nu
 & 
order of kernel
\\\hline

2
 & 
l1
 & 
length scale for the first dimension
\\\hline

3
 & 
l2
 & 
...and so on for all dimensions
\\\hline
\end{tabulary}


The kernel is defined as:
\begin{gather}
\begin{split}k_M = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}
\left (\sqrt{2\nu} \sum_i\left (\frac{\tau_i^2}{l_i^2}\right )\right )^\nu
K_\nu\left(\sqrt{2\nu} \sum_i\left(\frac{\tau_i^2}{l_i^2}\right)\right)\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{num\_dim} : int
\begin{quote}

Number of dimensions of the input data. Must be consistent with the \emph{X}
and \emph{Xstar} values passed to the {\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}}
you wish to use the covariance kernel with.
\end{quote}

\textbf{**kwargs} :
\begin{quote}

All keyword parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.ChainRuleKernel]{\code{ChainRuleKernel}}}.
\end{quote}

\item[{Raises }] \leavevmode
\textbf{ValueError} :
\begin{quote}

If \emph{num\_dim} is not a positive integer or the lengths of the input
vectors are inconsistent.
\end{quote}

\textbf{GPArgumentError} :
\begin{quote}

If \emph{fixed\_params} is passed but \emph{initial\_params} is not.
\end{quote}

\end{description}\end{quote}
\index{nu (gptools.kernel.matern.MaternKernel attribute)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.matern.MaternKernel.nu}\pysigline{\bfcode{nu}}
Returns the value of the order $\nu$.

\end{fulllineitems}


\end{fulllineitems}



\paragraph{\texttt{noise} Module}
\label{gptools.kernel:noise-module}\label{gptools.kernel:module-gptools.kernel.noise}\index{gptools.kernel.noise (module)}
Provides classes for implementing uncorrelated noise.
\index{DiagonalNoiseKernel (class in gptools.kernel.noise)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.noise.DiagonalNoiseKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.noise.}\bfcode{DiagonalNoiseKernel}}{\emph{num\_dim}, \emph{initial\_noise=None}, \emph{fixed\_noise=None}, \emph{noise\_bound=None}, \emph{n=0}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{gptools.kernel.core.Kernel}}}

Kernel that has constant, independent noise (i.e., a diagonal kernel).
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{num\_dim} : positive int
\begin{quote}

Number of dimensions of the input data.
\end{quote}

\textbf{initial\_noise} : float
\begin{quote}

Initial value for the noise standard deviation. Default value is None
(noise gets set to 1).
\end{quote}

\textbf{fixed\_noise} : bool
\begin{quote}

Whether or not the noise is taken to be fixed when optimizing the log
likelihood.
\end{quote}

\textbf{noise\_bound} : 2-tuple
\begin{quote}

The bounds for the noise when optimizing the log likelihood with
\code{scipy.optimize.minimize()}. Must be of the form
(\emph{lower}, \emph{upper}). Set a given entry to None to put no bound on
that side. Default is None, which gets set to (0, None).
\end{quote}

\textbf{n} : non-negative int or tuple of non-negative ints with length equal to \emph{num\_dim}
\begin{quote}

Indicates which derivative this noise is with respect to. Default is 0
(noise applies to value).
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.kernel.noise.DiagonalNoiseKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.noise.DiagonalNoiseKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. Since this kernel only has one hyperparameter, 0
is the only valid value. If None, no derivatives are taken. Default
is None (no hyperparameter derivatives).
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{ZeroKernel (class in gptools.kernel.noise)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.noise.ZeroKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.noise.}\bfcode{ZeroKernel}}{\emph{num\_dim}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.noise.DiagonalNoiseKernel]{\code{gptools.kernel.noise.DiagonalNoiseKernel}}}

Kernel that always evaluates to zero, used as the default noise kernel.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{num\_dim} : positive int
\begin{quote}

The number of dimensions of the inputs.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.kernel.noise.ZeroKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.noise.ZeroKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
Return zeros the same length as the input Xi.

Ignores all other arguments.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. Since this kernel only has one hyperparameter, 0
is the only valid value. If None, no derivatives are taken. Default
is None (no hyperparameter derivatives).
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{\texttt{rational\_quadratic} Module}
\label{gptools.kernel:module-gptools.kernel.rational_quadratic}\label{gptools.kernel:rational-quadratic-module}\index{gptools.kernel.rational\_quadratic (module)}
Provides the {\hyperref[gptools.kernel:gptools.kernel.rational_quadratic.RationalQuadraticKernel]{\code{RationalQuadraticKernel}}} class which implements the anisotropic rational quadratic (RQ) kernel.
\index{RationalQuadraticKernel (class in gptools.kernel.rational\_quadratic)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.rational_quadratic.RationalQuadraticKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.rational\_quadratic.}\bfcode{RationalQuadraticKernel}}{\emph{num\_dim}, \emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.ChainRuleKernel]{\code{gptools.kernel.core.ChainRuleKernel}}}

Rational quadratic (RQ) covariance kernel. Supports arbitrary derivatives.

The RQ kernel has the following hyperparameters, always referenced
in the order listed:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigma
 & 
prefactor.
\\\hline

1
 & 
alpha
 & 
order of kernel.
\\\hline

2
 & 
l1
 & 
length scale for the first dimension.
\\\hline

3
 & 
l2
 & 
...and so on for all dimensions.
\\\hline
\end{tabulary}


The kernel is defined as:
\begin{gather}
\begin{split}k_{RQ} = \sigma^2 \left(1 + \frac{1}{2\alpha} \sum_i\frac{\tau_i^2}{l_i^2}\right)^{-\alpha}\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{num\_dim} : int
\begin{quote}

Number of dimensions of the input data. Must be consistent
with the \emph{X} and \emph{Xstar} values passed to the
{\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} you
wish to use the covariance kernel with.
\end{quote}

\textbf{**kwargs} :
\begin{quote}

All keyword parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.ChainRuleKernel]{\code{ChainRuleKernel}}}.
\end{quote}

\item[{Raises }] \leavevmode
\textbf{ValueError} :
\begin{quote}

If \emph{num\_dim} is not a positive integer or the lengths of
the input vectors are inconsistent.
\end{quote}

\textbf{GPArgumentError} :
\begin{quote}

If \emph{fixed\_params} is passed but \emph{initial\_params} is not.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}



\paragraph{\texttt{squared\_exponential} Module}
\label{gptools.kernel:squared-exponential-module}\label{gptools.kernel:module-gptools.kernel.squared_exponential}\index{gptools.kernel.squared\_exponential (module)}
Provides the {\hyperref[gptools.kernel:gptools.kernel.squared_exponential.SquaredExponentialKernel]{\code{SquaredExponentialKernel}}} class that implements the anisotropic SE kernel.
\index{SquaredExponentialKernel (class in gptools.kernel.squared\_exponential)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.squared_exponential.SquaredExponentialKernel}\pysiglinewithargsret{\strong{class }\code{gptools.kernel.squared\_exponential.}\bfcode{SquaredExponentialKernel}}{\emph{num\_dim}, \emph{**kwargs}}{}
Bases: {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{gptools.kernel.core.Kernel}}}

Squared exponential covariance kernel. Supports arbitrary derivatives.

The squared exponential has the following hyperparameters, always
referenced in the order listed:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

0
 & 
sigma
 & 
prefactor on the SE
\\\hline

1
 & 
l1
 & 
length scale for the first dimension
\\\hline

2
 & 
l2
 & 
...and so on for all dimensions
\\\hline
\end{tabulary}


The kernel is defined as:
\begin{gather}
\begin{split}k_{SE} = \sigma^2 \exp\left(-\sum_i\frac{\tau_i^2}{l_i^2}\right)\end{split}\notag
\end{gather}\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{num\_dim} : int
\begin{quote}

Number of dimensions of the input data. Must be consistent
with the \emph{X} and \emph{Xstar} values passed to the
{\hyperref[gptools:gptools.gaussian_process.GaussianProcess]{\code{GaussianProcess}}} you
wish to use the covariance kernel with.
\end{quote}

\textbf{**kwargs} :
\begin{quote}

All keyword parameters are passed to {\hyperref[gptools.kernel:gptools.kernel.core.Kernel]{\code{Kernel}}}.
\end{quote}

\item[{Raises }] \leavevmode
\textbf{ValueError} :
\begin{quote}

If \emph{num\_dim} is not a positive integer or the lengths of
the input vectors are inconsistent.
\end{quote}

\textbf{GPArgumentError} :
\begin{quote}

If \emph{fixed\_params} is passed but \emph{initial\_params} is not.
\end{quote}

\end{description}\end{quote}
\index{\_\_call\_\_() (gptools.kernel.squared\_exponential.SquaredExponentialKernel method)}

\begin{fulllineitems}
\phantomsection\label{gptools.kernel:gptools.kernel.squared_exponential.SquaredExponentialKernel.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{Xi}, \emph{Xj}, \emph{ni}, \emph{nj}, \emph{hyper\_deriv=None}, \emph{symmetric=False}}{}
Evaluate the covariance between points \emph{Xi} and \emph{Xj} with derivative order \emph{ni}, \emph{nj}.
\begin{quote}\begin{description}
\item[{Parameters }] \leavevmode
\textbf{Xi} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{Xj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} inputs with dimension \emph{N}.
\end{quote}

\textbf{ni} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{i}.
\end{quote}

\textbf{nj} : \code{Matrix} or other Array-like, (\emph{M}, \emph{N})
\begin{quote}

\emph{M} derivative orders for set \emph{j}.
\end{quote}

\textbf{hyper\_deriv} : Non-negative int or None, optional
\begin{quote}

The index of the hyperparameter to compute the first derivative
with respect to. If None, no derivatives are taken. Default is None
(no hyperparameter derivatives). Hyperparameter derivatives are not
support for \emph{n} \textgreater{} 0 at this time.
\end{quote}

\textbf{symmetric} : bool, optional
\begin{quote}

Whether or not the input \emph{Xi}, \emph{Xj} are from a symmetric matrix.
Default is False.
\end{quote}

\item[{Returns }] \leavevmode
\textbf{Kij} : \code{Array}, (\emph{M},)
\begin{quote}

Covariances for each of the \emph{M} \emph{Xi}, \emph{Xj} pairs.
\end{quote}

\item[{Raises }] \leavevmode
\textbf{NotImplementedError} :
\begin{quote}

If hyper\_deriv is not None and \emph{n} \textgreater{} 0.
\end{quote}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Indices and tables}
\label{index:indices-and-tables}\begin{itemize}
\item {} 
\emph{genindex}

\item {} 
\emph{modindex}

\item {} 
\emph{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{theindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{g}
\item {\texttt{gptools.\_\_init\_\_}}, \pageref{gptools:module-gptools.__init__}
\item {\texttt{gptools.error\_handling}}, \pageref{gptools:module-gptools.error_handling}
\item {\texttt{gptools.gaussian\_process}}, \pageref{gptools:module-gptools.gaussian_process}
\item {\texttt{gptools.kernel}}, \pageref{gptools.kernel:module-gptools.kernel}
\item {\texttt{gptools.kernel.core}}, \pageref{gptools.kernel:module-gptools.kernel.core}
\item {\texttt{gptools.kernel.gibbs}}, \pageref{gptools.kernel:module-gptools.kernel.gibbs}
\item {\texttt{gptools.kernel.matern}}, \pageref{gptools.kernel:module-gptools.kernel.matern}
\item {\texttt{gptools.kernel.noise}}, \pageref{gptools.kernel:module-gptools.kernel.noise}
\item {\texttt{gptools.kernel.rational\_quadratic}}, \pageref{gptools.kernel:module-gptools.kernel.rational_quadratic}
\item {\texttt{gptools.kernel.squared\_exponential}}, \pageref{gptools.kernel:module-gptools.kernel.squared_exponential}
\item {\texttt{gptools.utils}}, \pageref{gptools:module-gptools.utils}
\end{theindex}
\renewcommand{\indexname}{Python Module Index}
\begin{theindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{g}
\item {\texttt{gptools.\_\_init\_\_}}, \pageref{gptools:module-gptools.__init__}
\item {\texttt{gptools.error\_handling}}, \pageref{gptools:module-gptools.error_handling}
\item {\texttt{gptools.gaussian\_process}}, \pageref{gptools:module-gptools.gaussian_process}
\item {\texttt{gptools.kernel}}, \pageref{gptools.kernel:module-gptools.kernel}
\item {\texttt{gptools.kernel.core}}, \pageref{gptools.kernel:module-gptools.kernel.core}
\item {\texttt{gptools.kernel.gibbs}}, \pageref{gptools.kernel:module-gptools.kernel.gibbs}
\item {\texttt{gptools.kernel.matern}}, \pageref{gptools.kernel:module-gptools.kernel.matern}
\item {\texttt{gptools.kernel.noise}}, \pageref{gptools.kernel:module-gptools.kernel.noise}
\item {\texttt{gptools.kernel.rational\_quadratic}}, \pageref{gptools.kernel:module-gptools.kernel.rational_quadratic}
\item {\texttt{gptools.kernel.squared\_exponential}}, \pageref{gptools.kernel:module-gptools.kernel.squared_exponential}
\item {\texttt{gptools.utils}}, \pageref{gptools:module-gptools.utils}
\end{theindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}
